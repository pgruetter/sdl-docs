(window.webpackJsonp=window.webpackJsonp||[]).push([[12],{122:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/files/application-download-part1-179e0e0ac63d44c55017a80369c8fe4b.conf"},78:function(e,t,a){"use strict";a.r(t),a.d(t,"frontMatter",(function(){return r})),a.d(t,"metadata",(function(){return s})),a.d(t,"rightToc",(function(){return c})),a.d(t,"default",(function(){return p}));var n=a(3),o=a(7),i=(a(0),a(96)),r={title:"Select Columns"},s={unversionedId:"getting-started/select-columns",id:"getting-started/select-columns",isDocsHomePage:!1,title:"Select Columns",description:"Goal",source:"@site/docs/getting-started/select-columns.md",slug:"/getting-started/select-columns",permalink:"/sdl-docs/docs/getting-started/select-columns",version:"current",sidebar:"docs",previous:{title:"Get Airports",permalink:"/sdl-docs/docs/getting-started/get-airports"},next:{title:"Joining It Together",permalink:"/sdl-docs/docs/getting-started/joining-it-together"}},c=[{value:"Goal",id:"goal",children:[]},{value:"Define output object",id:"define-output-object",children:[]},{value:"Define select-airport-cols action",id:"define-select-airport-cols-action",children:[]},{value:"Try it out",id:"try-it-out",children:[]},{value:"More on Feeds",id:"more-on-feeds",children:[]},{value:"Example of Common Mistake",id:"example-of-common-mistake",children:[]}],l={rightToc:c};function p(e){var t=e.components,r=Object(o.a)(e,["components"]);return Object(i.b)("wrapper",Object(n.a)({},l,r,{components:t,mdxType:"MDXLayout"}),Object(i.b)("h2",{id:"goal"},"Goal"),Object(i.b)("p",null,"In this step we write our first Action that modifies data.\nWe will continue based upon the config file available ",Object(i.b)("a",{target:"_blank",href:a(122).default},"here"),".\nWhen you look at the data in the folder ",Object(i.b)("em",{parentName:"p"},"data/stg-airports/result.csv"),", you will notice that we\ndon't need most of the columns. In this step, we will write a simple ",Object(i.b)("em",{parentName:"p"},"CopyAction")," that selects only the columns we\nare interested in."),Object(i.b)("p",null,"As usual, we need to define an output DataObject and an action.\nWe don't need to define a new input DataObject as we will wire our new action to the existing DataObject ",Object(i.b)("em",{parentName:"p"},"stg-airports"),". "),Object(i.b)("h2",{id:"define-output-object"},"Define output object"),Object(i.b)("p",null,"Let's use CsvFileDataObject again because that makes it easy for us to check the result.\nIn more advanced (speak: real-life) scenarios, we would use one of numerous other possibilities,\nsuch as HiveTableDataObject, SplunkDataObject...\nSee ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://github.com/smart-data-lake/smart-data-lake/blob/develop-spark3/docs/Reference.md#data-objects"}),"this list")," for an overview.\nYou can also consult the ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://smartdatalake.ch/docs/site/scaladocs/io/smartdatalake/workflow/dataobject/index.html"}),"API docs")," to see how to use all those Data Objects."),Object(i.b)("p",null,"In a first step, we want to make the airport data more understandable by removing any columns we don't need.\nSince we don't introduce any business logic into the transformation,\nthe resulting data object will reside in the integration layer and thus will be called ",Object(i.b)("em",{parentName:"p"},"int-airports"),".\nPut this in the existing dataObjects section:"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{}),'  int-airports {\n    type = CsvFileDataObject\n    path = "~{id}"\n  }\n')),Object(i.b)("h2",{id:"define-select-airport-cols-action"},"Define select-airport-cols action"),Object(i.b)("p",null,"Next, add these lines in the existing actions section:"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{}),'  select-airport-cols {\n    type = CopyAction\n    inputId = stg-airports\n    outputId = int-airports\n    transformers = [{\n        type = SQLDfTransformer\n        code = "select ident, name, latitude_deg, longitude_deg from stg_airports"\n    }]\n    metadata {\n      feed = compute\n    }\n  }\n')),Object(i.b)("p",null,"A couple of things to note here:"),Object(i.b)("ul",null,Object(i.b)("li",{parentName:"ul"},"We just defined a new action called ",Object(i.b)("em",{parentName:"li"},"select-airport-cols"),". "),Object(i.b)("li",{parentName:"ul"},"We wired it together with the two DataObjects ",Object(i.b)("em",{parentName:"li"},"stg-airports")," and ",Object(i.b)("em",{parentName:"li"},"int-airports"),"."),Object(i.b)("li",{parentName:"ul"},"A new type of Action was used: CopyAction. This action is intended to copy data from one data object to another\nwith some optional transformations of the data along the way."),Object(i.b)("li",{parentName:"ul"},"To define the transformations of an action, you define a list of HOCON Objects.\nHOCON-Objects are just like JSON-Objects (with a few added features, but more on that later)."),Object(i.b)("li",{parentName:"ul"},"Instead of allowing for just one transformer, we could potentially have multiple transformers within the same action that\nget executed one after the other. That's why we have the bracket followed by the curly brace \"[{\" :\nthe CustomSparkAction expects it's field ",Object(i.b)("em",{parentName:"li"},"transformers")," to be a list of HOCON Objects."),Object(i.b)("li",{parentName:"ul"},"There's different kinds of transformers, in this case we defined a ",Object(i.b)("em",{parentName:"li"},"SQLDfTransformer")," and provided it with a custom SQL-Code.\nThere are other transformer types such as ",Object(i.b)("em",{parentName:"li"},"ScalaCodeDfTransformer"),", ",Object(i.b)("em",{parentName:"li"},"PythonCodeDfTransformer"),"... More on that later.")),Object(i.b)("div",{className:"admonition admonition-caution alert alert--warning"},Object(i.b)("div",Object(n.a)({parentName:"div"},{className:"admonition-heading"}),Object(i.b)("h5",{parentName:"div"},Object(i.b)("span",Object(n.a)({parentName:"h5"},{className:"admonition-icon"}),Object(i.b)("svg",Object(n.a)({parentName:"span"},{xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",viewBox:"0 0 16 16"}),Object(i.b)("path",Object(n.a)({parentName:"svg"},{fillRule:"evenodd",d:"M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"})))),"caution")),Object(i.b)("div",Object(n.a)({parentName:"div"},{className:"admonition-content"}),Object(i.b)("p",{parentName:"div"},'Notice that we call our input DataObject stg-airports with a hyphen "-", but in the sql, we call it "stg',"_",'airports" with an underscore "',"_",'".\nThis is due to the SQL standard not allowing "-" in unquoted identifiers (e.g. table names).\nUnder the hood, Apache Spark SQL is used to execute the query, which implements SQL standard.\nSDL works around this by replacing special chars in DataObject names used in SQL statements for you.\nIn this case, it automatically replaced "-" with "_"'))),Object(i.b)("p",null,"There are numerous other options available for the CopyAction, which you can view in the ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"http://smartdatalake.ch/docs/site/scaladocs/io/smartdatalake/workflow/action/CopyAction.html"}),"API Docs"),"."),Object(i.b)("h2",{id:"try-it-out"},"Try it out"),Object(i.b)("p",null,"Note that we used a different feed this time, we called it ",Object(i.b)("em",{parentName:"p"},"compute"),".\nWe will keep expanding the feed ",Object(i.b)("em",{parentName:"p"},"compute")," in the next few steps.\nThis allows us to keep the data we downloaded in the previous steps in our local files and just\ntry out our new actions."),Object(i.b)("p",null,"To execute the pipeline, use the same command as before, but change the feed to compute:"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{}),"docker run --rm -v ${PWD}/data:/mnt/data -v ${PWD}/config:/mnt/config smart-data-lake/gs1:latest --config /mnt/config --feed-sel compute\n")),Object(i.b)("p",null,"Now you should see multiple files in the folder ",Object(i.b)("em",{parentName:"p"},"data/int-airports"),". Why is it split accross multiple files?\nThis is due to the fact that the query runs with Apache Spark under the hood which computes the query in parallel for different portions of the data.\nWe might work on a small data set for now, but keep in mind that this would scale up horizontally for large amounts of data."),Object(i.b)("h2",{id:"more-on-feeds"},"More on Feeds"),Object(i.b)("p",null,"SDL gives you precise control on which actions you want to execute.\nFor instance if you only want to execute the action that we just wrote, you can type"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{}),"docker run --rm -v ${PWD}/data:/mnt/data -v ${PWD}/config:/mnt/config smart-data-lake/gs1:latest --config /mnt/config --feed-sel ids:select-airport-cols\n")),Object(i.b)("p",null,"Of course, at this stage, the feed ",Object(i.b)("em",{parentName:"p"},"compute")," only contains this one action, so the result will be the same."),Object(i.b)("p",null,"SDL also allows you to use combinations of expressions to select the actions you want to execute. You can run"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{}),"docker run --rm smart-data-lake/gs1:latest --help\n")),Object(i.b)("p",null,"to see all options that are available. For your convenience, here is the current output of the help command:"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{}),"Usage: LocalSmartDataLakeBuilder [options]\n\n  -f, --feed-sel <value>   Select actions to execute by one or multiple expressions separated by semicolon (;). Results from multiple expressions are combined from left to right.\n                           Expression syntax: \"<operation?><prefix:?><regex>\"\n                           Operations:\n                           - pipe symbol (|): the two sets are combined by union operation (default)\n                           - ampersand symbol (&): the two sets are combined by intersection operation\n                           - minus symbol (-): the second set is subtracted from the first set\n                           Prefixes:\n                           - 'feeds': select actions where metadata.feed is matched by regex pattern (default)\n                           - 'names': select actions where metadata.name is matched by regex pattern\n                           - 'ids': select actions where id is matched by regex pattern\n                           - 'layers': select actions where metadata.layer of all output DataObjects is matched by regex pattern\n                           - 'startFromActionIds': select actions which with id is matched by regex pattern and any dependent action (=successors)\n                           - 'endWithActionIds': select actions which with id is matched by regex pattern and their predecessors\n                           - 'startFromDataObjectIds': select actions which have an input DataObject with id is matched by regex pattern and any dependent action (=successors)\n                           - 'endWithDataObjectIds': select actions which have an output DataObject with id is matched by regex pattern and their predecessors\n                           All matching is done case-insensitive.\n                           Example: to filter action 'A' and its successors but only in layer L1 and L2, use the following pattern: \"startFromActionIds:a;&layers:(l1|l2)\"\n  -n, --name <value>       Optional name of the application. If not specified feed-sel is used.\n  -c, --config <value>     One or multiple configuration files or directories containing configuration files, separated by comma. Entries must be valid Hadoop URIs or a special URI with scheme \"cp\" which is treated as classpath entr\ny.\n  --partition-values <value>\n                           Partition values to process in format <partitionColName>=<partitionValue>[,<partitionValue>,...].\n  --multi-partition-values <value>\n                           Multi partition values to process in format <partitionColName1>=<partitionValue>,<partitionColName2>=<partitionValue>[;(<partitionColName1>=<partitionValue>,<partitionColName2>=<partitionValue>;...].\n  --parallelism <value>    Parallelism for DAG run.\n  --state-path <value>     Path to save run state files. Must be set to enable recovery in case of failures.\n  --override-jars <value>  Comma separated list of jars for child-first class loader. The jars must be present in classpath.\n  --test <value>           Run in test mode: config -> validate configuration, dry-run -> execute prepare- and init-phase only to check environment and spark lineage\n  --help                   Display the help text.\n  --version                Display version information.\n  -m, --master <value>     The Spark master URL passed to SparkContext (default=local[*], yarn, spark://HOST:PORT, mesos://HOST:PORT, k8s://HOST:PORT).\n  -x, --deploy-mode <value>\n                           The Spark deploy mode passed to SparkContext (default=client, cluster).\n  -d, --kerberos-domain <value>\n                           Kerberos-Domain for authentication (USERNAME@KERBEROS-DOMAIN) in local mode.\n  -u, --username <value>   Kerberos username for authentication (USERNAME@KERBEROS-DOMAIN) in local mode.\n")),Object(i.b)("p",null,"One popular option is to use regular expressions to execute multiple feeds together.\nIn our case, we can run the entire data pipeline with the following command : "),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{}),"docker run --rm -v ${PWD}/data:/mnt/data -v ${PWD}/config:/mnt/config smart-data-lake/gs1:latest --config /mnt/config --feed-sel .*\n")),Object(i.b)("div",{className:"admonition admonition-caution alert alert--warning"},Object(i.b)("div",Object(n.a)({parentName:"div"},{className:"admonition-heading"}),Object(i.b)("h5",{parentName:"div"},Object(i.b)("span",Object(n.a)({parentName:"h5"},{className:"admonition-icon"}),Object(i.b)("svg",Object(n.a)({parentName:"span"},{xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",viewBox:"0 0 16 16"}),Object(i.b)("path",Object(n.a)({parentName:"svg"},{fillRule:"evenodd",d:"M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"})))),"caution")),Object(i.b)("div",Object(n.a)({parentName:"div"},{className:"admonition-content"}),Object(i.b)("p",{parentName:"div"},"In our tutorial, this command will only work if you already have some files under ",Object(i.b)("em",{parentName:"p"},"data/stg-airports")," and data/stg-departures.\nThis is because in the first step, we download files of which SDL doesn't know the schema of in advance.\nThe init-phase will require that for all Data Objects, the schema is known so that it can check for inconsistencies.\nWhen we already have some files, it will infer the schema based on the files."),Object(i.b)("p",{parentName:"div"},"To work around this, execute the feed download again. After that feed was successfully executed, the execution of\nthe feed .* will work. "),Object(i.b)("p",{parentName:"div"},"One way to prevent this problem is to explicitly provide the schema for the JSON and for the CSV-File,\nwhich is out of the scope for this part of the tutorial."))),Object(i.b)("h2",{id:"example-of-common-mistake"},"Example of Common Mistake"),Object(i.b)("p",null,"One common mistake is mixing up the types of Data Objects.\nTo give you some experience on how to debug your config, you can also try out what happens if you change the type of ",Object(i.b)("em",{parentName:"p"},"stg-airports")," to JsonFileDataObject.\nYou will get an error message which indicates that there might be some format problem, but it is hard to spot :"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{})," Error: cannot resolve '`ident`' given input columns: [stg_airports._corrupt_record]; line 1 pos 7;\n")),Object(i.b)("p",null,"The FileTransferAction will save the result from the Webservice with the JsonFileDataObject as file with filetype ","*",".json.\nThen Spark tries to parse the CSV-records in the ","*",".json file with a JSON-Parser. It is unable to properly read the data.\nHowever, it generates a column named ",Object(i.b)("em",{parentName:"p"},"_corrupt_record")," describing what went wrong.\nIf you know Apache Spark, this column will look very familiar to you.\nAfter that, the query fails, because it only finds that column with error messages instead of the actual data."),Object(i.b)("p",null,"One way to get a better error message is to tell Spark that it should promptly fail when reading a corrupt file.\nYou can do that with the option ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://smartdatalake.ch/docs/site/scaladocs/io/smartdatalake/workflow/dataobject/JsonFileDataObject.html"}),"jsonOptions"),",\nwhich allows you to directly pass on settings to Spark."),Object(i.b)("p",null,"In our case, we would end up with a faulty dataObject that looks like this:"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{}),'  stg-airports {\n    type = JsonFileDataObject\n    path = "~{id}"\n    jsonOptions {\n      "mode"="failfast"\n    }\n  }\n')),Object(i.b)("p",null,"This time, it will fail with this error message:"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{}),"Exception in thread \"main\" io.smartdatalake.workflow.TaskFailedException: Task select-airport-cols failed. \nRoot cause is 'SparkException: Malformed records are detected in schema inference. \nParse Mode: FAILFAST. Reasons: Failed to infer a common schema. Struct types are expected, but `string` was found.'\n")))}p.isMDXComponent=!0},96:function(e,t,a){"use strict";a.d(t,"a",(function(){return d})),a.d(t,"b",(function(){return b}));var n=a(0),o=a.n(n);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function s(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){i(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function c(e,t){if(null==e)return{};var a,n,o=function(e,t){if(null==e)return{};var a,n,o={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(o[a]=e[a]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(o[a]=e[a])}return o}var l=o.a.createContext({}),p=function(e){var t=o.a.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):s(s({},t),e)),a},d=function(e){var t=p(e.components);return o.a.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return o.a.createElement(o.a.Fragment,{},t)}},m=o.a.forwardRef((function(e,t){var a=e.components,n=e.mdxType,i=e.originalType,r=e.parentName,l=c(e,["components","mdxType","originalType","parentName"]),d=p(a),m=n,b=d["".concat(r,".").concat(m)]||d[m]||u[m]||i;return a?o.a.createElement(b,s(s({ref:t},l),{},{components:a})):o.a.createElement(b,s({ref:t},l))}));function b(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var i=a.length,r=new Array(i);r[0]=m;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s.mdxType="string"==typeof e?e:n,r[1]=s;for(var l=2;l<i;l++)r[l]=a[l];return o.a.createElement.apply(null,r)}return o.a.createElement.apply(null,a)}m.displayName="MDXCreateElement"}}]);