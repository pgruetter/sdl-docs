{
	"type" : "object",
	"version":"1.0.0",
	"id": "schema.json#",
	"additionalProperties" : true,
	"required" : [ "dataObjects", "actions" ],
	"properties" : {
	  "global" : {
	    "type" : "object",
	    "description" : "Global configuration options",
	    "additionalProperties" : false,
	    "properties" : {
	      "kryoClasses" : {
		"type" : "array",
		"description" : "classes to register for spark kryo serialization",
		"items" : {
		  "type" : "string"
		}
	      },
	      "sparkOptions" : {
		"type" : "object",
		"description" : "spark options",
		"additionalProperties" : {
		  "type" : "string"
		}
	      },
	      "enableHive" : {
		"type" : "string",
		"description" : "enable hive for spark session"
	      },
	      "memoryLogTimer" : {
		"type" : "object",
		"description" : "Configuration for periodic memory usage logging",
		"additionalProperties" : false,
		"required" : [ "intervalSec" ],
		"properties" : {
		  "intervalSec" : {
		    "type" : "integer",
		    "description" : "interval in seconds between memory usage logs"
		  },
		  "logLinuxMem" : {
		    "type" : "string",
		    "description" : "enable logging linux memory"
		  },
		  "logLinuxCGroupMem" : {
		    "type" : "string",
		    "description" : "enable logging details about linux cgroup memory"
		  },
		  "logBuffers" : {
		    "type" : "string",
		    "description" : "enable logging details about different jvm buffers"
		  }
		}
	      },
	      "shutdownHookLogger" : {
		"type" : "string",
		"description" : "enable shutdown hook logger to trace shutdown cause"
	      },
	      "stateListeners" : {
		"type" : "array",
		"description" : "Define state listeners to be registered for receiving events of the execution of SmartDataLake job",
		"items" : {
		  "type" : "object",
		  "description" : "Configuration to notify interested parties about action results & metric",
		  "additionalProperties" : false,
		  "required" : [ "className" ],
		  "properties" : {
		    "className" : {
		      "type" : "string",
		      "description" : "fully qualified class name of class implementing StateListener interface. The class needs a constructor with one parameter`options: Map[String,String]` ."
		    },
		    "options" : {
		      "type" : "object",
		      "description" : "Options are passed to StateListener constructor.",
		      "additionalProperties" : {
			"type" : "string"
		      }
		    }
		  }
		}
	      },
	      "sparkUDFs" : {
		"type" : "object",
		"description" : "Define UDFs to be registered in spark session. The registered UDFs are available in Spark SQL transformations\r\nand expression evaluation, e.g. configuration of ExecutionModes.",
		"additionalProperties" : {
		  "type" : "object",
		  "description" : "Configuration to register a UserDefinedFunction in the spark session of SmartDataLake.",
		  "additionalProperties" : false,
		  "required" : [ "className" ],
		  "properties" : {
		    "className" : {
		      "type" : "string",
		      "description" : "fully qualified class name of class implementing SparkUDFCreator interface. The class needs a constructor without parameters."
		    },
		    "options" : {
		      "type" : "object",
		      "description" : "Options are passed to SparkUDFCreator apply method.",
		      "additionalProperties" : {
			"type" : "string"
		      }
		    }
		  }
		}
	      },
	      "pythonUDFs" : {
		"type" : "object",
		"description" : "Define UDFs in python to be registered in spark session. The registered UDFs are available in Spark SQL transformations\r\nbut not for expression evaluation.",
		"additionalProperties" : {
		  "type" : "object",
		  "description" : "Configuration to register a Python UDF in the spark session of SmartDataLake.\r\nDefine a python function with type hints i python code and register it in global configuration.\r\nThe name of the function must match the name you use to declare it in GlobalConf.\r\nThe Python function can then be used in Spark SQL expressions.",
		  "additionalProperties" : false,
		  "properties" : {
		    "pythonFile" : {
		      "type" : "string",
		      "description" : "Optional pythonFile to use for python UDF."
		    },
		    "pythonCode" : {
		      "type" : "string",
		      "description" : "Optional pythonCode to use for python UDF."
		    },
		    "options" : {
		      "type" : "object",
		      "description" : "Options are available in your python code as variable options.",
		      "additionalProperties" : {
			"type" : "string"
		      }
		    }
		  }
		}
	      },
	      "secretProviders" : {
		"type" : "object",
		"description" : "Define SecretProvider\\'s to be registered.",
		"additionalProperties" : {
		  "type" : "object",
		  "description" : "Configuration to register a SecretProvider.",
		  "additionalProperties" : false,
		  "required" : [ "className" ],
		  "properties" : {
		    "className" : {
		      "type" : "string",
		      "description" : "fully qualified class name of class implementing SecretProvider interface. The class needs a constructor with parameter \\\"options: Map[String,String]\\\"."
		    },
		    "options" : {
		      "type" : "object",
		      "description" : "Options are passed to SecretProvider apply method.",
		      "additionalProperties" : {
			"type" : "string"
		      }
		    }
		  }
		}
	      },
	      "allowOverwriteAllPartitionsWithoutPartitionValues" : {
		"type" : "array",
		"description" : "Configure a list of exceptions for partitioned DataObject id\\'s,\r\nwhich are allowed to overwrite the all partitions of a table if no partition values are set.\r\nThis is used to override/avoid a protective error when using SDLSaveMode.OverwriteOptimized|OverwritePreserveDirectories.\r\nDefine it as a list of DataObject id\\'s.",
		"items" : {
		  "type" : "string"
		}
	      },
	      "runtimeDataNumberOfExecutionsToKeep" : {
		"type" : "integer",
		"description" : "Number of Executions to keep runtime data for in streaming mode (default = 10).\r\nMust be bigger than 1."
	      },
	      "synchronousStreamingTriggerIntervalSec" : {
		"type" : "integer",
		"description" : "Trigger interval for synchronous actions in streaming mode in seconds (default = 60 seconds)\r\nThe synchronous actions of the DAG will be executed with this interval if possile.\r\nNote that for asynchronous actions there are separate settings, e.g. SparkStreamingMode.triggerInterval."
	      }
	    }
	  },
	  "dataObjects" : {
	    "type" : "object",
	    "additionalProperties" : {
	      "oneOf" : [ {
		"$ref" : "#/definitions/ExcelFileDataObject"
	      }, {
		"$ref" : "#/definitions/RelaxedCsvFileDataObject"
	      }, {
		"$ref" : "#/definitions/CustomFileDataObject"
	      }, {
		"$ref" : "#/definitions/DeltaLakeTableDataObject"
	      }, {
		"$ref" : "#/definitions/DataObjectsExporterDataObject"
	      }, {
		"$ref" : "#/definitions/CustomDfDataObject"
	      }, {
		"$ref" : "#/definitions/JdbcTableDataObject"
	      }, {
		"$ref" : "#/definitions/AccessTableDataObject"
	      }, {
		"$ref" : "#/definitions/HiveTableDataObject"
	      }, {
		"$ref" : "#/definitions/ParquetFileDataObject"
	      }, {
		"$ref" : "#/definitions/WebserviceFileDataObject"
	      }, {
		"$ref" : "#/definitions/CsvFileDataObject"
	      }, {
		"$ref" : "#/definitions/RawFileDataObject"
	      }, {
		"$ref" : "#/definitions/ActionsExporterDataObject"
	      }, {
		"$ref" : "#/definitions/AvroFileDataObject"
	      }, {
		"$ref" : "#/definitions/PKViolatorsDataObject"
	      }, {
		"$ref" : "#/definitions/SFtpFileRefDataObject"
	      }, {
		"$ref" : "#/definitions/XmlFileDataObject"
	      }, {
		"$ref" : "#/definitions/TickTockHiveTableDataObject"
	      }, {
		"$ref" : "#/definitions/JsonFileDataObject"
	      } ]
	    }
	  },
	  "actions" : {
	    "type" : "object",
	    "additionalProperties" : {
	      "oneOf" : [ {
		"$ref" : "#/definitions/FileTransferAction"
	      }, {
		"$ref" : "#/definitions/CopyAction"
	      }, {
		"$ref" : "#/definitions/HistorizeAction"
	      }, {
		"$ref" : "#/definitions/CustomFileAction"
	      }, {
		"$ref" : "#/definitions/CustomSparkAction"
	      }, {
		"$ref" : "#/definitions/DeduplicateAction"
	      }, {
		"$ref" : "#/definitions/CustomScriptAction"
	      } ]
	    }
	  },
	  "connections" : {
	    "type" : "object",
	    "additionalProperties" : {
	      "oneOf" : [ {
		"$ref" : "#/definitions/DeltaLakeTableConnection"
	      }, {
		"$ref" : "#/definitions/JdbcTableConnection"
	      }, {
		"$ref" : "#/definitions/HiveTableConnection"
	      }, {
		"$ref" : "#/definitions/SftpFileRefConnection"
	      }, {
		"$ref" : "#/definitions/HadoopFileConnection"
	      } ]
	    }
	  }
	},
	"definitions" : {
	  "HiveTableDataObject" : {
	    "type" : "object",
	    "description" : "[[DataObject]] of type Hive.\r\nProvides details to access Hive tables to an Action",
	    "additionalProperties" : false,
	    "required" : [ "id", "table" ],
	    "properties" : {
	      "id" : {
		"type" : "string",
		"description" : "unique name of this data object"
	      },
	      "path" : {
		"type" : "string",
		"description" : "hadoop directory for this table. If it doesn\\'t contain scheme and authority, the connections pathPrefix is applied.\r\nIf pathPrefix is not defined or doesn\\'t define scheme and authority, default schema and authority is applied.\r\nIf DataObject is only used for reading or if the HiveTable already exist, the path can be omitted.\r\nIf the HiveTable already exists but with a different path, a warning is issued"
	      },
	      "partitions" : {
		"type" : "array",
		"description" : "partition columns for this data object",
		"items" : {
		  "type" : "string"
		}
	      },
	      "analyzeTableAfterWrite" : {
		"type" : "string",
		"description" : "enable compute statistics after writing data (default=false)"
	      },
	      "dateColumnType" : {
		"type" : "string",
		"enum" : [ "Default ", "String ", "Date " ],
		"description" : "type of date column"
	      },
	      "schemaMin" : {
		"type" : "string",
		"description" : "An optional, minimal schema that this DataObject must have to pass schema validation on reading and writing."
	      },
	      "table" : {
		"oneOf" : [ {
		  "$ref" : "#/definitions/Table"
		} ]
	      },
	      "numInitialHdfsPartitions" : {
		"type" : "integer",
		"description" : "number of files created when writing into an empty table (otherwise the number will be derived from the existing data)"
	      },
	      "saveMode" : {
		"type" : "string",
		"enum" : [ "Merge ", "OverwriteOptimized ", "OverwritePreserveDirectories ", "Ignore ", "ErrorIfExists ", "Append ", "Overwrite " ],
		"description" : "spark[[SaveMode]] to use when writing files, default is \\\"overwrite\\\""
	      },
	      "acl" : {
		"type" : "object",
		"description" : "Describes a complete ACL Specification (basic owner/group/other permissions AND extended ACLS)\r\nto be applied to a Data Object on writing",
		"additionalProperties" : false,
		"required" : [ "permission", "acls" ],
		"properties" : {
		  "permission" : {
		    "type" : "string",
		    "description" : ": File system permission string in symbolic notation form (e.g. rwxr-xr-x)"
		  },
		  "acls" : {
		    "type" : "array",
		    "description" : ": a sequence of[[AclElement]] s",
		    "items" : {
		      "type" : "object",
		      "description" : "Describes a single extended ACL to be applied to a Data Object\r\nin addition to the basic file system permissions",
		      "additionalProperties" : false,
		      "required" : [ "aclType", "name", "permission" ],
		      "properties" : {
			"aclType" : {
			  "type" : "string",
			  "description" : ": type of ACL to be added \\\"group\\\", \\\"user\\\""
			},
			"name" : {
			  "type" : "string",
			  "description" : ": the name of the user/group for which an ACL definition is being added"
			},
			"permission" : {
			  "type" : "string",
			  "description" : ": the permission (rwx syntax) to be granted"
			}
		      }
		    }
		  }
		}
	      },
	      "connectionId" : {
		"type" : "string",
		"description" : "optional id of[[io.smartdatalake.workflow.connection.HiveTableConnection]]"
	      },
	      "expectedPartitionsCondition" : {
		"type" : "string",
		"description" : "Optional definition of partitions expected to exist.\r\nDefine a Spark SQL expression that is evaluated against a[[PartitionValues]] instance and returns true or false\r\nDefault is to expect all partitions to exist."
	      },
	      "housekeepingMode" : {
		"oneOf" : [ {
		  "type" : "object",
		  "description" : "Archive and compact old partitions:\r\nArchive partition reduces the number of partitions in the past by moving older partitions into special \\\"archive partitions\\\".\r\nCompact partition reduces the number of files in a partition by rewriting them with Spark.\r\nExample: archive and compact a table with partition layout run_id=<integer>\r\n- archive partitions after 1000 partitions into \\\"archive partition\\\" equal to floor(run_id/1000)\r\n- compact \\\"archive partition\\\" when full\r\n{{{\r\nhousekeepingMode = {\r\ntype = PartitionArchiveCompactionMode\r\narchivePartitionExpression = \\\"if( elements[\\'run_id\\'] < runId - 1000, map(\\'run_id\\', elements[\\'run_id\\'] div 1000), elements)\\\"\r\ncompactPartitionExpression = \\\"elements[\\'run_id\\'] % 1000 = 0 and elements[\\'run_id\\'] <= runId - 2000\\\"\r\n}\r\n}}}",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "PartitionArchiveCompactionMode"
		    },
		    "archivePartitionExpression" : {
		      "type" : "string",
		      "description" : "Expression to define the archive partition for a given partition. Define a spark\r\nsql expression working with the attributes of[[PartitionExpressionData]] returning archive\r\npartition values as Map[String,String]. If return value is the same as input elements, partition is not touched,\r\notherwise all files of the partition are moved to the returned partition definition.\r\nBe aware that the value of the partition columns changes for these files/records."
		    },
		    "compactPartitionExpression" : {
		      "type" : "string",
		      "description" : "Expression to define partitions which should be compacted. Define a spark\r\nsql expression working with the attributes of[[PartitionExpressionData]] returning a\r\nboolean = true when this partition should be compacted.\r\nOnce a partition is compacted, it is marked as compacted and will not be compacted again.\r\nIt is therefore ok to return true for all partitions which should be compacted, regardless if they have been compacted already."
		    },
		    "description" : {
		      "type" : "string"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Keep partitions while retention condition is fulfilled, delete other partitions.\r\nExample: cleanup partitions with partition layout dt=<yyyymmdd> after 90 days:\r\n{{{\r\nhousekeepingMode = {\r\ntype = PartitionRetentionMode\r\nretentionCondition = \\\"datediff(now(), to_date(elements[\\'dt\\'], \\'yyyyMMdd\\')) <= 90\\\"\r\n}\r\n}}}",
		  "additionalProperties" : false,
		  "required" : [ "retentionCondition", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "PartitionRetentionMode"
		    },
		    "retentionCondition" : {
		      "type" : "string",
		      "description" : "Condition to decide if a partition should be kept. Define a spark sql expression\r\nworking with the attributes of[[PartitionExpressionData]] returning a boolean with value true if the partition should be kept."
		    },
		    "description" : {
		      "type" : "string"
		    }
		  }
		} ]
	      },
	      "metadata" : {
		"oneOf" : [ {
		  "$ref" : "#/definitions/DataObjectMetadata"
		} ]
	      }
	    }
	  },
	  "AccessTableDataObject" : {
	    "type" : "object",
	    "description" : "[[DataObject]]of type JDBC / Access.\r\nProvides access to a Access DB to an Action. The functionality is handled seperately from[[JdbcTableDataObject]] \r\nto avoid problems with net.ucanaccess.jdbc.UcanaccessDriver",
	    "additionalProperties" : false,
	    "required" : [ "id", "path", "table" ],
	    "properties" : {
	      "id" : {
		"type" : "string"
	      },
	      "path" : {
		"type" : "string"
	      },
	      "schemaMin" : {
		"type" : "string"
	      },
	      "table" : {
		"oneOf" : [ {
		  "$ref" : "#/definitions/Table"
		} ]
	      },
	      "metadata" : {
		"oneOf" : [ {
		  "$ref" : "#/definitions/DataObjectMetadata"
		} ]
	      }
	    }
	  },
	  "RawFileDataObject" : {
	    "type" : "object",
	    "description" : "DataObject of type raw for files with unknown content.\r\nProvides details to an Action to access raw files.\r\nBy specifying format you can custom Spark data formats",
	    "additionalProperties" : false,
	    "required" : [ "id", "path" ],
	    "properties" : {
	      "id" : {
		"type" : "string"
	      },
	      "path" : {
		"type" : "string"
	      },
	      "customFormat" : {
		"type" : "string",
		"description" : "Custom Spark data source format, e.g. binaryFile or text. Only needed if you want to read/write this DataObject with Spark."
	      },
	      "options" : {
		"type" : "object",
		"description" : "Options for custom Spark data source format. Only of use if you want to read/write this DataObject with Spark.",
		"additionalProperties" : {
		  "type" : "string"
		}
	      },
	      "fileName" : {
		"type" : "string",
		"description" : "Definition of fileName. This is concatenated with path and partition layout to search for files. Default is an asterix to match everything."
	      },
	      "partitions" : {
		"type" : "array",
		"items" : {
		  "type" : "string"
		}
	      },
	      "schema" : {
		"type" : "string"
	      },
	      "schemaMin" : {
		"type" : "string"
	      },
	      "saveMode" : {
		"type" : "string",
		"enum" : [ "Merge ", "OverwriteOptimized ", "OverwritePreserveDirectories ", "Ignore ", "ErrorIfExists ", "Append ", "Overwrite " ],
		"description" : "Overwrite or Append new data."
	      },
	      "sparkRepartition" : {
		"type" : "object",
		"description" : "This controls repartitioning of the DataFrame before writing with Spark to Hadoop.\r\nWhen writing multiple partitions of a partitioned DataObject, the number of spark tasks created is equal to numberOfTasksPerPartition\r\nmultiplied with the number of partitions to write. To spread the records of a partition only over numberOfTasksPerPartition spark tasks,\r\nkeyCols must be given which are used to derive a task number inside the partition (hashvalue(keyCols) modulo numberOfTasksPerPartition).\r\nWhen writing to an unpartitioned DataObject or only one partition of a partitioned DataObject, the number of spark tasks created is equal\r\nto numberOfTasksPerPartition. Optional keyCols can be used to keep corresponding records together in the same task/file.",
		"additionalProperties" : false,
		"required" : [ "numberOfTasksPerPartition" ],
		"properties" : {
		  "numberOfTasksPerPartition" : {
		    "type" : "integer",
		    "description" : "Number of Spark tasks to create per partition before writing to DataObject by repartitioning the DataFrame.\r\nThis controls how many files are created in each Hadoop partition."
		  },
		  "keyCols" : {
		    "type" : "array",
		    "description" : "Optional key columns to distribute records over Spark tasks inside a Hadoop partition.\r\nIf DataObject has Hadoop partitions defined, keyCols must be defined.",
		    "items" : {
		      "type" : "string"
		    }
		  },
		  "sortCols" : {
		    "type" : "array",
		    "description" : "Optional columns to sort records inside files created.",
		    "items" : {
		      "type" : "string"
		    }
		  },
		  "filename" : {
		    "type" : "string",
		    "description" : "Option filename to rename target file(s). If numberOfTasksPerPartition is greater than 1,\r\nmultiple files can exist in a directory and a number is inserted into the filename after the first \\'.\\'.\r\nExample: filename=data.csv -> files created are data.1.csv, data.2.csv, ..."
		  }
		}
	      },
	      "acl" : {
		"type" : "object",
		"description" : "Describes a complete ACL Specification (basic owner/group/other permissions AND extended ACLS)\r\nto be applied to a Data Object on writing",
		"additionalProperties" : false,
		"required" : [ "permission", "acls" ],
		"properties" : {
		  "permission" : {
		    "type" : "string",
		    "description" : ": File system permission string in symbolic notation form (e.g. rwxr-xr-x)"
		  },
		  "acls" : {
		    "type" : "array",
		    "description" : ": a sequence of[[AclElement]] s",
		    "items" : {
		      "type" : "object",
		      "description" : "Describes a single extended ACL to be applied to a Data Object\r\nin addition to the basic file system permissions",
		      "additionalProperties" : false,
		      "required" : [ "aclType", "name", "permission" ],
		      "properties" : {
			"aclType" : {
			  "type" : "string",
			  "description" : ": type of ACL to be added \\\"group\\\", \\\"user\\\""
			},
			"name" : {
			  "type" : "string",
			  "description" : ": the name of the user/group for which an ACL definition is being added"
			},
			"permission" : {
			  "type" : "string",
			  "description" : ": the permission (rwx syntax) to be granted"
			}
		      }
		    }
		  }
		}
	      },
	      "connectionId" : {
		"type" : "string"
	      },
	      "filenameColumn" : {
		"type" : "string"
	      },
	      "expectedPartitionsCondition" : {
		"type" : "string",
		"description" : "Optional definition of partitions expected to exist.\r\nDefine a Spark SQL expression that is evaluated against a[[PartitionValues]] instance and returns true or false\r\nDefault is to expect all partitions to exist."
	      },
	      "housekeepingMode" : {
		"oneOf" : [ {
		  "type" : "object",
		  "description" : "Archive and compact old partitions:\r\nArchive partition reduces the number of partitions in the past by moving older partitions into special \\\"archive partitions\\\".\r\nCompact partition reduces the number of files in a partition by rewriting them with Spark.\r\nExample: archive and compact a table with partition layout run_id=<integer>\r\n- archive partitions after 1000 partitions into \\\"archive partition\\\" equal to floor(run_id/1000)\r\n- compact \\\"archive partition\\\" when full\r\n{{{\r\nhousekeepingMode = {\r\ntype = PartitionArchiveCompactionMode\r\narchivePartitionExpression = \\\"if( elements[\\'run_id\\'] < runId - 1000, map(\\'run_id\\', elements[\\'run_id\\'] div 1000), elements)\\\"\r\ncompactPartitionExpression = \\\"elements[\\'run_id\\'] % 1000 = 0 and elements[\\'run_id\\'] <= runId - 2000\\\"\r\n}\r\n}}}",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "PartitionArchiveCompactionMode"
		    },
		    "archivePartitionExpression" : {
		      "type" : "string",
		      "description" : "Expression to define the archive partition for a given partition. Define a spark\r\nsql expression working with the attributes of[[PartitionExpressionData]] returning archive\r\npartition values as Map[String,String]. If return value is the same as input elements, partition is not touched,\r\notherwise all files of the partition are moved to the returned partition definition.\r\nBe aware that the value of the partition columns changes for these files/records."
		    },
		    "compactPartitionExpression" : {
		      "type" : "string",
		      "description" : "Expression to define partitions which should be compacted. Define a spark\r\nsql expression working with the attributes of[[PartitionExpressionData]] returning a\r\nboolean = true when this partition should be compacted.\r\nOnce a partition is compacted, it is marked as compacted and will not be compacted again.\r\nIt is therefore ok to return true for all partitions which should be compacted, regardless if they have been compacted already."
		    },
		    "description" : {
		      "type" : "string"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Keep partitions while retention condition is fulfilled, delete other partitions.\r\nExample: cleanup partitions with partition layout dt=<yyyymmdd> after 90 days:\r\n{{{\r\nhousekeepingMode = {\r\ntype = PartitionRetentionMode\r\nretentionCondition = \\\"datediff(now(), to_date(elements[\\'dt\\'], \\'yyyyMMdd\\')) <= 90\\\"\r\n}\r\n}}}",
		  "additionalProperties" : false,
		  "required" : [ "retentionCondition", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "PartitionRetentionMode"
		    },
		    "retentionCondition" : {
		      "type" : "string",
		      "description" : "Condition to decide if a partition should be kept. Define a spark sql expression\r\nworking with the attributes of[[PartitionExpressionData]] returning a boolean with value true if the partition should be kept."
		    },
		    "description" : {
		      "type" : "string"
		    }
		  }
		} ]
	      },
	      "metadata" : {
		"oneOf" : [ {
		  "$ref" : "#/definitions/DataObjectMetadata"
		} ]
	      }
	    }
	  },
	  "PKViolatorsDataObject" : {
	    "type" : "object",
	    "description" : "Checks for Primary Key violations for all[[DataObject]]s with Primary Keys defined that are registered in the current[[InstanceRegistry]].\r\nReturns the list of Primary Key violations as a[[DataFrame]].\r\nAlternatively, it can check for Primary Key violations of all[[DataObject]]s defined in config files. For this, the\r\nconfiguration \\\"config\\\" has to be set to the location of the config.\r\nExample:\r\n{{{\r\n```dataObjects = {\r\n...\r\nprimarykey-violations {\r\ntype = PKViolatorsDataObject\r\nconfig = path/to/myconfiguration.conf\r\n}\r\n...\r\n}\r\n}}}\n\nSEE: Refer to[[ConfigLoader.loadConfigFromFilesystem()]] for details about the configuration loading.",
	    "additionalProperties" : false,
	    "required" : [ "id" ],
	    "properties" : {
	      "id" : {
		"type" : "string"
	      },
	      "config" : {
		"type" : "string",
		"description" : ": The config value can point to a configuration file or a directory containing configuration files."
	      },
	      "flattenOutput" : {
		"type" : "string",
		"description" : ": if true, key and data column are converted from type map<k,v> to string (default).\r\n*"
	      },
	      "metadata" : {
		"oneOf" : [ {
		  "$ref" : "#/definitions/DataObjectMetadata"
		} ]
	      }
	    }
	  },
	  "DeduplicateAction" : {
	    "type" : "object",
	    "description" : "[[Action]]to deduplicate a subfeed.\r\nDeduplication keeps the last record for every key, also after it has been deleted in the source.\r\nDeduplicateAction adds an additional Column[[TechnicalTableColumn.captured]]. It contains the timestamp of the last occurrence of the record in the source.\r\nThis creates lots of updates. Especially when using saveMode.Merge it is better to set[[TechnicalTableColumn.captured]]to the last change of the record in the source. Use updateCapturedColumnOnlyWhenChanged = true to enable this optimization.\r\nDeduplicateAction needs a transactional table (e.g.[[TransactionalSparkTableDataObject]]) as output with defined primary keys.\r\nIf output implements[[CanMergeDataFrame]] , saveMode.Merge can be enabled by setting mergeModeEnable = true. This allows for much better performance.",
	    "additionalProperties" : false,
	    "required" : [ "id", "inputId", "outputId" ],
	    "properties" : {
	      "id" : {
		"type" : "string"
	      },
	      "inputId" : {
		"type" : "string",
		"description" : "inputs DataObject"
	      },
	      "outputId" : {
		"type" : "string",
		"description" : "output DataObject"
	      },
	      "transformer" : {
		"type" : "object",
		"description" : "Configuration of a custom Spark-DataFrame transformation between one input and one output (1:1)\r\nDefine a transform function which receives a DataObjectIds, a DataFrames and a map of options and has to return a\r\nDataFrame, see also[[CustomDfTransformer]].\r\nNote about Python transformation: Environment with Python and PySpark needed.\r\nPySpark session is initialize and available under variables`sc`,`session`,`sqlContext`.\r\nOther variables available are\r\n-`inputDf`: Input DataFrame\r\n-`options`: Transformation options as Map[String,String]\r\n-`dataObjectId`: Id of input dataObject as String\r\nOutput DataFrame must be set with`setOutputDf(df)` .",
		"additionalProperties" : false,
		"properties" : {
		  "className" : {
		    "type" : "string",
		    "description" : "Optional class name implementing trait[[CustomDfTransformer]]"
		  },
		  "scalaFile" : {
		    "type" : "string",
		    "description" : "Optional file where scala code for transformation is loaded from. The scala code in the file needs to be a function of type[[fnTransformType]] ."
		  },
		  "scalaCode" : {
		    "type" : "string",
		    "description" : "Optional scala code for transformation. The scala code needs to be a function of type[[fnTransformType]] ."
		  },
		  "sqlCode" : {
		    "type" : "string",
		    "description" : "Optional SQL code for transformation.\r\nUse tokens %{<key>} to replace with runtimeOptions in SQL code.\r\nExample: \\\"select * from test where run = %{runId}\\\""
		  },
		  "pythonFile" : {
		    "type" : "string",
		    "description" : "Optional pythonFile to use for python transformation. The python code can use variables inputDf, dataObjectId and options. The transformed DataFrame has to be set with setOutputDf."
		  },
		  "pythonCode" : {
		    "type" : "string",
		    "description" : "Optional pythonCode to use for python transformation. The python code can use variables inputDf, dataObjectId and options. The transformed DataFrame has to be set with setOutputDf."
		  },
		  "options" : {
		    "type" : "object",
		    "description" : "Options to pass to the transformation",
		    "additionalProperties" : {
		      "type" : "string"
		    }
		  },
		  "runtimeOptions" : {
		    "type" : "object",
		    "description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\r\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
		    "additionalProperties" : {
		      "type" : "string"
		    }
		  }
		}
	      },
	      "transformers" : {
		"type" : "array",
		"description" : "optional list of transformations to apply before deduplication. See[[sparktransformer]] for a list of included Transformers.\r\nThe transformations are applied according to the lists ordering.",
		"items" : {
		  "oneOf" : [ {
		    "type" : "object",
		    "description" : "Interface to implement Spark-DataFrame transformers working with one input and one output (1:1).\r\nThis trait extends DfSparkTransformer to pass a map of options as parameter to the transform function. This is mainly\r\nused by custom transformers.",
		    "additionalProperties" : false,
		    "required" : [ "type" ],
		    "properties" : {
		      "type" : {
			"const" : "OptionsDfTransformer"
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Repartition DataFrame\r\nFor detailled description about repartitioning DataFrames see also[[SparkRepartitionDef]]",
		    "additionalProperties" : false,
		    "required" : [ "numberOfTasksPerPartition", "type" ],
		    "properties" : {
		      "type" : {
			"const" : "RepartitionTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      },
		      "numberOfTasksPerPartition" : {
			"type" : "integer",
			"description" : "Number of Spark tasks to create per partition value by repartitioning the DataFrame."
		      },
		      "keyCols" : {
			"type" : "array",
			"description" : "Optional key columns to distribute records over Spark tasks inside a partition value.",
			"items" : {
			  "type" : "string"
			}
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Standardize datatypes of a DataFrame.\r\nCurrent implementation converts all decimal datatypes to a corresponding integral or float datatype",
		    "additionalProperties" : false,
		    "required" : [ "type" ],
		    "properties" : {
		      "type" : {
			"const" : "StandardizeDatatypesTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Configuration of a custom Spark-DataFrame transformation between one input and one output (1:1) as Scala code which is compiled at runtime.\r\nDefine a transform function which receives a DataObjectId, a DataFrame and a map of options and has to return a\r\nDataFrame. The scala code has to implement a function of type[[fnTransformType]] .",
		    "additionalProperties" : false,
		    "required" : [ "type" ],
		    "properties" : {
		      "type" : {
			"const" : "ScalaCodeDfTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      },
		      "code" : {
			"type" : "string",
			"description" : "Scala code for transformation. The scala code needs to be a function of type[[fnTransformType]] ."
		      },
		      "file" : {
			"type" : "string",
			"description" : "File where scala code for transformation is loaded from. The scala code in the file needs to be a function of type[[fnTransformType]] ."
		      },
		      "options" : {
			"type" : "object",
			"description" : "Options to pass to the transformation",
			"additionalProperties" : {
			  "type" : "string"
			}
		      },
		      "runtimeOptions" : {
			"type" : "object",
			"description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\r\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
			"additionalProperties" : {
			  "type" : "string"
			}
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Apply a column blacklist to a DataFrame.",
		    "additionalProperties" : false,
		    "required" : [ "columnBlacklist", "type" ],
		    "properties" : {
		      "type" : {
			"const" : "BlacklistTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      },
		      "columnBlacklist" : {
			"type" : "array",
			"description" : "List of columns to exclude from DataFrame",
			"items" : {
			  "type" : "string"
			}
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Configuration of a custom Spark-DataFrame transformation between one input and one output (1:1) as Java/Scala Class.\r\nDefine a transform function which receives a DataObjectId, a DataFrame and a map of options and has to return a\r\nDataFrame. The Java/Scala class has to implement interface[[CustomDfTransformer]] .",
		    "additionalProperties" : false,
		    "required" : [ "className", "type" ],
		    "properties" : {
		      "type" : {
			"const" : "ScalaClassDfTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      },
		      "className" : {
			"type" : "string",
			"description" : "class name implementing trait[[CustomDfTransformer]]"
		      },
		      "options" : {
			"type" : "object",
			"description" : "Options to pass to the transformation",
			"additionalProperties" : {
			  "type" : "string"
			}
		      },
		      "runtimeOptions" : {
			"type" : "object",
			"description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\r\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
			"additionalProperties" : {
			  "type" : "string"
			}
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Add additional columns to the DataFrame by extracting information from the context.",
		    "additionalProperties" : false,
		    "required" : [ "additionalColumns", "type" ],
		    "properties" : {
		      "type" : {
			"const" : "AdditionalColumnsTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      },
		      "additionalColumns" : {
			"type" : "object",
			"description" : "optional tuples of [column name, spark sql expression] to be added as additional columns to the dataframe.\r\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
			"additionalProperties" : {
			  "type" : "string"
			}
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Configuration of a custom Spark-DataFrame transformation between one input and one output (1:1) as Python/PySpark code.\r\nNote that this transformer needs a Python and PySpark environment installed.\r\nPySpark session is initialize and available under variables`sc`,`session`,`sqlContext`.\r\nOther variables available are\r\n-`inputDf`: Input DataFrame\r\n-`options`: Transformation options as Map[String,String]\r\n-`dataObjectId`: Id of input dataObject as String\r\nOutput DataFrame must be set with`setOutputDf(df)` .",
		    "additionalProperties" : false,
		    "required" : [ "type" ],
		    "properties" : {
		      "type" : {
			"const" : "PythonCodeDfTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      },
		      "code" : {
			"type" : "string",
			"description" : "Optional python code to user for python transformation. The python code can use variables inputDf, dataObjectId and options. The transformed DataFrame has to be set with setOutputDf."
		      },
		      "file" : {
			"type" : "string",
			"description" : "Optional file with python code to use for python transformation. The python code can use variables inputDf, dataObjectId and options. The transformed DataFrame has to be set with setOutputDf."
		      },
		      "options" : {
			"type" : "object",
			"description" : "Options to pass to the transformation",
			"additionalProperties" : {
			  "type" : "string"
			}
		      },
		      "runtimeOptions" : {
			"type" : "object",
			"description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\r\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
			"additionalProperties" : {
			  "type" : "string"
			}
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Configuration of a custom Spark-DataFrame transformation between one input and one output (1:1) as SQL code.\r\nThe input data is available as temporary view in SQL. As name for the temporary view the input DataObjectId is used\r\n(special characters are replaces by underscores). A special token \\'%{inputViewName}\\' will be replaced with the name of\r\nthe temporary view at runtime.",
		    "additionalProperties" : false,
		    "required" : [ "code", "type" ],
		    "properties" : {
		      "type" : {
			"const" : "SQLDfTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      },
		      "code" : {
			"type" : "string",
			"description" : "SQL code for transformation.\r\nUse tokens %{<key>} to replace with runtimeOptions in SQL code.\r\nExample: \\\"select * from test where run = %{runId}\\\"\r\nA special token %{inputViewName} can be used to insert the temporary view name."
		      },
		      "options" : {
			"type" : "object",
			"description" : "Options to pass to the transformation",
			"additionalProperties" : {
			  "type" : "string"
			}
		      },
		      "runtimeOptions" : {
			"type" : "object",
			"description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\r\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
			"additionalProperties" : {
			  "type" : "string"
			}
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Apply validation rules to a DataFrame and collect potential violation error messages in a new column.",
		    "additionalProperties" : false,
		    "required" : [ "rules", "type" ],
		    "properties" : {
		      "type" : {
			"const" : "DataValidationTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      },
		      "rules" : {
			"type" : "array",
			"description" : "list of validation rules to apply to the DataFrame",
			"items" : {
			  "oneOf" : [ {
			    "type" : "object",
			    "description" : "Definition for a row level data validation rule.",
			    "additionalProperties" : false,
			    "required" : [ "condition", "type" ],
			    "properties" : {
			      "type" : {
				"const" : "RowLevelValidationRule"
			      },
			      "condition" : {
				"type" : "string",
				"description" : "a Spark SQL expression defining the condition to be tested."
			      },
			      "errorMsg" : {
				"type" : "string",
				"description" : "Optional error msg to be create if the condition fails. Default is to use a text representation of the condition."
			      }
			    }
			  } ]
			}
		      },
		      "errorsColumn" : {
			"type" : "string",
			"description" : "Optional column name for the list of error messages. Default is \\\"errors\\\"."
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Apply a column whitelist to a DataFrame.",
		    "additionalProperties" : false,
		    "required" : [ "columnWhitelist", "type" ],
		    "properties" : {
		      "type" : {
			"const" : "WhitelistTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      },
		      "columnWhitelist" : {
			"type" : "array",
			"description" : "List of columns to keep from DataFrame",
			"items" : {
			  "type" : "string"
			}
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Apply a filter condition to a DataFrame.",
		    "additionalProperties" : false,
		    "required" : [ "filterClause", "type" ],
		    "properties" : {
		      "type" : {
			"const" : "FilterTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      },
		      "filterClause" : {
			"type" : "string",
			"description" : "Spark SQL expression to filter the DataFrame"
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Configuration of a custom Spark-DataFrame transformation between one input and one output (1:1) as Scala code which is compiled at runtime.\r\nThe code is loaded from a Notebook. It should define a transform function with a configurable name, which receives a DataObjectId, a DataFrame\r\nand a map of options and has to return a DataFrame, see also ([[fnTransformType]] ).\r\nNotebook-cells starting with \\\"//!IGNORE\\\" will be ignored.",
		    "additionalProperties" : false,
		    "required" : [ "url", "functionName", "type" ],
		    "properties" : {
		      "type" : {
			"const" : "ScalaNotebookDfTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      },
		      "url" : {
			"type" : "string",
			"description" : "Url to download notebook in IPYNB-format, which defines transformation."
		      },
		      "functionName" : {
			"type" : "string",
			"description" : "The notebook needs to contain a Scala-function with this name and type[[fnTransformType]] ."
		      },
		      "authMode" : {
			"oneOf" : [ {
			  "type" : "object",
			  "description" : "Connect by custom authorization header",
			  "additionalProperties" : false,
			  "required" : [ "secretVariable", "type" ],
			  "properties" : {
			    "type" : {
			      "const" : "AuthHeaderMode"
			    },
			    "headerName" : {
			      "type" : "string"
			    },
			    "secretVariable" : {
			      "type" : "string"
			    }
			  }
			}, {
			  "type" : "object",
			  "description" : "Validate by SSL Certificates : Only location an credentials. Additional attributes should be\r\nsupplied via options map",
			  "additionalProperties" : false,
			  "required" : [ "keystorePath", "keystorePassVariable", "truststorePath", "truststorePassVariable", "type" ],
			  "properties" : {
			    "type" : {
			      "const" : "SSLCertsAuthMode"
			    },
			    "keystorePath" : {
			      "type" : "string"
			    },
			    "keystoreType" : {
			      "type" : "string"
			    },
			    "keystorePassVariable" : {
			      "type" : "string"
			    },
			    "truststorePath" : {
			      "type" : "string"
			    },
			    "truststoreType" : {
			      "type" : "string"
			    },
			    "truststorePassVariable" : {
			      "type" : "string"
			    }
			  }
			}, {
			  "type" : "object",
			  "description" : "Connect by basic authentication",
			  "additionalProperties" : false,
			  "required" : [ "userVariable", "passwordVariable", "type" ],
			  "properties" : {
			    "type" : {
			      "const" : "BasicAuthMode"
			    },
			    "userVariable" : {
			      "type" : "string"
			    },
			    "passwordVariable" : {
			      "type" : "string"
			    }
			  }
			}, {
			  "type" : "object",
			  "description" : "Connect with custom HTTP authentication",
			  "additionalProperties" : false,
			  "required" : [ "className", "options", "type" ],
			  "properties" : {
			    "type" : {
			      "const" : "CustomHttpAuthMode"
			    },
			    "className" : {
			      "type" : "string",
			      "description" : "class name implementing trait[[CustomHttpAuthModeLogic]]"
			    },
			    "options" : {
			      "type" : "object",
			      "description" : "Options to pass to the custom auth mode logc in prepare function",
			      "additionalProperties" : {
				"type" : "string"
			      }
			    }
			  }
			}, {
			  "type" : "object",
			  "description" : "Validate by SASL_SSL Authentication : user / password and truststore",
			  "additionalProperties" : false,
			  "required" : [ "username", "passwordVariable", "sslMechanism", "truststorePath", "truststorePassVariable", "type" ],
			  "properties" : {
			    "type" : {
			      "const" : "SASLSCRAMAuthMode"
			    },
			    "username" : {
			      "type" : "string"
			    },
			    "passwordVariable" : {
			      "type" : "string"
			    },
			    "sslMechanism" : {
			      "type" : "string"
			    },
			    "truststorePath" : {
			      "type" : "string"
			    },
			    "truststoreType" : {
			      "type" : "string"
			    },
			    "truststorePassVariable" : {
			      "type" : "string"
			    }
			  }
			}, {
			  "type" : "object",
			  "description" : "Connect by token\r\nFor HTTP Connection this is used as Bearer token in Authorization header.",
			  "additionalProperties" : false,
			  "required" : [ "tokenVariable", "type" ],
			  "properties" : {
			    "type" : {
			      "const" : "TokenAuthMode"
			    },
			    "tokenVariable" : {
			      "type" : "string"
			    }
			  }
			}, {
			  "type" : "object",
			  "description" : "Connect by using Keycloak to manage token and token refresh giving clientId/secret as information.\r\nFor HTTP Connection this is used as Bearer token in Authorization header.",
			  "additionalProperties" : false,
			  "required" : [ "ssoServer", "ssoRealm", "ssoGrantType", "clientIdVariable", "clientSecretVariable", "type" ],
			  "properties" : {
			    "type" : {
			      "const" : "KeycloakClientSecretAuthMode"
			    },
			    "ssoServer" : {
			      "type" : "string"
			    },
			    "ssoRealm" : {
			      "type" : "string"
			    },
			    "ssoGrantType" : {
			      "type" : "string"
			    },
			    "clientIdVariable" : {
			      "type" : "string"
			    },
			    "clientSecretVariable" : {
			      "type" : "string"
			    }
			  }
			}, {
			  "type" : "object",
			  "description" : "Validate by user and private/public key\r\nPrivate key is read from .ssh",
			  "additionalProperties" : false,
			  "required" : [ "userVariable", "type" ],
			  "properties" : {
			    "type" : {
			      "const" : "PublicKeyAuthMode"
			    },
			    "userVariable" : {
			      "type" : "string"
			    }
			  }
			} ]
		      },
		      "options" : {
			"type" : "object",
			"description" : "Options to pass to the transformation",
			"additionalProperties" : {
			  "type" : "string"
			}
		      },
		      "runtimeOptions" : {
			"type" : "object",
			"description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\r\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
			"additionalProperties" : {
			  "type" : "string"
			}
		      }
		    }
		  } ]
		}
	      },
	      "columnBlacklist" : {
		"type" : "array",
		"description" : "Remove all columns on blacklist from dataframe",
		"items" : {
		  "type" : "string"
		}
	      },
	      "columnWhitelist" : {
		"type" : "array",
		"description" : "Keep only columns on whitelist in dataframe",
		"items" : {
		  "type" : "string"
		}
	      },
	      "additionalColumns" : {
		"type" : "object",
		"description" : "optional tuples of [column name, spark sql expression] to be added as additional columns to the dataframe.\r\nThe spark sql expressions are evaluated against an instance of[[io.smartdatalake.util.misc.DefaultExpressionData]] .",
		"additionalProperties" : {
		  "type" : "string"
		}
	      },
	      "filterClause" : {
		"type" : "string"
	      },
	      "standardizeDatatypes" : {
		"type" : "string"
	      },
	      "ignoreOldDeletedColumns" : {
		"type" : "string",
		"description" : "if true, remove no longer existing columns in Schema Evolution"
	      },
	      "ignoreOldDeletedNestedColumns" : {
		"type" : "string",
		"description" : "if true, remove no longer existing columns from nested data types in Schema Evolution.\r\nKeeping deleted columns in complex data types has performance impact as all new data\r\nin the future has to be converted by a complex function."
	      },
	      "updateCapturedColumnOnlyWhenChanged" : {
		"type" : "string",
		"description" : "Set to true to enable update Column[[TechnicalTableColumn.captured]] only if Record has changed in the source, instead of updating it with every execution (default=false).\r\nThis results in much less records updated with saveMode.Merge."
	      },
	      "mergeModeEnable" : {
		"type" : "string",
		"description" : "Set to true to use saveMode.Merge for much better performance. Output DataObject must implement[[CanMergeDataFrame]] if enabled (default = false)."
	      },
	      "mergeModeAdditionalJoinPredicate" : {
		"type" : "string",
		"description" : "To optimize performance it might be interesting to limit the records read from the existing table data, e.g. it might be sufficient to use only the last 7 days.\r\nSpecify a condition to select existing data to be used in transformation as Spark SQL expression.\r\nUse table alias \\'existing\\' to reference columns of the existing table data."
	      },
	      "breakDataFrameLineage" : {
		"type" : "string"
	      },
	      "persist" : {
		"type" : "string"
	      },
	      "executionMode" : {
		"oneOf" : [ {
		  "type" : "object",
		  "description" : "Execution mode to incrementally process file-based DataObjects.\r\nIt takes all existing files in the input DataObject and removes (deletes) them after processing.\r\nInput partition values are applied when searching for files and also used as output partition values.",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "FileIncrementalMoveMode"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "An execution mode for incremental processing by remembering DataObjects state from last increment.",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "DataObjectStateIncrementalMode"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Spark streaming execution mode uses Spark Structured Streaming to incrementally execute data loads and keep track of processed data.\r\nThis mode needs a DataObject implementing CanCreateStreamingDataFrame and works only with SparkSubFeeds.\r\nThis mode can be executed synchronously in the DAG by using triggerType=Once, or asynchronously as Streaming Query with triggerType = ProcessingTime or Continuous.",
		  "additionalProperties" : false,
		  "required" : [ "checkpointLocation", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "SparkStreamingMode"
		    },
		    "checkpointLocation" : {
		      "type" : "string",
		      "description" : "location for checkpoints of streaming query to keep state"
		    },
		    "triggerType" : {
		      "type" : "string",
		      "description" : "define execution interval of Spark streaming query. Possible values are Once (default), ProcessingTime & Continuous. See[[Trigger]] for details.\r\n                      Note that this is only applied if SDL is executed in streaming mode. If SDL is executed in normal mode, TriggerType=Once is used always.\r\nIf triggerType=Once, the action is repeated with Trigger.Once in SDL streaming mode."
		    },
		    "triggerTime" : {
		      "type" : "string",
		      "description" : "Time as String in triggerType = ProcessingTime or Continuous. See[[Trigger]] for details."
		    },
		    "inputOptions" : {
		      "type" : "object",
		      "description" : "additional option to apply when reading streaming source. This overwrites options set by the DataObjects.",
		      "additionalProperties" : {
			"type" : "string"
		      }
		    },
		    "outputOptions" : {
		      "type" : "object",
		      "description" : "additional option to apply when writing to streaming sink. This overwrites options set by the DataObjects.",
		      "additionalProperties" : {
			"type" : "string"
		      }
		    },
		    "outputMode" : {
		      "type" : "string",
		      "enum" : [ "Append", "Complete", "Update" ]
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Compares max entry in \\\"compare column\\\" between mainOutput and mainInput and incrementally loads the delta.\r\nThis mode works only with SparkSubFeeds. The filter is not propagated to following actions.",
		  "additionalProperties" : false,
		  "required" : [ "compareCol", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "SparkIncrementalMode"
		    },
		    "compareCol" : {
		      "type" : "string",
		      "description" : "a comparable column name existing in mainInput and mainOutput used to identify the delta. Column content should be bigger for newer records."
		    },
		    "alternativeOutputId" : {
		      "type" : "string",
		      "description" : "optional alternative outputId of DataObject later in the DAG. This replaces the mainOutputId.\r\nIt can be used to ensure processing all partitions over multiple actions in case of errors."
		    },
		    "applyCondition" : {
		      "type" : "object",
		      "description" : "Definition of a Spark SQL condition with description.\r\nThis is used for example to define failConditions of[[PartitionDiffMode]] .",
		      "additionalProperties" : false,
		      "required" : [ "expression" ],
		      "properties" : {
			"expression" : {
			  "type" : "string",
			  "description" : "Condition formulated as Spark SQL. The attributes available are dependent on the context."
			},
			"description" : {
			  "type" : "string",
			  "description" : "A textual description of the condition to be shown in error messages."
			}
		      }
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "An execution mode which forces processing all data from it\\'s inputs.",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "ProcessAllMode"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "An execution mode which just validates that partition values are given.\r\nNote: For start nodes of the DAG partition values can be defined by command line, for subsequent nodes partition values are passed on from previous nodes.",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "FailIfNoPartitionValuesMode"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Partition difference execution mode lists partitions on mainInput & mainOutput DataObject and starts loading all missing partitions.\r\nPartition columns to be used for comparision need to be a common \\'init\\' of input and output partition columns.\r\nThis mode needs mainInput/Output DataObjects which CanHandlePartitions to list partitions.\r\nPartition values are passed to following actions for partition columns which they have in common.",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "PartitionDiffMode"
		    },
		    "partitionColNb" : {
		      "type" : "integer",
		      "description" : "optional number of partition columns to use as a common \\'init\\'."
		    },
		    "alternativeOutputId" : {
		      "type" : "string",
		      "description" : "optional alternative outputId of DataObject later in the DAG. This replaces the mainOutputId.\r\nIt can be used to ensure processing all partitions over multiple actions in case of errors."
		    },
		    "nbOfPartitionValuesPerRun" : {
		      "type" : "integer",
		      "description" : "optional restriction of the number of partition values per run."
		    },
		    "applyCondition" : {
		      "type" : "string",
		      "description" : "Condition to decide if execution mode should be applied or not. Define a spark sql expression working with attributes of[[DefaultExecutionModeExpressionData]] returning a boolean.\r\nDefault is to apply the execution mode if given partition values (partition values from command line or passed from previous action) are not empty."
		    },
		    "failCondition" : {
		      "type" : "string"
		    },
		    "failConditions" : {
		      "type" : "array",
		      "description" : "List of conditions to fail application of execution mode if true. Define as spark sql expressions working with attributes of[[PartitionDiffModeExpressionData]] returning a boolean.\r\nDefault is that the application of the PartitionDiffMode does not fail the action. If there is no data to process, the following actions are skipped.\r\nMultiple conditions are evaluated individually and every condition may fail the execution mode (or-logic)",
		      "items" : {
			"type" : "object",
			"description" : "Definition of a Spark SQL condition with description.\r\nThis is used for example to define failConditions of[[PartitionDiffMode]] .",
			"additionalProperties" : false,
			"required" : [ "expression" ],
			"properties" : {
			  "expression" : {
			    "type" : "string",
			    "description" : "Condition formulated as Spark SQL. The attributes available are dependent on the context."
			  },
			  "description" : {
			    "type" : "string",
			    "description" : "A textual description of the condition to be shown in error messages."
			  }
			}
		      }
		    },
		    "selectExpression" : {
		      "type" : "string",
		      "description" : "optional expression to define or refine the list of selected output partitions. Define a spark sql expression working with the attributes of[[PartitionDiffModeExpressionData]] returning a list<map<string,string>>.\r\nDefault is to return the originally selected output partitions found in attribute selectedOutputPartitionValues."
		    },
		    "applyPartitionValuesTransform" : {
		      "type" : "string",
		      "description" : "If true applies the partition values transform of custom transformations on input partition values before comparison with output partition values.\r\nIf enabled input and output partition columns can be different. Default is to disable the transformation of partition values."
		    },
		    "selectAdditionalInputExpression" : {
		      "type" : "string",
		      "description" : "optional expression to refine the list of selected input partitions. Note that primarily output partitions are selected by PartitionDiffMode.\r\nThe selected output partitions are then transformed back to the input partitions needed to create the selected output partitions. This is one-to-one except if applyPartitionValuesTransform=true.\r\nAnd sometimes there is a need for additional input data to create the output partitions, e.g. if you aggregate a window of 7 days for every day.\r\nYou can customize selected input partitions by defining a spark sql expression working with the attributes of[[PartitionDiffModeExpressionData]] returning a list<map<string,string>>.\r\nDefault is to return the originally selected input partitions found in attribute selectedInputPartitionValues."
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Execution mode to create custom partition execution mode logic.\r\nDefine a function which receives main input&output DataObject and returns partition values to process as Seq[Map[String,String]\\\\]",
		  "additionalProperties" : false,
		  "required" : [ "className", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "CustomPartitionMode"
		    },
		    "className" : {
		      "type" : "string",
		      "description" : "class name implementing trait[[CustomPartitionModeLogic]]"
		    },
		    "alternativeOutputId" : {
		      "type" : "string",
		      "description" : "optional alternative outputId of DataObject later in the DAG. This replaces the mainOutputId.\r\nIt can be used to ensure processing all partitions over multiple actions in case of errors."
		    },
		    "options" : {
		      "type" : "object",
		      "description" : "Options specified in the configuration for this execution mode",
		      "additionalProperties" : {
			"type" : "string"
		      }
		    }
		  }
		} ]
	      },
	      "executionCondition" : {
		"type" : "object",
		"description" : "Definition of a Spark SQL condition with description.\r\nThis is used for example to define failConditions of[[PartitionDiffMode]] .",
		"additionalProperties" : false,
		"required" : [ "expression" ],
		"properties" : {
		  "expression" : {
		    "type" : "string",
		    "description" : "Condition formulated as Spark SQL. The attributes available are dependent on the context."
		  },
		  "description" : {
		    "type" : "string",
		    "description" : "A textual description of the condition to be shown in error messages."
		  }
		}
	      },
	      "metricsFailCondition" : {
		"type" : "string",
		"description" : "optional spark sql expression evaluated as where-clause against dataframe of metrics. Available columns are dataObjectId, key, value.\r\nIf there are any rows passing the where clause, a MetricCheckFailed exception is thrown."
	      },
	      "metadata" : {
		"oneOf" : [ {
		  "$ref" : "#/definitions/ActionMetadata"
		} ]
	      }
	    }
	  },
	  "ActionsExporterDataObject" : {
	    "type" : "object",
	    "description" : "Exports a util[[DataFrame]]that contains properties and metadata extracted from all[[io.smartdatalake.workflow.action.Action]]s\r\nthat are registered in the current[[InstanceRegistry]].\r\nAlternatively, it can export the properties and metadata of all[[io.smartdatalake.workflow.action.Action]]s defined in config files. For this, the\r\nconfiguration \\\"config\\\" has to be set to the location of the config.\r\nExample:\r\n{{{\r\ndataObjects = {\r\n...\r\nactions-exporter {\r\ntype = ActionsExporterDataObject\r\nconfig = path/to/myconfiguration.conf\r\n}\r\n...\r\n}\r\n}}}\nThe config value can point to a configuration file or a directory containing configuration files.\r\n\nSEE: Refer to[[ConfigLoader.loadConfigFromFilesystem()]] for details about the configuration loading.",
	    "additionalProperties" : false,
	    "required" : [ "id" ],
	    "properties" : {
	      "id" : {
		"type" : "string"
	      },
	      "config" : {
		"type" : "string"
	      },
	      "metadata" : {
		"oneOf" : [ {
		  "$ref" : "#/definitions/DataObjectMetadata"
		} ]
	      }
	    }
	  },
	  "XmlFileDataObject" : {
	    "type" : "object",
	    "description" : "A[[io.smartdatalake.workflow.dataobject.DataObject]]backed by an XML data source.\r\nIt manages read and write access and configurations required for[[io.smartdatalake.workflow.action.Action]]s to\r\nwork on XML formatted files.\r\nReading and writing details are delegated to Apache Spark[[org.apache.spark.sql.DataFrameReader]]\r\nand[[org.apache.spark.sql.DataFrameWriter]]respectively. The reader and writer implementations are provided by\r\nthe[[https://github.com/databricks/spark-xml databricks spark-xml]] project.\r\nNote that writing XML-file partitioned is not supported by spark-xml.\r\n\nSEE: [[org.apache.spark.sql.DataFrameReader]]\r\nSEE: [[org.apache.spark.sql.DataFrameWriter]]",
	    "additionalProperties" : false,
	    "required" : [ "id", "path" ],
	    "properties" : {
	      "id" : {
		"type" : "string"
	      },
	      "path" : {
		"type" : "string"
	      },
	      "rowTag" : {
		"type" : "string"
	      },
	      "xmlOptions" : {
		"type" : "object",
		"description" : "Settings for the underlying[[org.apache.spark.sql.DataFrameReader]]and[[org.apache.spark.sql.DataFrameWriter]] .",
		"additionalProperties" : {
		  "type" : "string"
		}
	      },
	      "partitions" : {
		"type" : "array",
		"items" : {
		  "type" : "string"
		}
	      },
	      "schema" : {
		"type" : "string",
		"description" : "An optional data object schema. If defined, any automatic schema inference is avoided. As this corresponds to the schema on write, it must not include the optional filenameColumn on read."
	      },
	      "schemaMin" : {
		"type" : "string"
	      },
	      "saveMode" : {
		"type" : "string",
		"enum" : [ "Merge ", "OverwriteOptimized ", "OverwritePreserveDirectories ", "Ignore ", "ErrorIfExists ", "Append ", "Overwrite " ]
	      },
	      "sparkRepartition" : {
		"type" : "object",
		"description" : "This controls repartitioning of the DataFrame before writing with Spark to Hadoop.\r\nWhen writing multiple partitions of a partitioned DataObject, the number of spark tasks created is equal to numberOfTasksPerPartition\r\nmultiplied with the number of partitions to write. To spread the records of a partition only over numberOfTasksPerPartition spark tasks,\r\nkeyCols must be given which are used to derive a task number inside the partition (hashvalue(keyCols) modulo numberOfTasksPerPartition).\r\nWhen writing to an unpartitioned DataObject or only one partition of a partitioned DataObject, the number of spark tasks created is equal\r\nto numberOfTasksPerPartition. Optional keyCols can be used to keep corresponding records together in the same task/file.",
		"additionalProperties" : false,
		"required" : [ "numberOfTasksPerPartition" ],
		"properties" : {
		  "numberOfTasksPerPartition" : {
		    "type" : "integer",
		    "description" : "Number of Spark tasks to create per partition before writing to DataObject by repartitioning the DataFrame.\r\nThis controls how many files are created in each Hadoop partition."
		  },
		  "keyCols" : {
		    "type" : "array",
		    "description" : "Optional key columns to distribute records over Spark tasks inside a Hadoop partition.\r\nIf DataObject has Hadoop partitions defined, keyCols must be defined.",
		    "items" : {
		      "type" : "string"
		    }
		  },
		  "sortCols" : {
		    "type" : "array",
		    "description" : "Optional columns to sort records inside files created.",
		    "items" : {
		      "type" : "string"
		    }
		  },
		  "filename" : {
		    "type" : "string",
		    "description" : "Option filename to rename target file(s). If numberOfTasksPerPartition is greater than 1,\r\nmultiple files can exist in a directory and a number is inserted into the filename after the first \\'.\\'.\r\nExample: filename=data.csv -> files created are data.1.csv, data.2.csv, ..."
		  }
		}
	      },
	      "flatten" : {
		"type" : "string"
	      },
	      "acl" : {
		"type" : "object",
		"description" : "Describes a complete ACL Specification (basic owner/group/other permissions AND extended ACLS)\r\nto be applied to a Data Object on writing",
		"additionalProperties" : false,
		"required" : [ "permission", "acls" ],
		"properties" : {
		  "permission" : {
		    "type" : "string",
		    "description" : ": File system permission string in symbolic notation form (e.g. rwxr-xr-x)"
		  },
		  "acls" : {
		    "type" : "array",
		    "description" : ": a sequence of[[AclElement]] s",
		    "items" : {
		      "type" : "object",
		      "description" : "Describes a single extended ACL to be applied to a Data Object\r\nin addition to the basic file system permissions",
		      "additionalProperties" : false,
		      "required" : [ "aclType", "name", "permission" ],
		      "properties" : {
			"aclType" : {
			  "type" : "string",
			  "description" : ": type of ACL to be added \\\"group\\\", \\\"user\\\""
			},
			"name" : {
			  "type" : "string",
			  "description" : ": the name of the user/group for which an ACL definition is being added"
			},
			"permission" : {
			  "type" : "string",
			  "description" : ": the permission (rwx syntax) to be granted"
			}
		      }
		    }
		  }
		}
	      },
	      "connectionId" : {
		"type" : "string"
	      },
	      "filenameColumn" : {
		"type" : "string"
	      },
	      "expectedPartitionsCondition" : {
		"type" : "string",
		"description" : "Optional definition of partitions expected to exist.\r\nDefine a Spark SQL expression that is evaluated against a[[PartitionValues]] instance and returns true or false\r\nDefault is to expect all partitions to exist."
	      },
	      "housekeepingMode" : {
		"oneOf" : [ {
		  "type" : "object",
		  "description" : "Archive and compact old partitions:\r\nArchive partition reduces the number of partitions in the past by moving older partitions into special \\\"archive partitions\\\".\r\nCompact partition reduces the number of files in a partition by rewriting them with Spark.\r\nExample: archive and compact a table with partition layout run_id=<integer>\r\n- archive partitions after 1000 partitions into \\\"archive partition\\\" equal to floor(run_id/1000)\r\n- compact \\\"archive partition\\\" when full\r\n{{{\r\nhousekeepingMode = {\r\ntype = PartitionArchiveCompactionMode\r\narchivePartitionExpression = \\\"if( elements[\\'run_id\\'] < runId - 1000, map(\\'run_id\\', elements[\\'run_id\\'] div 1000), elements)\\\"\r\ncompactPartitionExpression = \\\"elements[\\'run_id\\'] % 1000 = 0 and elements[\\'run_id\\'] <= runId - 2000\\\"\r\n}\r\n}}}",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "PartitionArchiveCompactionMode"
		    },
		    "archivePartitionExpression" : {
		      "type" : "string",
		      "description" : "Expression to define the archive partition for a given partition. Define a spark\r\nsql expression working with the attributes of[[PartitionExpressionData]] returning archive\r\npartition values as Map[String,String]. If return value is the same as input elements, partition is not touched,\r\notherwise all files of the partition are moved to the returned partition definition.\r\nBe aware that the value of the partition columns changes for these files/records."
		    },
		    "compactPartitionExpression" : {
		      "type" : "string",
		      "description" : "Expression to define partitions which should be compacted. Define a spark\r\nsql expression working with the attributes of[[PartitionExpressionData]] returning a\r\nboolean = true when this partition should be compacted.\r\nOnce a partition is compacted, it is marked as compacted and will not be compacted again.\r\nIt is therefore ok to return true for all partitions which should be compacted, regardless if they have been compacted already."
		    },
		    "description" : {
		      "type" : "string"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Keep partitions while retention condition is fulfilled, delete other partitions.\r\nExample: cleanup partitions with partition layout dt=<yyyymmdd> after 90 days:\r\n{{{\r\nhousekeepingMode = {\r\ntype = PartitionRetentionMode\r\nretentionCondition = \\\"datediff(now(), to_date(elements[\\'dt\\'], \\'yyyyMMdd\\')) <= 90\\\"\r\n}\r\n}}}",
		  "additionalProperties" : false,
		  "required" : [ "retentionCondition", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "PartitionRetentionMode"
		    },
		    "retentionCondition" : {
		      "type" : "string",
		      "description" : "Condition to decide if a partition should be kept. Define a spark sql expression\r\nworking with the attributes of[[PartitionExpressionData]] returning a boolean with value true if the partition should be kept."
		    },
		    "description" : {
		      "type" : "string"
		    }
		  }
		} ]
	      },
	      "metadata" : {
		"oneOf" : [ {
		  "$ref" : "#/definitions/DataObjectMetadata"
		} ]
	      }
	    }
	  },
	  "WebserviceFileDataObject" : {
	    "type" : "object",
	    "description" : "[[DataObject]] to call webservice and return response as InputStream\r\nThis is implemented as FileRefDataObject because the response is treated as some file content.\r\nFileRefDataObjects support partitioned data. For a WebserviceFileDataObject partitions are mapped as query parameters to create query string.\r\nAll possible query parameter values must be given in configuration.",
	    "additionalProperties" : false,
	    "required" : [ "id", "url" ],
	    "properties" : {
	      "id" : {
		"type" : "string"
	      },
	      "url" : {
		"type" : "string"
	      },
	      "additionalHeaders" : {
		"type" : "object",
		"additionalProperties" : {
		  "type" : "string"
		}
	      },
	      "timeouts" : {
		"type" : "object",
		"additionalProperties" : false,
		"required" : [ "connectionTimeoutMs", "readTimeoutMs" ],
		"properties" : {
		  "connectionTimeoutMs" : {
		    "type" : "integer"
		  },
		  "readTimeoutMs" : {
		    "type" : "integer"
		  }
		}
	      },
	      "readTimeoutMs" : {
		"type" : "integer"
	      },
	      "authMode" : {
		"oneOf" : [ {
		  "type" : "object",
		  "description" : "Connect by custom authorization header",
		  "additionalProperties" : false,
		  "required" : [ "secretVariable", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "AuthHeaderMode"
		    },
		    "headerName" : {
		      "type" : "string"
		    },
		    "secretVariable" : {
		      "type" : "string"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Validate by SSL Certificates : Only location an credentials. Additional attributes should be\r\nsupplied via options map",
		  "additionalProperties" : false,
		  "required" : [ "keystorePath", "keystorePassVariable", "truststorePath", "truststorePassVariable", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "SSLCertsAuthMode"
		    },
		    "keystorePath" : {
		      "type" : "string"
		    },
		    "keystoreType" : {
		      "type" : "string"
		    },
		    "keystorePassVariable" : {
		      "type" : "string"
		    },
		    "truststorePath" : {
		      "type" : "string"
		    },
		    "truststoreType" : {
		      "type" : "string"
		    },
		    "truststorePassVariable" : {
		      "type" : "string"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Connect by basic authentication",
		  "additionalProperties" : false,
		  "required" : [ "userVariable", "passwordVariable", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "BasicAuthMode"
		    },
		    "userVariable" : {
		      "type" : "string"
		    },
		    "passwordVariable" : {
		      "type" : "string"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Connect with custom HTTP authentication",
		  "additionalProperties" : false,
		  "required" : [ "className", "options", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "CustomHttpAuthMode"
		    },
		    "className" : {
		      "type" : "string",
		      "description" : "class name implementing trait[[CustomHttpAuthModeLogic]]"
		    },
		    "options" : {
		      "type" : "object",
		      "description" : "Options to pass to the custom auth mode logc in prepare function",
		      "additionalProperties" : {
			"type" : "string"
		      }
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Validate by SASL_SSL Authentication : user / password and truststore",
		  "additionalProperties" : false,
		  "required" : [ "username", "passwordVariable", "sslMechanism", "truststorePath", "truststorePassVariable", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "SASLSCRAMAuthMode"
		    },
		    "username" : {
		      "type" : "string"
		    },
		    "passwordVariable" : {
		      "type" : "string"
		    },
		    "sslMechanism" : {
		      "type" : "string"
		    },
		    "truststorePath" : {
		      "type" : "string"
		    },
		    "truststoreType" : {
		      "type" : "string"
		    },
		    "truststorePassVariable" : {
		      "type" : "string"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Connect by token\r\nFor HTTP Connection this is used as Bearer token in Authorization header.",
		  "additionalProperties" : false,
		  "required" : [ "tokenVariable", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "TokenAuthMode"
		    },
		    "tokenVariable" : {
		      "type" : "string"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Connect by using Keycloak to manage token and token refresh giving clientId/secret as information.\r\nFor HTTP Connection this is used as Bearer token in Authorization header.",
		  "additionalProperties" : false,
		  "required" : [ "ssoServer", "ssoRealm", "ssoGrantType", "clientIdVariable", "clientSecretVariable", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "KeycloakClientSecretAuthMode"
		    },
		    "ssoServer" : {
		      "type" : "string"
		    },
		    "ssoRealm" : {
		      "type" : "string"
		    },
		    "ssoGrantType" : {
		      "type" : "string"
		    },
		    "clientIdVariable" : {
		      "type" : "string"
		    },
		    "clientSecretVariable" : {
		      "type" : "string"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Validate by user and private/public key\r\nPrivate key is read from .ssh",
		  "additionalProperties" : false,
		  "required" : [ "userVariable", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "PublicKeyAuthMode"
		    },
		    "userVariable" : {
		      "type" : "string"
		    }
		  }
		} ]
	      },
	      "mimeType" : {
		"type" : "string"
	      },
	      "writeMethod" : {
		"type" : "string",
		"enum" : [ "Put ", "Post " ]
	      },
	      "proxy" : {
		"type" : "object",
		"additionalProperties" : false,
		"required" : [ "host", "port" ],
		"properties" : {
		  "host" : {
		    "type" : "string"
		  },
		  "port" : {
		    "type" : "integer"
		  }
		}
	      },
	      "followRedirects" : {
		"type" : "string"
	      },
	      "partitionDefs" : {
		"type" : "array",
		"description" : "list of partitions with list of possible values for every entry",
		"items" : {
		  "type" : "object",
		  "additionalProperties" : false,
		  "required" : [ "name", "values" ],
		  "properties" : {
		    "name" : {
		      "type" : "string"
		    },
		    "values" : {
		      "type" : "array",
		      "items" : {
			"type" : "string"
		      }
		    }
		  }
		}
	      },
	      "partitionLayout" : {
		"type" : "string",
		"description" : "definition of partitions in query string. Use %<partitionColName>% as placeholder for partition column value in layout."
	      },
	      "metadata" : {
		"oneOf" : [ {
		  "$ref" : "#/definitions/DataObjectMetadata"
		} ]
	      }
	    }
	  },
	  "ExcelFileDataObject" : {
	    "type" : "object",
	    "description" : "A[[DataObject]]backed by an Microsoft Excel data source.\r\nIt manages read and write access and configurations required for[[io.smartdatalake.workflow.action.Action]]s to\r\nwork on Microsoft Excel (.xslx) formatted files.\r\nReading and writing details are delegated to Apache Spark[[org.apache.spark.sql.DataFrameReader]]\r\nand[[org.apache.spark.sql.DataFrameWriter]]respectively. The reader and writer implementation is provided by the\r\n[[https://github.com/crealytics/spark-excel Crealytics spark-excel]]project.\r\nRead Schema:\r\nWhen`useHeader`is set to true (default), the reader will use the first row of the Excel sheet as column names for\r\nthe schema and not include the first row as data values. Otherwise the column names are taken from the schema.\r\nIf the schema is not provided or inferred, then each column name is defined as \\\"_c#\\\" where \\\"#\\\" is the column index.\r\nWhen a data object schema is provided, it is used as the schema for the DataFrame. Otherwise if`inferSchema`is\r\nenabled (default), then the data types of the columns are inferred based on the first`excerptSize`rows\r\n(excluding the first).\r\nWhen no schema is provided and`inferSchema` is disabled, all columns are assumed to be of string type.",
	    "additionalProperties" : false,
	    "required" : [ "id", "path" ],
	    "properties" : {
	      "id" : {
		"type" : "string"
	      },
	      "path" : {
		"type" : "string"
	      },
	      "excelOptions" : {
		"type" : "object",
		"description" : "Options passed to[[org.apache.spark.sql.DataFrameReader]]and[[org.apache.spark.sql.DataFrameWriter]] for\r\nreading and writing Microsoft Excel files. Excel support is provided by the spark-excel project (see link below).\r\n\nSEE: [[https://github.com/crealytics/spark-excel]]",
		"additionalProperties" : false,
		"properties" : {
		  "sheetName" : {
		    "type" : "string",
		    "description" : "Optional name of the Excel Sheet to read from/write to."
		  },
		  "numLinesToSkip" : {
		    "type" : "integer",
		    "description" : "Optional number of rows in the excel spreadsheet to skip before any data is read.\r\nThis option must not be set for writing."
		  },
		  "startColumn" : {
		    "type" : "string",
		    "description" : "Optional first column in the specified Excel Sheet to read from (as string, e.g B).\r\nThis option must not be set for writing."
		  },
		  "endColumn" : {
		    "type" : "string",
		    "description" : "Optional last column in the specified Excel Sheet to read from (as string, e.g. F)."
		  },
		  "rowLimit" : {
		    "type" : "integer",
		    "description" : "Optional limit of the number of rows being returned on read.\r\nThis is applied after`numLinesToSkip` ."
		  },
		  "useHeader" : {
		    "type" : "string",
		    "description" : "If`true` , the first row of the excel sheet specifies the column names (default: true)."
		  },
		  "treatEmptyValuesAsNulls" : {
		    "type" : "string",
		    "description" : "Empty cells are parsed as`null` values (default: true)."
		  },
		  "inferSchema" : {
		    "type" : "string",
		    "description" : "Infer the schema of the excel sheet automatically (default: true)."
		  },
		  "timestampFormat" : {
		    "type" : "string",
		    "description" : "A format string specifying the format to use when writing timestamps (default: dd-MM-yyyy HH:mm:ss)."
		  },
		  "dateFormat" : {
		    "type" : "string",
		    "description" : "A format string specifying the format to use when writing dates."
		  },
		  "maxRowsInMemory" : {
		    "type" : "integer",
		    "description" : "The number of rows that are stored in memory.\r\nIf set, a streaming reader is used which can help with big files."
		  },
		  "excerptSize" : {
		    "type" : "integer",
		    "description" : "Sample size for schema inference."
		  }
		}
	      },
	      "partitions" : {
		"type" : "array",
		"items" : {
		  "type" : "string"
		}
	      },
	      "schema" : {
		"type" : "string",
		"description" : "An optional data object schema. If defined, any automatic schema inference is avoided. As this corresponds to the schema on write, it must not include the optional filenameColumn on read."
	      },
	      "schemaMin" : {
		"type" : "string"
	      },
	      "saveMode" : {
		"type" : "string",
		"enum" : [ "Merge ", "OverwriteOptimized ", "OverwritePreserveDirectories ", "Ignore ", "ErrorIfExists ", "Append ", "Overwrite " ]
	      },
	      "sparkRepartition" : {
		"type" : "object",
		"description" : "This controls repartitioning of the DataFrame before writing with Spark to Hadoop.\r\nWhen writing multiple partitions of a partitioned DataObject, the number of spark tasks created is equal to numberOfTasksPerPartition\r\nmultiplied with the number of partitions to write. To spread the records of a partition only over numberOfTasksPerPartition spark tasks,\r\nkeyCols must be given which are used to derive a task number inside the partition (hashvalue(keyCols) modulo numberOfTasksPerPartition).\r\nWhen writing to an unpartitioned DataObject or only one partition of a partitioned DataObject, the number of spark tasks created is equal\r\nto numberOfTasksPerPartition. Optional keyCols can be used to keep corresponding records together in the same task/file.",
		"additionalProperties" : false,
		"required" : [ "numberOfTasksPerPartition" ],
		"properties" : {
		  "numberOfTasksPerPartition" : {
		    "type" : "integer",
		    "description" : "Number of Spark tasks to create per partition before writing to DataObject by repartitioning the DataFrame.\r\nThis controls how many files are created in each Hadoop partition."
		  },
		  "keyCols" : {
		    "type" : "array",
		    "description" : "Optional key columns to distribute records over Spark tasks inside a Hadoop partition.\r\nIf DataObject has Hadoop partitions defined, keyCols must be defined.",
		    "items" : {
		      "type" : "string"
		    }
		  },
		  "sortCols" : {
		    "type" : "array",
		    "description" : "Optional columns to sort records inside files created.",
		    "items" : {
		      "type" : "string"
		    }
		  },
		  "filename" : {
		    "type" : "string",
		    "description" : "Option filename to rename target file(s). If numberOfTasksPerPartition is greater than 1,\r\nmultiple files can exist in a directory and a number is inserted into the filename after the first \\'.\\'.\r\nExample: filename=data.csv -> files created are data.1.csv, data.2.csv, ..."
		  }
		}
	      },
	      "acl" : {
		"type" : "object",
		"description" : "Describes a complete ACL Specification (basic owner/group/other permissions AND extended ACLS)\r\nto be applied to a Data Object on writing",
		"additionalProperties" : false,
		"required" : [ "permission", "acls" ],
		"properties" : {
		  "permission" : {
		    "type" : "string",
		    "description" : ": File system permission string in symbolic notation form (e.g. rwxr-xr-x)"
		  },
		  "acls" : {
		    "type" : "array",
		    "description" : ": a sequence of[[AclElement]] s",
		    "items" : {
		      "type" : "object",
		      "description" : "Describes a single extended ACL to be applied to a Data Object\r\nin addition to the basic file system permissions",
		      "additionalProperties" : false,
		      "required" : [ "aclType", "name", "permission" ],
		      "properties" : {
			"aclType" : {
			  "type" : "string",
			  "description" : ": type of ACL to be added \\\"group\\\", \\\"user\\\""
			},
			"name" : {
			  "type" : "string",
			  "description" : ": the name of the user/group for which an ACL definition is being added"
			},
			"permission" : {
			  "type" : "string",
			  "description" : ": the permission (rwx syntax) to be granted"
			}
		      }
		    }
		  }
		}
	      },
	      "connectionId" : {
		"type" : "string"
	      },
	      "filenameColumn" : {
		"type" : "string"
	      },
	      "expectedPartitionsCondition" : {
		"type" : "string",
		"description" : "Optional definition of partitions expected to exist.\r\nDefine a Spark SQL expression that is evaluated against a[[PartitionValues]] instance and returns true or false\r\nDefault is to expect all partitions to exist."
	      },
	      "housekeepingMode" : {
		"oneOf" : [ {
		  "type" : "object",
		  "description" : "Archive and compact old partitions:\r\nArchive partition reduces the number of partitions in the past by moving older partitions into special \\\"archive partitions\\\".\r\nCompact partition reduces the number of files in a partition by rewriting them with Spark.\r\nExample: archive and compact a table with partition layout run_id=<integer>\r\n- archive partitions after 1000 partitions into \\\"archive partition\\\" equal to floor(run_id/1000)\r\n- compact \\\"archive partition\\\" when full\r\n{{{\r\nhousekeepingMode = {\r\ntype = PartitionArchiveCompactionMode\r\narchivePartitionExpression = \\\"if( elements[\\'run_id\\'] < runId - 1000, map(\\'run_id\\', elements[\\'run_id\\'] div 1000), elements)\\\"\r\ncompactPartitionExpression = \\\"elements[\\'run_id\\'] % 1000 = 0 and elements[\\'run_id\\'] <= runId - 2000\\\"\r\n}\r\n}}}",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "PartitionArchiveCompactionMode"
		    },
		    "archivePartitionExpression" : {
		      "type" : "string",
		      "description" : "Expression to define the archive partition for a given partition. Define a spark\r\nsql expression working with the attributes of[[PartitionExpressionData]] returning archive\r\npartition values as Map[String,String]. If return value is the same as input elements, partition is not touched,\r\notherwise all files of the partition are moved to the returned partition definition.\r\nBe aware that the value of the partition columns changes for these files/records."
		    },
		    "compactPartitionExpression" : {
		      "type" : "string",
		      "description" : "Expression to define partitions which should be compacted. Define a spark\r\nsql expression working with the attributes of[[PartitionExpressionData]] returning a\r\nboolean = true when this partition should be compacted.\r\nOnce a partition is compacted, it is marked as compacted and will not be compacted again.\r\nIt is therefore ok to return true for all partitions which should be compacted, regardless if they have been compacted already."
		    },
		    "description" : {
		      "type" : "string"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Keep partitions while retention condition is fulfilled, delete other partitions.\r\nExample: cleanup partitions with partition layout dt=<yyyymmdd> after 90 days:\r\n{{{\r\nhousekeepingMode = {\r\ntype = PartitionRetentionMode\r\nretentionCondition = \\\"datediff(now(), to_date(elements[\\'dt\\'], \\'yyyyMMdd\\')) <= 90\\\"\r\n}\r\n}}}",
		  "additionalProperties" : false,
		  "required" : [ "retentionCondition", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "PartitionRetentionMode"
		    },
		    "retentionCondition" : {
		      "type" : "string",
		      "description" : "Condition to decide if a partition should be kept. Define a spark sql expression\r\nworking with the attributes of[[PartitionExpressionData]] returning a boolean with value true if the partition should be kept."
		    },
		    "description" : {
		      "type" : "string"
		    }
		  }
		} ]
	      },
	      "metadata" : {
		"oneOf" : [ {
		  "$ref" : "#/definitions/DataObjectMetadata"
		} ]
	      }
	    }
	  },
	  "CustomFileAction" : {
	    "type" : "object",
	    "description" : "[[Action]] to transform files between two Hadoop Data Objects.\r\nThe transformation is executed in distributed mode on the Spark executors.\r\nA custom file transformer must be given, which reads a file from Hadoop and writes it back to Hadoop.",
	    "additionalProperties" : false,
	    "required" : [ "id", "inputId", "outputId", "transformer" ],
	    "properties" : {
	      "id" : {
		"type" : "string"
	      },
	      "inputId" : {
		"type" : "string",
		"description" : "inputs DataObject"
	      },
	      "outputId" : {
		"type" : "string",
		"description" : "output DataObject"
	      },
	      "transformer" : {
		"type" : "object",
		"description" : "Configuration of custom file transformation between one input and one output (1:1)",
		"additionalProperties" : false,
		"properties" : {
		  "className" : {
		    "type" : "string",
		    "description" : "Optional class name to load transformer code from"
		  },
		  "scalaFile" : {
		    "type" : "string",
		    "description" : "Optional file where scala code for transformation is loaded from"
		  },
		  "scalaCode" : {
		    "type" : "string",
		    "description" : "Optional scala code for transformation"
		  },
		  "options" : {
		    "type" : "object",
		    "description" : "Options to pass to the transformation",
		    "additionalProperties" : {
		      "type" : "string"
		    }
		  }
		}
	      },
	      "filesPerPartition" : {
		"type" : "integer",
		"description" : "number of files per Spark partition"
	      },
	      "breakFileRefLineage" : {
		"type" : "string"
	      },
	      "executionMode" : {
		"oneOf" : [ {
		  "type" : "object",
		  "description" : "Execution mode to incrementally process file-based DataObjects.\r\nIt takes all existing files in the input DataObject and removes (deletes) them after processing.\r\nInput partition values are applied when searching for files and also used as output partition values.",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "FileIncrementalMoveMode"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "An execution mode for incremental processing by remembering DataObjects state from last increment.",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "DataObjectStateIncrementalMode"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Spark streaming execution mode uses Spark Structured Streaming to incrementally execute data loads and keep track of processed data.\r\nThis mode needs a DataObject implementing CanCreateStreamingDataFrame and works only with SparkSubFeeds.\r\nThis mode can be executed synchronously in the DAG by using triggerType=Once, or asynchronously as Streaming Query with triggerType = ProcessingTime or Continuous.",
		  "additionalProperties" : false,
		  "required" : [ "checkpointLocation", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "SparkStreamingMode"
		    },
		    "checkpointLocation" : {
		      "type" : "string",
		      "description" : "location for checkpoints of streaming query to keep state"
		    },
		    "triggerType" : {
		      "type" : "string",
		      "description" : "define execution interval of Spark streaming query. Possible values are Once (default), ProcessingTime & Continuous. See[[Trigger]] for details.\r\n                      Note that this is only applied if SDL is executed in streaming mode. If SDL is executed in normal mode, TriggerType=Once is used always.\r\nIf triggerType=Once, the action is repeated with Trigger.Once in SDL streaming mode."
		    },
		    "triggerTime" : {
		      "type" : "string",
		      "description" : "Time as String in triggerType = ProcessingTime or Continuous. See[[Trigger]] for details."
		    },
		    "inputOptions" : {
		      "type" : "object",
		      "description" : "additional option to apply when reading streaming source. This overwrites options set by the DataObjects.",
		      "additionalProperties" : {
			"type" : "string"
		      }
		    },
		    "outputOptions" : {
		      "type" : "object",
		      "description" : "additional option to apply when writing to streaming sink. This overwrites options set by the DataObjects.",
		      "additionalProperties" : {
			"type" : "string"
		      }
		    },
		    "outputMode" : {
		      "type" : "string",
		      "enum" : [ "Append", "Complete", "Update" ]
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Compares max entry in \\\"compare column\\\" between mainOutput and mainInput and incrementally loads the delta.\r\nThis mode works only with SparkSubFeeds. The filter is not propagated to following actions.",
		  "additionalProperties" : false,
		  "required" : [ "compareCol", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "SparkIncrementalMode"
		    },
		    "compareCol" : {
		      "type" : "string",
		      "description" : "a comparable column name existing in mainInput and mainOutput used to identify the delta. Column content should be bigger for newer records."
		    },
		    "alternativeOutputId" : {
		      "type" : "string",
		      "description" : "optional alternative outputId of DataObject later in the DAG. This replaces the mainOutputId.\r\nIt can be used to ensure processing all partitions over multiple actions in case of errors."
		    },
		    "applyCondition" : {
		      "type" : "object",
		      "description" : "Definition of a Spark SQL condition with description.\r\nThis is used for example to define failConditions of[[PartitionDiffMode]] .",
		      "additionalProperties" : false,
		      "required" : [ "expression" ],
		      "properties" : {
			"expression" : {
			  "type" : "string",
			  "description" : "Condition formulated as Spark SQL. The attributes available are dependent on the context."
			},
			"description" : {
			  "type" : "string",
			  "description" : "A textual description of the condition to be shown in error messages."
			}
		      }
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "An execution mode which forces processing all data from it\\'s inputs.",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "ProcessAllMode"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "An execution mode which just validates that partition values are given.\r\nNote: For start nodes of the DAG partition values can be defined by command line, for subsequent nodes partition values are passed on from previous nodes.",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "FailIfNoPartitionValuesMode"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Partition difference execution mode lists partitions on mainInput & mainOutput DataObject and starts loading all missing partitions.\r\nPartition columns to be used for comparision need to be a common \\'init\\' of input and output partition columns.\r\nThis mode needs mainInput/Output DataObjects which CanHandlePartitions to list partitions.\r\nPartition values are passed to following actions for partition columns which they have in common.",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "PartitionDiffMode"
		    },
		    "partitionColNb" : {
		      "type" : "integer",
		      "description" : "optional number of partition columns to use as a common \\'init\\'."
		    },
		    "alternativeOutputId" : {
		      "type" : "string",
		      "description" : "optional alternative outputId of DataObject later in the DAG. This replaces the mainOutputId.\r\nIt can be used to ensure processing all partitions over multiple actions in case of errors."
		    },
		    "nbOfPartitionValuesPerRun" : {
		      "type" : "integer",
		      "description" : "optional restriction of the number of partition values per run."
		    },
		    "applyCondition" : {
		      "type" : "string",
		      "description" : "Condition to decide if execution mode should be applied or not. Define a spark sql expression working with attributes of[[DefaultExecutionModeExpressionData]] returning a boolean.\r\nDefault is to apply the execution mode if given partition values (partition values from command line or passed from previous action) are not empty."
		    },
		    "failCondition" : {
		      "type" : "string"
		    },
		    "failConditions" : {
		      "type" : "array",
		      "description" : "List of conditions to fail application of execution mode if true. Define as spark sql expressions working with attributes of[[PartitionDiffModeExpressionData]] returning a boolean.\r\nDefault is that the application of the PartitionDiffMode does not fail the action. If there is no data to process, the following actions are skipped.\r\nMultiple conditions are evaluated individually and every condition may fail the execution mode (or-logic)",
		      "items" : {
			"type" : "object",
			"description" : "Definition of a Spark SQL condition with description.\r\nThis is used for example to define failConditions of[[PartitionDiffMode]] .",
			"additionalProperties" : false,
			"required" : [ "expression" ],
			"properties" : {
			  "expression" : {
			    "type" : "string",
			    "description" : "Condition formulated as Spark SQL. The attributes available are dependent on the context."
			  },
			  "description" : {
			    "type" : "string",
			    "description" : "A textual description of the condition to be shown in error messages."
			  }
			}
		      }
		    },
		    "selectExpression" : {
		      "type" : "string",
		      "description" : "optional expression to define or refine the list of selected output partitions. Define a spark sql expression working with the attributes of[[PartitionDiffModeExpressionData]] returning a list<map<string,string>>.\r\nDefault is to return the originally selected output partitions found in attribute selectedOutputPartitionValues."
		    },
		    "applyPartitionValuesTransform" : {
		      "type" : "string",
		      "description" : "If true applies the partition values transform of custom transformations on input partition values before comparison with output partition values.\r\nIf enabled input and output partition columns can be different. Default is to disable the transformation of partition values."
		    },
		    "selectAdditionalInputExpression" : {
		      "type" : "string",
		      "description" : "optional expression to refine the list of selected input partitions. Note that primarily output partitions are selected by PartitionDiffMode.\r\nThe selected output partitions are then transformed back to the input partitions needed to create the selected output partitions. This is one-to-one except if applyPartitionValuesTransform=true.\r\nAnd sometimes there is a need for additional input data to create the output partitions, e.g. if you aggregate a window of 7 days for every day.\r\nYou can customize selected input partitions by defining a spark sql expression working with the attributes of[[PartitionDiffModeExpressionData]] returning a list<map<string,string>>.\r\nDefault is to return the originally selected input partitions found in attribute selectedInputPartitionValues."
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Execution mode to create custom partition execution mode logic.\r\nDefine a function which receives main input&output DataObject and returns partition values to process as Seq[Map[String,String]\\\\]",
		  "additionalProperties" : false,
		  "required" : [ "className", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "CustomPartitionMode"
		    },
		    "className" : {
		      "type" : "string",
		      "description" : "class name implementing trait[[CustomPartitionModeLogic]]"
		    },
		    "alternativeOutputId" : {
		      "type" : "string",
		      "description" : "optional alternative outputId of DataObject later in the DAG. This replaces the mainOutputId.\r\nIt can be used to ensure processing all partitions over multiple actions in case of errors."
		    },
		    "options" : {
		      "type" : "object",
		      "description" : "Options specified in the configuration for this execution mode",
		      "additionalProperties" : {
			"type" : "string"
		      }
		    }
		  }
		} ]
	      },
	      "executionCondition" : {
		"type" : "object",
		"description" : "Definition of a Spark SQL condition with description.\r\nThis is used for example to define failConditions of[[PartitionDiffMode]] .",
		"additionalProperties" : false,
		"required" : [ "expression" ],
		"properties" : {
		  "expression" : {
		    "type" : "string",
		    "description" : "Condition formulated as Spark SQL. The attributes available are dependent on the context."
		  },
		  "description" : {
		    "type" : "string",
		    "description" : "A textual description of the condition to be shown in error messages."
		  }
		}
	      },
	      "metricsFailCondition" : {
		"type" : "string",
		"description" : "optional spark sql expression evaluated as where-clause against dataframe of metrics. Available columns are dataObjectId, key, value.\r\nIf there are any rows passing the where clause, a MetricCheckFailed exception is thrown."
	      },
	      "metadata" : {
		"oneOf" : [ {
		  "$ref" : "#/definitions/ActionMetadata"
		} ]
	      }
	    }
	  },
	  "CustomScriptAction" : {
	    "type" : "object",
	    "description" : "[[Action]] execute script after multiple input DataObjects are ready, notifying multiple output DataObjects when script succeeded.",
	    "additionalProperties" : false,
	    "required" : [ "id", "inputIds", "outputIds" ],
	    "properties" : {
	      "id" : {
		"type" : "string"
	      },
	      "inputIds" : {
		"type" : "array",
		"description" : "input DataObject\\'s",
		"items" : {
		  "type" : "string"
		}
	      },
	      "outputIds" : {
		"type" : "array",
		"description" : "output DataObject\\'s",
		"items" : {
		  "type" : "string"
		}
	      },
	      "scripts" : {
		"type" : "array",
		"description" : "definition of scripts to execute",
		"items" : {
		  "oneOf" : [ {
		    "type" : "object",
		    "description" : "Execute a command.\r\nCommand can be different for windows and linux operating systems, but it must be defined for at least one of them.\r\nIf return value is not zero an exception is thrown.\r\nThe last line of the scripts standard output is parsed as key-value and passed on as parameters in the output subfeed.\r\nKey-value format: k1=v1 k2=v2",
		    "additionalProperties" : false,
		    "required" : [ "type" ],
		    "properties" : {
		      "type" : {
			"const" : "CmdScript"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      },
		      "winCmd" : {
			"type" : "string",
			"description" : "Cmd to execute on windows operating systems - note that it is executed with \\\"cmd /C\\\" prefixed"
		      },
		      "linuxCmd" : {
			"type" : "string",
			"description" : "Cmd to execute on linux operating systems - note that it is executed with \\\"sh -c\\\" prefixed."
		      }
		    }
		  } ]
		}
	      },
	      "executionCondition" : {
		"type" : "object",
		"description" : "Definition of a Spark SQL condition with description.\r\nThis is used for example to define failConditions of[[PartitionDiffMode]] .",
		"additionalProperties" : false,
		"required" : [ "expression" ],
		"properties" : {
		  "expression" : {
		    "type" : "string",
		    "description" : "Condition formulated as Spark SQL. The attributes available are dependent on the context."
		  },
		  "description" : {
		    "type" : "string",
		    "description" : "A textual description of the condition to be shown in error messages."
		  }
		}
	      },
	      "metadata" : {
		"oneOf" : [ {
		  "$ref" : "#/definitions/ActionMetadata"
		} ]
	      }
	    }
	  },
	  "JdbcTableConnection" : {
	    "type" : "object",
	    "description" : "Connection information for jdbc tables.\r\nIf authentication is needed, user and password must be provided.",
	    "additionalProperties" : false,
	    "required" : [ "id", "url", "driver" ],
	    "properties" : {
	      "id" : {
		"type" : "string",
		"description" : "unique id of this connection"
	      },
	      "url" : {
		"type" : "string",
		"description" : "jdbc connection url"
	      },
	      "driver" : {
		"type" : "string",
		"description" : "class name of jdbc driver"
	      },
	      "authMode" : {
		"oneOf" : [ {
		  "type" : "object",
		  "description" : "Connect by custom authorization header",
		  "additionalProperties" : false,
		  "required" : [ "secretVariable", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "AuthHeaderMode"
		    },
		    "headerName" : {
		      "type" : "string"
		    },
		    "secretVariable" : {
		      "type" : "string"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Validate by SSL Certificates : Only location an credentials. Additional attributes should be\r\nsupplied via options map",
		  "additionalProperties" : false,
		  "required" : [ "keystorePath", "keystorePassVariable", "truststorePath", "truststorePassVariable", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "SSLCertsAuthMode"
		    },
		    "keystorePath" : {
		      "type" : "string"
		    },
		    "keystoreType" : {
		      "type" : "string"
		    },
		    "keystorePassVariable" : {
		      "type" : "string"
		    },
		    "truststorePath" : {
		      "type" : "string"
		    },
		    "truststoreType" : {
		      "type" : "string"
		    },
		    "truststorePassVariable" : {
		      "type" : "string"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Connect by basic authentication",
		  "additionalProperties" : false,
		  "required" : [ "userVariable", "passwordVariable", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "BasicAuthMode"
		    },
		    "userVariable" : {
		      "type" : "string"
		    },
		    "passwordVariable" : {
		      "type" : "string"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Connect with custom HTTP authentication",
		  "additionalProperties" : false,
		  "required" : [ "className", "options", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "CustomHttpAuthMode"
		    },
		    "className" : {
		      "type" : "string",
		      "description" : "class name implementing trait[[CustomHttpAuthModeLogic]]"
		    },
		    "options" : {
		      "type" : "object",
		      "description" : "Options to pass to the custom auth mode logc in prepare function",
		      "additionalProperties" : {
			"type" : "string"
		      }
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Validate by SASL_SSL Authentication : user / password and truststore",
		  "additionalProperties" : false,
		  "required" : [ "username", "passwordVariable", "sslMechanism", "truststorePath", "truststorePassVariable", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "SASLSCRAMAuthMode"
		    },
		    "username" : {
		      "type" : "string"
		    },
		    "passwordVariable" : {
		      "type" : "string"
		    },
		    "sslMechanism" : {
		      "type" : "string"
		    },
		    "truststorePath" : {
		      "type" : "string"
		    },
		    "truststoreType" : {
		      "type" : "string"
		    },
		    "truststorePassVariable" : {
		      "type" : "string"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Connect by token\r\nFor HTTP Connection this is used as Bearer token in Authorization header.",
		  "additionalProperties" : false,
		  "required" : [ "tokenVariable", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "TokenAuthMode"
		    },
		    "tokenVariable" : {
		      "type" : "string"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Connect by using Keycloak to manage token and token refresh giving clientId/secret as information.\r\nFor HTTP Connection this is used as Bearer token in Authorization header.",
		  "additionalProperties" : false,
		  "required" : [ "ssoServer", "ssoRealm", "ssoGrantType", "clientIdVariable", "clientSecretVariable", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "KeycloakClientSecretAuthMode"
		    },
		    "ssoServer" : {
		      "type" : "string"
		    },
		    "ssoRealm" : {
		      "type" : "string"
		    },
		    "ssoGrantType" : {
		      "type" : "string"
		    },
		    "clientIdVariable" : {
		      "type" : "string"
		    },
		    "clientSecretVariable" : {
		      "type" : "string"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Validate by user and private/public key\r\nPrivate key is read from .ssh",
		  "additionalProperties" : false,
		  "required" : [ "userVariable", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "PublicKeyAuthMode"
		    },
		    "userVariable" : {
		      "type" : "string"
		    }
		  }
		} ]
	      },
	      "db" : {
		"type" : "string",
		"description" : "jdbc database"
	      },
	      "maxParallelConnections" : {
		"type" : "integer",
		"description" : "number of parallel jdbc connections created by an instance of this connection\r\nNote that Spark manages JDBC Connections on its own. This setting only applies to JDBC connection\r\nused by SDL for validating metadata or pre/postSQL."
	      },
	      "connectionPoolMaxIdleTimeSec" : {
		"type" : "integer",
		"description" : "timeout to close unused connections in the pool"
	      },
	      "metadata" : {
		"oneOf" : [ {
		  "$ref" : "#/definitions/ConnectionMetadata"
		} ]
	      }
	    }
	  },
	  "FileTransferAction" : {
	    "type" : "object",
	    "description" : "[[Action]] to transfer files between SFtp, Hadoop and local Fs.",
	    "additionalProperties" : false,
	    "required" : [ "id", "inputId", "outputId" ],
	    "properties" : {
	      "id" : {
		"type" : "string"
	      },
	      "inputId" : {
		"type" : "string",
		"description" : "inputs DataObject"
	      },
	      "outputId" : {
		"type" : "string",
		"description" : "output DataObject"
	      },
	      "overwrite" : {
		"type" : "string"
	      },
	      "breakFileRefLineage" : {
		"type" : "string",
		"description" : "If set to true, file references passed on from previous action are ignored by this action.\r\nThe action will detect on its own what files it is going to process."
	      },
	      "executionMode" : {
		"oneOf" : [ {
		  "type" : "object",
		  "description" : "Execution mode to incrementally process file-based DataObjects.\r\nIt takes all existing files in the input DataObject and removes (deletes) them after processing.\r\nInput partition values are applied when searching for files and also used as output partition values.",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "FileIncrementalMoveMode"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "An execution mode for incremental processing by remembering DataObjects state from last increment.",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "DataObjectStateIncrementalMode"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Spark streaming execution mode uses Spark Structured Streaming to incrementally execute data loads and keep track of processed data.\r\nThis mode needs a DataObject implementing CanCreateStreamingDataFrame and works only with SparkSubFeeds.\r\nThis mode can be executed synchronously in the DAG by using triggerType=Once, or asynchronously as Streaming Query with triggerType = ProcessingTime or Continuous.",
		  "additionalProperties" : false,
		  "required" : [ "checkpointLocation", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "SparkStreamingMode"
		    },
		    "checkpointLocation" : {
		      "type" : "string",
		      "description" : "location for checkpoints of streaming query to keep state"
		    },
		    "triggerType" : {
		      "type" : "string",
		      "description" : "define execution interval of Spark streaming query. Possible values are Once (default), ProcessingTime & Continuous. See[[Trigger]] for details.\r\n                      Note that this is only applied if SDL is executed in streaming mode. If SDL is executed in normal mode, TriggerType=Once is used always.\r\nIf triggerType=Once, the action is repeated with Trigger.Once in SDL streaming mode."
		    },
		    "triggerTime" : {
		      "type" : "string",
		      "description" : "Time as String in triggerType = ProcessingTime or Continuous. See[[Trigger]] for details."
		    },
		    "inputOptions" : {
		      "type" : "object",
		      "description" : "additional option to apply when reading streaming source. This overwrites options set by the DataObjects.",
		      "additionalProperties" : {
			"type" : "string"
		      }
		    },
		    "outputOptions" : {
		      "type" : "object",
		      "description" : "additional option to apply when writing to streaming sink. This overwrites options set by the DataObjects.",
		      "additionalProperties" : {
			"type" : "string"
		      }
		    },
		    "outputMode" : {
		      "type" : "string",
		      "enum" : [ "Append", "Complete", "Update" ]
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Compares max entry in \\\"compare column\\\" between mainOutput and mainInput and incrementally loads the delta.\r\nThis mode works only with SparkSubFeeds. The filter is not propagated to following actions.",
		  "additionalProperties" : false,
		  "required" : [ "compareCol", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "SparkIncrementalMode"
		    },
		    "compareCol" : {
		      "type" : "string",
		      "description" : "a comparable column name existing in mainInput and mainOutput used to identify the delta. Column content should be bigger for newer records."
		    },
		    "alternativeOutputId" : {
		      "type" : "string",
		      "description" : "optional alternative outputId of DataObject later in the DAG. This replaces the mainOutputId.\r\nIt can be used to ensure processing all partitions over multiple actions in case of errors."
		    },
		    "applyCondition" : {
		      "type" : "object",
		      "description" : "Definition of a Spark SQL condition with description.\r\nThis is used for example to define failConditions of[[PartitionDiffMode]] .",
		      "additionalProperties" : false,
		      "required" : [ "expression" ],
		      "properties" : {
			"expression" : {
			  "type" : "string",
			  "description" : "Condition formulated as Spark SQL. The attributes available are dependent on the context."
			},
			"description" : {
			  "type" : "string",
			  "description" : "A textual description of the condition to be shown in error messages."
			}
		      }
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "An execution mode which forces processing all data from it\\'s inputs.",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "ProcessAllMode"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "An execution mode which just validates that partition values are given.\r\nNote: For start nodes of the DAG partition values can be defined by command line, for subsequent nodes partition values are passed on from previous nodes.",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "FailIfNoPartitionValuesMode"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Partition difference execution mode lists partitions on mainInput & mainOutput DataObject and starts loading all missing partitions.\r\nPartition columns to be used for comparision need to be a common \\'init\\' of input and output partition columns.\r\nThis mode needs mainInput/Output DataObjects which CanHandlePartitions to list partitions.\r\nPartition values are passed to following actions for partition columns which they have in common.",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "PartitionDiffMode"
		    },
		    "partitionColNb" : {
		      "type" : "integer",
		      "description" : "optional number of partition columns to use as a common \\'init\\'."
		    },
		    "alternativeOutputId" : {
		      "type" : "string",
		      "description" : "optional alternative outputId of DataObject later in the DAG. This replaces the mainOutputId.\r\nIt can be used to ensure processing all partitions over multiple actions in case of errors."
		    },
		    "nbOfPartitionValuesPerRun" : {
		      "type" : "integer",
		      "description" : "optional restriction of the number of partition values per run."
		    },
		    "applyCondition" : {
		      "type" : "string",
		      "description" : "Condition to decide if execution mode should be applied or not. Define a spark sql expression working with attributes of[[DefaultExecutionModeExpressionData]] returning a boolean.\r\nDefault is to apply the execution mode if given partition values (partition values from command line or passed from previous action) are not empty."
		    },
		    "failCondition" : {
		      "type" : "string"
		    },
		    "failConditions" : {
		      "type" : "array",
		      "description" : "List of conditions to fail application of execution mode if true. Define as spark sql expressions working with attributes of[[PartitionDiffModeExpressionData]] returning a boolean.\r\nDefault is that the application of the PartitionDiffMode does not fail the action. If there is no data to process, the following actions are skipped.\r\nMultiple conditions are evaluated individually and every condition may fail the execution mode (or-logic)",
		      "items" : {
			"type" : "object",
			"description" : "Definition of a Spark SQL condition with description.\r\nThis is used for example to define failConditions of[[PartitionDiffMode]] .",
			"additionalProperties" : false,
			"required" : [ "expression" ],
			"properties" : {
			  "expression" : {
			    "type" : "string",
			    "description" : "Condition formulated as Spark SQL. The attributes available are dependent on the context."
			  },
			  "description" : {
			    "type" : "string",
			    "description" : "A textual description of the condition to be shown in error messages."
			  }
			}
		      }
		    },
		    "selectExpression" : {
		      "type" : "string",
		      "description" : "optional expression to define or refine the list of selected output partitions. Define a spark sql expression working with the attributes of[[PartitionDiffModeExpressionData]] returning a list<map<string,string>>.\r\nDefault is to return the originally selected output partitions found in attribute selectedOutputPartitionValues."
		    },
		    "applyPartitionValuesTransform" : {
		      "type" : "string",
		      "description" : "If true applies the partition values transform of custom transformations on input partition values before comparison with output partition values.\r\nIf enabled input and output partition columns can be different. Default is to disable the transformation of partition values."
		    },
		    "selectAdditionalInputExpression" : {
		      "type" : "string",
		      "description" : "optional expression to refine the list of selected input partitions. Note that primarily output partitions are selected by PartitionDiffMode.\r\nThe selected output partitions are then transformed back to the input partitions needed to create the selected output partitions. This is one-to-one except if applyPartitionValuesTransform=true.\r\nAnd sometimes there is a need for additional input data to create the output partitions, e.g. if you aggregate a window of 7 days for every day.\r\nYou can customize selected input partitions by defining a spark sql expression working with the attributes of[[PartitionDiffModeExpressionData]] returning a list<map<string,string>>.\r\nDefault is to return the originally selected input partitions found in attribute selectedInputPartitionValues."
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Execution mode to create custom partition execution mode logic.\r\nDefine a function which receives main input&output DataObject and returns partition values to process as Seq[Map[String,String]\\\\]",
		  "additionalProperties" : false,
		  "required" : [ "className", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "CustomPartitionMode"
		    },
		    "className" : {
		      "type" : "string",
		      "description" : "class name implementing trait[[CustomPartitionModeLogic]]"
		    },
		    "alternativeOutputId" : {
		      "type" : "string",
		      "description" : "optional alternative outputId of DataObject later in the DAG. This replaces the mainOutputId.\r\nIt can be used to ensure processing all partitions over multiple actions in case of errors."
		    },
		    "options" : {
		      "type" : "object",
		      "description" : "Options specified in the configuration for this execution mode",
		      "additionalProperties" : {
			"type" : "string"
		      }
		    }
		  }
		} ]
	      },
	      "executionCondition" : {
		"type" : "object",
		"description" : "Definition of a Spark SQL condition with description.\r\nThis is used for example to define failConditions of[[PartitionDiffMode]] .",
		"additionalProperties" : false,
		"required" : [ "expression" ],
		"properties" : {
		  "expression" : {
		    "type" : "string",
		    "description" : "Condition formulated as Spark SQL. The attributes available are dependent on the context."
		  },
		  "description" : {
		    "type" : "string",
		    "description" : "A textual description of the condition to be shown in error messages."
		  }
		}
	      },
	      "metricsFailCondition" : {
		"type" : "string",
		"description" : "optional spark sql expression evaluated as where-clause against dataframe of metrics. Available columns are dataObjectId, key, value.\r\nIf there are any rows passing the where clause, a MetricCheckFailed exception is thrown."
	      },
	      "metadata" : {
		"oneOf" : [ {
		  "$ref" : "#/definitions/ActionMetadata"
		} ]
	      }
	    }
	  },
	  "ConnectionMetadata" : {
	    "type" : "object",
	    "description" : "Additional metadata for a Connection",
	    "additionalProperties" : false,
	    "properties" : {
	      "name" : {
		"type" : "string",
		"description" : "Readable name of the Connection"
	      },
	      "description" : {
		"type" : "string",
		"description" : "Description of the content of the Connection"
	      },
	      "layer" : {
		"type" : "string",
		"description" : "Name of the layer this Connection belongs to"
	      },
	      "subjectArea" : {
		"type" : "string",
		"description" : "Name of the subject area this Connection belongs to"
	      },
	      "tags" : {
		"type" : "array",
		"description" : "Optional custom tags for this object",
		"items" : {
		  "type" : "string"
		}
	      }
	    }
	  },
	  "CustomFileDataObject" : {
	    "type" : "object",
	    "additionalProperties" : false,
	    "required" : [ "id", "creator" ],
	    "properties" : {
	      "id" : {
		"type" : "string"
	      },
	      "creator" : {
		"type" : "object",
		"additionalProperties" : false,
		"properties" : {
		  "className" : {
		    "type" : "string"
		  },
		  "scalaFile" : {
		    "type" : "string"
		  },
		  "scalaCode" : {
		    "type" : "string"
		  },
		  "options" : {
		    "type" : "object",
		    "additionalProperties" : {
		      "type" : "string"
		    }
		  }
		}
	      },
	      "metadata" : {
		"oneOf" : [ {
		  "$ref" : "#/definitions/DataObjectMetadata"
		} ]
	      }
	    }
	  },
	  "JsonFileDataObject" : {
	    "type" : "object",
	    "description" : "A[[io.smartdatalake.workflow.dataobject.DataObject]]backed by a JSON data source.\r\nIt manages read and write access and configurations required for[[io.smartdatalake.workflow.action.Action]]s to\r\nwork on JSON formatted files.\r\nReading and writing details are delegated to Apache Spark[[org.apache.spark.sql.DataFrameReader]]\r\nand[[org.apache.spark.sql.DataFrameWriter]] respectively.\r\n\nNOTE: By default, the JSON option`multiline` is enabled.\r\n\nSEE: [[org.apache.spark.sql.DataFrameReader]]\r\nSEE: [[org.apache.spark.sql.DataFrameWriter]]",
	    "additionalProperties" : false,
	    "required" : [ "id", "path" ],
	    "properties" : {
	      "id" : {
		"type" : "string"
	      },
	      "path" : {
		"type" : "string"
	      },
	      "jsonOptions" : {
		"type" : "object",
		"description" : "Settings for the underlying[[org.apache.spark.sql.DataFrameReader]]and\r\n[[org.apache.spark.sql.DataFrameWriter]] .",
		"additionalProperties" : {
		  "type" : "string"
		}
	      },
	      "partitions" : {
		"type" : "array",
		"items" : {
		  "type" : "string"
		}
	      },
	      "schema" : {
		"type" : "string",
		"description" : "An optional data object schema. If defined, any automatic schema inference is avoided. As this corresponds to the schema on write, it must not include the optional filenameColumn on read."
	      },
	      "schemaMin" : {
		"type" : "string"
	      },
	      "saveMode" : {
		"type" : "string",
		"enum" : [ "Merge ", "OverwriteOptimized ", "OverwritePreserveDirectories ", "Ignore ", "ErrorIfExists ", "Append ", "Overwrite " ]
	      },
	      "sparkRepartition" : {
		"type" : "object",
		"description" : "This controls repartitioning of the DataFrame before writing with Spark to Hadoop.\r\nWhen writing multiple partitions of a partitioned DataObject, the number of spark tasks created is equal to numberOfTasksPerPartition\r\nmultiplied with the number of partitions to write. To spread the records of a partition only over numberOfTasksPerPartition spark tasks,\r\nkeyCols must be given which are used to derive a task number inside the partition (hashvalue(keyCols) modulo numberOfTasksPerPartition).\r\nWhen writing to an unpartitioned DataObject or only one partition of a partitioned DataObject, the number of spark tasks created is equal\r\nto numberOfTasksPerPartition. Optional keyCols can be used to keep corresponding records together in the same task/file.",
		"additionalProperties" : false,
		"required" : [ "numberOfTasksPerPartition" ],
		"properties" : {
		  "numberOfTasksPerPartition" : {
		    "type" : "integer",
		    "description" : "Number of Spark tasks to create per partition before writing to DataObject by repartitioning the DataFrame.\r\nThis controls how many files are created in each Hadoop partition."
		  },
		  "keyCols" : {
		    "type" : "array",
		    "description" : "Optional key columns to distribute records over Spark tasks inside a Hadoop partition.\r\nIf DataObject has Hadoop partitions defined, keyCols must be defined.",
		    "items" : {
		      "type" : "string"
		    }
		  },
		  "sortCols" : {
		    "type" : "array",
		    "description" : "Optional columns to sort records inside files created.",
		    "items" : {
		      "type" : "string"
		    }
		  },
		  "filename" : {
		    "type" : "string",
		    "description" : "Option filename to rename target file(s). If numberOfTasksPerPartition is greater than 1,\r\nmultiple files can exist in a directory and a number is inserted into the filename after the first \\'.\\'.\r\nExample: filename=data.csv -> files created are data.1.csv, data.2.csv, ..."
		  }
		}
	      },
	      "stringify" : {
		"type" : "string",
		"description" : "Set the data type for all values to string."
	      },
	      "acl" : {
		"type" : "object",
		"description" : "Describes a complete ACL Specification (basic owner/group/other permissions AND extended ACLS)\r\nto be applied to a Data Object on writing",
		"additionalProperties" : false,
		"required" : [ "permission", "acls" ],
		"properties" : {
		  "permission" : {
		    "type" : "string",
		    "description" : ": File system permission string in symbolic notation form (e.g. rwxr-xr-x)"
		  },
		  "acls" : {
		    "type" : "array",
		    "description" : ": a sequence of[[AclElement]] s",
		    "items" : {
		      "type" : "object",
		      "description" : "Describes a single extended ACL to be applied to a Data Object\r\nin addition to the basic file system permissions",
		      "additionalProperties" : false,
		      "required" : [ "aclType", "name", "permission" ],
		      "properties" : {
			"aclType" : {
			  "type" : "string",
			  "description" : ": type of ACL to be added \\\"group\\\", \\\"user\\\""
			},
			"name" : {
			  "type" : "string",
			  "description" : ": the name of the user/group for which an ACL definition is being added"
			},
			"permission" : {
			  "type" : "string",
			  "description" : ": the permission (rwx syntax) to be granted"
			}
		      }
		    }
		  }
		}
	      },
	      "connectionId" : {
		"type" : "string"
	      },
	      "filenameColumn" : {
		"type" : "string"
	      },
	      "expectedPartitionsCondition" : {
		"type" : "string",
		"description" : "Optional definition of partitions expected to exist.\r\nDefine a Spark SQL expression that is evaluated against a[[PartitionValues]] instance and returns true or false\r\nDefault is to expect all partitions to exist."
	      },
	      "housekeepingMode" : {
		"oneOf" : [ {
		  "type" : "object",
		  "description" : "Archive and compact old partitions:\r\nArchive partition reduces the number of partitions in the past by moving older partitions into special \\\"archive partitions\\\".\r\nCompact partition reduces the number of files in a partition by rewriting them with Spark.\r\nExample: archive and compact a table with partition layout run_id=<integer>\r\n- archive partitions after 1000 partitions into \\\"archive partition\\\" equal to floor(run_id/1000)\r\n- compact \\\"archive partition\\\" when full\r\n{{{\r\nhousekeepingMode = {\r\ntype = PartitionArchiveCompactionMode\r\narchivePartitionExpression = \\\"if( elements[\\'run_id\\'] < runId - 1000, map(\\'run_id\\', elements[\\'run_id\\'] div 1000), elements)\\\"\r\ncompactPartitionExpression = \\\"elements[\\'run_id\\'] % 1000 = 0 and elements[\\'run_id\\'] <= runId - 2000\\\"\r\n}\r\n}}}",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "PartitionArchiveCompactionMode"
		    },
		    "archivePartitionExpression" : {
		      "type" : "string",
		      "description" : "Expression to define the archive partition for a given partition. Define a spark\r\nsql expression working with the attributes of[[PartitionExpressionData]] returning archive\r\npartition values as Map[String,String]. If return value is the same as input elements, partition is not touched,\r\notherwise all files of the partition are moved to the returned partition definition.\r\nBe aware that the value of the partition columns changes for these files/records."
		    },
		    "compactPartitionExpression" : {
		      "type" : "string",
		      "description" : "Expression to define partitions which should be compacted. Define a spark\r\nsql expression working with the attributes of[[PartitionExpressionData]] returning a\r\nboolean = true when this partition should be compacted.\r\nOnce a partition is compacted, it is marked as compacted and will not be compacted again.\r\nIt is therefore ok to return true for all partitions which should be compacted, regardless if they have been compacted already."
		    },
		    "description" : {
		      "type" : "string"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Keep partitions while retention condition is fulfilled, delete other partitions.\r\nExample: cleanup partitions with partition layout dt=<yyyymmdd> after 90 days:\r\n{{{\r\nhousekeepingMode = {\r\ntype = PartitionRetentionMode\r\nretentionCondition = \\\"datediff(now(), to_date(elements[\\'dt\\'], \\'yyyyMMdd\\')) <= 90\\\"\r\n}\r\n}}}",
		  "additionalProperties" : false,
		  "required" : [ "retentionCondition", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "PartitionRetentionMode"
		    },
		    "retentionCondition" : {
		      "type" : "string",
		      "description" : "Condition to decide if a partition should be kept. Define a spark sql expression\r\nworking with the attributes of[[PartitionExpressionData]] returning a boolean with value true if the partition should be kept."
		    },
		    "description" : {
		      "type" : "string"
		    }
		  }
		} ]
	      },
	      "metadata" : {
		"oneOf" : [ {
		  "$ref" : "#/definitions/DataObjectMetadata"
		} ]
	      }
	    }
	  },
	  "CopyAction" : {
	    "type" : "object",
	    "description" : "[[Action]] to copy files (i.e. from stage to integration)",
	    "additionalProperties" : false,
	    "required" : [ "id", "inputId", "outputId" ],
	    "properties" : {
	      "id" : {
		"type" : "string"
	      },
	      "inputId" : {
		"type" : "string",
		"description" : "inputs DataObject"
	      },
	      "outputId" : {
		"type" : "string",
		"description" : "output DataObject"
	      },
	      "deleteDataAfterRead" : {
		"type" : "string",
		"description" : "a flag to enable deletion of input partitions after copying."
	      },
	      "transformer" : {
		"type" : "object",
		"description" : "Configuration of a custom Spark-DataFrame transformation between one input and one output (1:1)\r\nDefine a transform function which receives a DataObjectIds, a DataFrames and a map of options and has to return a\r\nDataFrame, see also[[CustomDfTransformer]].\r\nNote about Python transformation: Environment with Python and PySpark needed.\r\nPySpark session is initialize and available under variables`sc`,`session`,`sqlContext`.\r\nOther variables available are\r\n-`inputDf`: Input DataFrame\r\n-`options`: Transformation options as Map[String,String]\r\n-`dataObjectId`: Id of input dataObject as String\r\nOutput DataFrame must be set with`setOutputDf(df)` .",
		"additionalProperties" : false,
		"properties" : {
		  "className" : {
		    "type" : "string",
		    "description" : "Optional class name implementing trait[[CustomDfTransformer]]"
		  },
		  "scalaFile" : {
		    "type" : "string",
		    "description" : "Optional file where scala code for transformation is loaded from. The scala code in the file needs to be a function of type[[fnTransformType]] ."
		  },
		  "scalaCode" : {
		    "type" : "string",
		    "description" : "Optional scala code for transformation. The scala code needs to be a function of type[[fnTransformType]] ."
		  },
		  "sqlCode" : {
		    "type" : "string",
		    "description" : "Optional SQL code for transformation.\r\nUse tokens %{<key>} to replace with runtimeOptions in SQL code.\r\nExample: \\\"select * from test where run = %{runId}\\\""
		  },
		  "pythonFile" : {
		    "type" : "string",
		    "description" : "Optional pythonFile to use for python transformation. The python code can use variables inputDf, dataObjectId and options. The transformed DataFrame has to be set with setOutputDf."
		  },
		  "pythonCode" : {
		    "type" : "string",
		    "description" : "Optional pythonCode to use for python transformation. The python code can use variables inputDf, dataObjectId and options. The transformed DataFrame has to be set with setOutputDf."
		  },
		  "options" : {
		    "type" : "object",
		    "description" : "Options to pass to the transformation",
		    "additionalProperties" : {
		      "type" : "string"
		    }
		  },
		  "runtimeOptions" : {
		    "type" : "object",
		    "description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\r\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
		    "additionalProperties" : {
		      "type" : "string"
		    }
		  }
		}
	      },
	      "transformers" : {
		"type" : "array",
		"description" : "optional list of transformations to apply. See[[sparktransformer]] for a list of included Transformers.\r\nThe transformations are applied according to the lists ordering.",
		"items" : {
		  "oneOf" : [ {
		    "type" : "object",
		    "description" : "Interface to implement Spark-DataFrame transformers working with one input and one output (1:1).\r\nThis trait extends DfSparkTransformer to pass a map of options as parameter to the transform function. This is mainly\r\nused by custom transformers.",
		    "additionalProperties" : false,
		    "required" : [ "type" ],
		    "properties" : {
		      "type" : {
			"const" : "OptionsDfTransformer"
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Repartition DataFrame\r\nFor detailled description about repartitioning DataFrames see also[[SparkRepartitionDef]]",
		    "additionalProperties" : false,
		    "required" : [ "numberOfTasksPerPartition", "type" ],
		    "properties" : {
		      "type" : {
			"const" : "RepartitionTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      },
		      "numberOfTasksPerPartition" : {
			"type" : "integer",
			"description" : "Number of Spark tasks to create per partition value by repartitioning the DataFrame."
		      },
		      "keyCols" : {
			"type" : "array",
			"description" : "Optional key columns to distribute records over Spark tasks inside a partition value.",
			"items" : {
			  "type" : "string"
			}
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Standardize datatypes of a DataFrame.\r\nCurrent implementation converts all decimal datatypes to a corresponding integral or float datatype",
		    "additionalProperties" : false,
		    "required" : [ "type" ],
		    "properties" : {
		      "type" : {
			"const" : "StandardizeDatatypesTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Configuration of a custom Spark-DataFrame transformation between one input and one output (1:1) as Scala code which is compiled at runtime.\r\nDefine a transform function which receives a DataObjectId, a DataFrame and a map of options and has to return a\r\nDataFrame. The scala code has to implement a function of type[[fnTransformType]] .",
		    "additionalProperties" : false,
		    "required" : [ "type" ],
		    "properties" : {
		      "type" : {
			"const" : "ScalaCodeDfTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      },
		      "code" : {
			"type" : "string",
			"description" : "Scala code for transformation. The scala code needs to be a function of type[[fnTransformType]] ."
		      },
		      "file" : {
			"type" : "string",
			"description" : "File where scala code for transformation is loaded from. The scala code in the file needs to be a function of type[[fnTransformType]] ."
		      },
		      "options" : {
			"type" : "object",
			"description" : "Options to pass to the transformation",
			"additionalProperties" : {
			  "type" : "string"
			}
		      },
		      "runtimeOptions" : {
			"type" : "object",
			"description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\r\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
			"additionalProperties" : {
			  "type" : "string"
			}
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Apply a column blacklist to a DataFrame.",
		    "additionalProperties" : false,
		    "required" : [ "columnBlacklist", "type" ],
		    "properties" : {
		      "type" : {
			"const" : "BlacklistTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      },
		      "columnBlacklist" : {
			"type" : "array",
			"description" : "List of columns to exclude from DataFrame",
			"items" : {
			  "type" : "string"
			}
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Configuration of a custom Spark-DataFrame transformation between one input and one output (1:1) as Java/Scala Class.\r\nDefine a transform function which receives a DataObjectId, a DataFrame and a map of options and has to return a\r\nDataFrame. The Java/Scala class has to implement interface[[CustomDfTransformer]] .",
		    "additionalProperties" : false,
		    "required" : [ "className", "type" ],
		    "properties" : {
		      "type" : {
			"const" : "ScalaClassDfTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      },
		      "className" : {
			"type" : "string",
			"description" : "class name implementing trait[[CustomDfTransformer]]"
		      },
		      "options" : {
			"type" : "object",
			"description" : "Options to pass to the transformation",
			"additionalProperties" : {
			  "type" : "string"
			}
		      },
		      "runtimeOptions" : {
			"type" : "object",
			"description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\r\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
			"additionalProperties" : {
			  "type" : "string"
			}
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Add additional columns to the DataFrame by extracting information from the context.",
		    "additionalProperties" : false,
		    "required" : [ "additionalColumns", "type" ],
		    "properties" : {
		      "type" : {
			"const" : "AdditionalColumnsTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      },
		      "additionalColumns" : {
			"type" : "object",
			"description" : "optional tuples of [column name, spark sql expression] to be added as additional columns to the dataframe.\r\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
			"additionalProperties" : {
			  "type" : "string"
			}
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Configuration of a custom Spark-DataFrame transformation between one input and one output (1:1) as Python/PySpark code.\r\nNote that this transformer needs a Python and PySpark environment installed.\r\nPySpark session is initialize and available under variables`sc`,`session`,`sqlContext`.\r\nOther variables available are\r\n-`inputDf`: Input DataFrame\r\n-`options`: Transformation options as Map[String,String]\r\n-`dataObjectId`: Id of input dataObject as String\r\nOutput DataFrame must be set with`setOutputDf(df)` .",
		    "additionalProperties" : false,
		    "required" : [ "type" ],
		    "properties" : {
		      "type" : {
			"const" : "PythonCodeDfTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      },
		      "code" : {
			"type" : "string",
			"description" : "Optional python code to user for python transformation. The python code can use variables inputDf, dataObjectId and options. The transformed DataFrame has to be set with setOutputDf."
		      },
		      "file" : {
			"type" : "string",
			"description" : "Optional file with python code to use for python transformation. The python code can use variables inputDf, dataObjectId and options. The transformed DataFrame has to be set with setOutputDf."
		      },
		      "options" : {
			"type" : "object",
			"description" : "Options to pass to the transformation",
			"additionalProperties" : {
			  "type" : "string"
			}
		      },
		      "runtimeOptions" : {
			"type" : "object",
			"description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\r\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
			"additionalProperties" : {
			  "type" : "string"
			}
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Configuration of a custom Spark-DataFrame transformation between one input and one output (1:1) as SQL code.\r\nThe input data is available as temporary view in SQL. As name for the temporary view the input DataObjectId is used\r\n(special characters are replaces by underscores). A special token \\'%{inputViewName}\\' will be replaced with the name of\r\nthe temporary view at runtime.",
		    "additionalProperties" : false,
		    "required" : [ "code", "type" ],
		    "properties" : {
		      "type" : {
			"const" : "SQLDfTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      },
		      "code" : {
			"type" : "string",
			"description" : "SQL code for transformation.\r\nUse tokens %{<key>} to replace with runtimeOptions in SQL code.\r\nExample: \\\"select * from test where run = %{runId}\\\"\r\nA special token %{inputViewName} can be used to insert the temporary view name."
		      },
		      "options" : {
			"type" : "object",
			"description" : "Options to pass to the transformation",
			"additionalProperties" : {
			  "type" : "string"
			}
		      },
		      "runtimeOptions" : {
			"type" : "object",
			"description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\r\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
			"additionalProperties" : {
			  "type" : "string"
			}
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Apply validation rules to a DataFrame and collect potential violation error messages in a new column.",
		    "additionalProperties" : false,
		    "required" : [ "rules", "type" ],
		    "properties" : {
		      "type" : {
			"const" : "DataValidationTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      },
		      "rules" : {
			"type" : "array",
			"description" : "list of validation rules to apply to the DataFrame",
			"items" : {
			  "oneOf" : [ {
			    "type" : "object",
			    "description" : "Definition for a row level data validation rule.",
			    "additionalProperties" : false,
			    "required" : [ "condition", "type" ],
			    "properties" : {
			      "type" : {
				"const" : "RowLevelValidationRule"
			      },
			      "condition" : {
				"type" : "string",
				"description" : "a Spark SQL expression defining the condition to be tested."
			      },
			      "errorMsg" : {
				"type" : "string",
				"description" : "Optional error msg to be create if the condition fails. Default is to use a text representation of the condition."
			      }
			    }
			  } ]
			}
		      },
		      "errorsColumn" : {
			"type" : "string",
			"description" : "Optional column name for the list of error messages. Default is \\\"errors\\\"."
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Apply a column whitelist to a DataFrame.",
		    "additionalProperties" : false,
		    "required" : [ "columnWhitelist", "type" ],
		    "properties" : {
		      "type" : {
			"const" : "WhitelistTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      },
		      "columnWhitelist" : {
			"type" : "array",
			"description" : "List of columns to keep from DataFrame",
			"items" : {
			  "type" : "string"
			}
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Apply a filter condition to a DataFrame.",
		    "additionalProperties" : false,
		    "required" : [ "filterClause", "type" ],
		    "properties" : {
		      "type" : {
			"const" : "FilterTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      },
		      "filterClause" : {
			"type" : "string",
			"description" : "Spark SQL expression to filter the DataFrame"
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Configuration of a custom Spark-DataFrame transformation between one input and one output (1:1) as Scala code which is compiled at runtime.\r\nThe code is loaded from a Notebook. It should define a transform function with a configurable name, which receives a DataObjectId, a DataFrame\r\nand a map of options and has to return a DataFrame, see also ([[fnTransformType]] ).\r\nNotebook-cells starting with \\\"//!IGNORE\\\" will be ignored.",
		    "additionalProperties" : false,
		    "required" : [ "url", "functionName", "type" ],
		    "properties" : {
		      "type" : {
			"const" : "ScalaNotebookDfTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      },
		      "url" : {
			"type" : "string",
			"description" : "Url to download notebook in IPYNB-format, which defines transformation."
		      },
		      "functionName" : {
			"type" : "string",
			"description" : "The notebook needs to contain a Scala-function with this name and type[[fnTransformType]] ."
		      },
		      "authMode" : {
			"oneOf" : [ {
			  "type" : "object",
			  "description" : "Connect by custom authorization header",
			  "additionalProperties" : false,
			  "required" : [ "secretVariable", "type" ],
			  "properties" : {
			    "type" : {
			      "const" : "AuthHeaderMode"
			    },
			    "headerName" : {
			      "type" : "string"
			    },
			    "secretVariable" : {
			      "type" : "string"
			    }
			  }
			}, {
			  "type" : "object",
			  "description" : "Validate by SSL Certificates : Only location an credentials. Additional attributes should be\r\nsupplied via options map",
			  "additionalProperties" : false,
			  "required" : [ "keystorePath", "keystorePassVariable", "truststorePath", "truststorePassVariable", "type" ],
			  "properties" : {
			    "type" : {
			      "const" : "SSLCertsAuthMode"
			    },
			    "keystorePath" : {
			      "type" : "string"
			    },
			    "keystoreType" : {
			      "type" : "string"
			    },
			    "keystorePassVariable" : {
			      "type" : "string"
			    },
			    "truststorePath" : {
			      "type" : "string"
			    },
			    "truststoreType" : {
			      "type" : "string"
			    },
			    "truststorePassVariable" : {
			      "type" : "string"
			    }
			  }
			}, {
			  "type" : "object",
			  "description" : "Connect by basic authentication",
			  "additionalProperties" : false,
			  "required" : [ "userVariable", "passwordVariable", "type" ],
			  "properties" : {
			    "type" : {
			      "const" : "BasicAuthMode"
			    },
			    "userVariable" : {
			      "type" : "string"
			    },
			    "passwordVariable" : {
			      "type" : "string"
			    }
			  }
			}, {
			  "type" : "object",
			  "description" : "Connect with custom HTTP authentication",
			  "additionalProperties" : false,
			  "required" : [ "className", "options", "type" ],
			  "properties" : {
			    "type" : {
			      "const" : "CustomHttpAuthMode"
			    },
			    "className" : {
			      "type" : "string",
			      "description" : "class name implementing trait[[CustomHttpAuthModeLogic]]"
			    },
			    "options" : {
			      "type" : "object",
			      "description" : "Options to pass to the custom auth mode logc in prepare function",
			      "additionalProperties" : {
				"type" : "string"
			      }
			    }
			  }
			}, {
			  "type" : "object",
			  "description" : "Validate by SASL_SSL Authentication : user / password and truststore",
			  "additionalProperties" : false,
			  "required" : [ "username", "passwordVariable", "sslMechanism", "truststorePath", "truststorePassVariable", "type" ],
			  "properties" : {
			    "type" : {
			      "const" : "SASLSCRAMAuthMode"
			    },
			    "username" : {
			      "type" : "string"
			    },
			    "passwordVariable" : {
			      "type" : "string"
			    },
			    "sslMechanism" : {
			      "type" : "string"
			    },
			    "truststorePath" : {
			      "type" : "string"
			    },
			    "truststoreType" : {
			      "type" : "string"
			    },
			    "truststorePassVariable" : {
			      "type" : "string"
			    }
			  }
			}, {
			  "type" : "object",
			  "description" : "Connect by token\r\nFor HTTP Connection this is used as Bearer token in Authorization header.",
			  "additionalProperties" : false,
			  "required" : [ "tokenVariable", "type" ],
			  "properties" : {
			    "type" : {
			      "const" : "TokenAuthMode"
			    },
			    "tokenVariable" : {
			      "type" : "string"
			    }
			  }
			}, {
			  "type" : "object",
			  "description" : "Connect by using Keycloak to manage token and token refresh giving clientId/secret as information.\r\nFor HTTP Connection this is used as Bearer token in Authorization header.",
			  "additionalProperties" : false,
			  "required" : [ "ssoServer", "ssoRealm", "ssoGrantType", "clientIdVariable", "clientSecretVariable", "type" ],
			  "properties" : {
			    "type" : {
			      "const" : "KeycloakClientSecretAuthMode"
			    },
			    "ssoServer" : {
			      "type" : "string"
			    },
			    "ssoRealm" : {
			      "type" : "string"
			    },
			    "ssoGrantType" : {
			      "type" : "string"
			    },
			    "clientIdVariable" : {
			      "type" : "string"
			    },
			    "clientSecretVariable" : {
			      "type" : "string"
			    }
			  }
			}, {
			  "type" : "object",
			  "description" : "Validate by user and private/public key\r\nPrivate key is read from .ssh",
			  "additionalProperties" : false,
			  "required" : [ "userVariable", "type" ],
			  "properties" : {
			    "type" : {
			      "const" : "PublicKeyAuthMode"
			    },
			    "userVariable" : {
			      "type" : "string"
			    }
			  }
			} ]
		      },
		      "options" : {
			"type" : "object",
			"description" : "Options to pass to the transformation",
			"additionalProperties" : {
			  "type" : "string"
			}
		      },
		      "runtimeOptions" : {
			"type" : "object",
			"description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\r\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
			"additionalProperties" : {
			  "type" : "string"
			}
		      }
		    }
		  } ]
		}
	      },
	      "columnBlacklist" : {
		"type" : "array",
		"description" : "Remove all columns on blacklist from dataframe",
		"items" : {
		  "type" : "string"
		}
	      },
	      "columnWhitelist" : {
		"type" : "array",
		"description" : "Keep only columns on whitelist in dataframe",
		"items" : {
		  "type" : "string"
		}
	      },
	      "additionalColumns" : {
		"type" : "object",
		"description" : "optional tuples of [column name, spark sql expression] to be added as additional columns to the dataframe.\r\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
		"additionalProperties" : {
		  "type" : "string"
		}
	      },
	      "filterClause" : {
		"type" : "string"
	      },
	      "standardizeDatatypes" : {
		"type" : "string"
	      },
	      "breakDataFrameLineage" : {
		"type" : "string"
	      },
	      "persist" : {
		"type" : "string"
	      },
	      "executionMode" : {
		"oneOf" : [ {
		  "type" : "object",
		  "description" : "Execution mode to incrementally process file-based DataObjects.\r\nIt takes all existing files in the input DataObject and removes (deletes) them after processing.\r\nInput partition values are applied when searching for files and also used as output partition values.",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "FileIncrementalMoveMode"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "An execution mode for incremental processing by remembering DataObjects state from last increment.",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "DataObjectStateIncrementalMode"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Spark streaming execution mode uses Spark Structured Streaming to incrementally execute data loads and keep track of processed data.\r\nThis mode needs a DataObject implementing CanCreateStreamingDataFrame and works only with SparkSubFeeds.\r\nThis mode can be executed synchronously in the DAG by using triggerType=Once, or asynchronously as Streaming Query with triggerType = ProcessingTime or Continuous.",
		  "additionalProperties" : false,
		  "required" : [ "checkpointLocation", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "SparkStreamingMode"
		    },
		    "checkpointLocation" : {
		      "type" : "string",
		      "description" : "location for checkpoints of streaming query to keep state"
		    },
		    "triggerType" : {
		      "type" : "string",
		      "description" : "define execution interval of Spark streaming query. Possible values are Once (default), ProcessingTime & Continuous. See[[Trigger]] for details.\r\n                      Note that this is only applied if SDL is executed in streaming mode. If SDL is executed in normal mode, TriggerType=Once is used always.\r\nIf triggerType=Once, the action is repeated with Trigger.Once in SDL streaming mode."
		    },
		    "triggerTime" : {
		      "type" : "string",
		      "description" : "Time as String in triggerType = ProcessingTime or Continuous. See[[Trigger]] for details."
		    },
		    "inputOptions" : {
		      "type" : "object",
		      "description" : "additional option to apply when reading streaming source. This overwrites options set by the DataObjects.",
		      "additionalProperties" : {
			"type" : "string"
		      }
		    },
		    "outputOptions" : {
		      "type" : "object",
		      "description" : "additional option to apply when writing to streaming sink. This overwrites options set by the DataObjects.",
		      "additionalProperties" : {
			"type" : "string"
		      }
		    },
		    "outputMode" : {
		      "type" : "string",
		      "enum" : [ "Append", "Complete", "Update" ]
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Compares max entry in \\\"compare column\\\" between mainOutput and mainInput and incrementally loads the delta.\r\nThis mode works only with SparkSubFeeds. The filter is not propagated to following actions.",
		  "additionalProperties" : false,
		  "required" : [ "compareCol", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "SparkIncrementalMode"
		    },
		    "compareCol" : {
		      "type" : "string",
		      "description" : "a comparable column name existing in mainInput and mainOutput used to identify the delta. Column content should be bigger for newer records."
		    },
		    "alternativeOutputId" : {
		      "type" : "string",
		      "description" : "optional alternative outputId of DataObject later in the DAG. This replaces the mainOutputId.\r\nIt can be used to ensure processing all partitions over multiple actions in case of errors."
		    },
		    "applyCondition" : {
		      "type" : "object",
		      "description" : "Definition of a Spark SQL condition with description.\r\nThis is used for example to define failConditions of[[PartitionDiffMode]] .",
		      "additionalProperties" : false,
		      "required" : [ "expression" ],
		      "properties" : {
			"expression" : {
			  "type" : "string",
			  "description" : "Condition formulated as Spark SQL. The attributes available are dependent on the context."
			},
			"description" : {
			  "type" : "string",
			  "description" : "A textual description of the condition to be shown in error messages."
			}
		      }
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "An execution mode which forces processing all data from it\\'s inputs.",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "ProcessAllMode"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "An execution mode which just validates that partition values are given.\r\nNote: For start nodes of the DAG partition values can be defined by command line, for subsequent nodes partition values are passed on from previous nodes.",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "FailIfNoPartitionValuesMode"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Partition difference execution mode lists partitions on mainInput & mainOutput DataObject and starts loading all missing partitions.\r\nPartition columns to be used for comparision need to be a common \\'init\\' of input and output partition columns.\r\nThis mode needs mainInput/Output DataObjects which CanHandlePartitions to list partitions.\r\nPartition values are passed to following actions for partition columns which they have in common.",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "PartitionDiffMode"
		    },
		    "partitionColNb" : {
		      "type" : "integer",
		      "description" : "optional number of partition columns to use as a common \\'init\\'."
		    },
		    "alternativeOutputId" : {
		      "type" : "string",
		      "description" : "optional alternative outputId of DataObject later in the DAG. This replaces the mainOutputId.\r\nIt can be used to ensure processing all partitions over multiple actions in case of errors."
		    },
		    "nbOfPartitionValuesPerRun" : {
		      "type" : "integer",
		      "description" : "optional restriction of the number of partition values per run."
		    },
		    "applyCondition" : {
		      "type" : "string",
		      "description" : "Condition to decide if execution mode should be applied or not. Define a spark sql expression working with attributes of[[DefaultExecutionModeExpressionData]] returning a boolean.\r\nDefault is to apply the execution mode if given partition values (partition values from command line or passed from previous action) are not empty."
		    },
		    "failCondition" : {
		      "type" : "string"
		    },
		    "failConditions" : {
		      "type" : "array",
		      "description" : "List of conditions to fail application of execution mode if true. Define as spark sql expressions working with attributes of[[PartitionDiffModeExpressionData]] returning a boolean.\r\nDefault is that the application of the PartitionDiffMode does not fail the action. If there is no data to process, the following actions are skipped.\r\nMultiple conditions are evaluated individually and every condition may fail the execution mode (or-logic)",
		      "items" : {
			"type" : "object",
			"description" : "Definition of a Spark SQL condition with description.\r\nThis is used for example to define failConditions of[[PartitionDiffMode]] .",
			"additionalProperties" : false,
			"required" : [ "expression" ],
			"properties" : {
			  "expression" : {
			    "type" : "string",
			    "description" : "Condition formulated as Spark SQL. The attributes available are dependent on the context."
			  },
			  "description" : {
			    "type" : "string",
			    "description" : "A textual description of the condition to be shown in error messages."
			  }
			}
		      }
		    },
		    "selectExpression" : {
		      "type" : "string",
		      "description" : "optional expression to define or refine the list of selected output partitions. Define a spark sql expression working with the attributes of[[PartitionDiffModeExpressionData]] returning a list<map<string,string>>.\r\nDefault is to return the originally selected output partitions found in attribute selectedOutputPartitionValues."
		    },
		    "applyPartitionValuesTransform" : {
		      "type" : "string",
		      "description" : "If true applies the partition values transform of custom transformations on input partition values before comparison with output partition values.\r\nIf enabled input and output partition columns can be different. Default is to disable the transformation of partition values."
		    },
		    "selectAdditionalInputExpression" : {
		      "type" : "string",
		      "description" : "optional expression to refine the list of selected input partitions. Note that primarily output partitions are selected by PartitionDiffMode.\r\nThe selected output partitions are then transformed back to the input partitions needed to create the selected output partitions. This is one-to-one except if applyPartitionValuesTransform=true.\r\nAnd sometimes there is a need for additional input data to create the output partitions, e.g. if you aggregate a window of 7 days for every day.\r\nYou can customize selected input partitions by defining a spark sql expression working with the attributes of[[PartitionDiffModeExpressionData]] returning a list<map<string,string>>.\r\nDefault is to return the originally selected input partitions found in attribute selectedInputPartitionValues."
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Execution mode to create custom partition execution mode logic.\r\nDefine a function which receives main input&output DataObject and returns partition values to process as Seq[Map[String,String]\\\\]",
		  "additionalProperties" : false,
		  "required" : [ "className", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "CustomPartitionMode"
		    },
		    "className" : {
		      "type" : "string",
		      "description" : "class name implementing trait[[CustomPartitionModeLogic]]"
		    },
		    "alternativeOutputId" : {
		      "type" : "string",
		      "description" : "optional alternative outputId of DataObject later in the DAG. This replaces the mainOutputId.\r\nIt can be used to ensure processing all partitions over multiple actions in case of errors."
		    },
		    "options" : {
		      "type" : "object",
		      "description" : "Options specified in the configuration for this execution mode",
		      "additionalProperties" : {
			"type" : "string"
		      }
		    }
		  }
		} ]
	      },
	      "executionCondition" : {
		"type" : "object",
		"description" : "Definition of a Spark SQL condition with description.\r\nThis is used for example to define failConditions of[[PartitionDiffMode]] .",
		"additionalProperties" : false,
		"required" : [ "expression" ],
		"properties" : {
		  "expression" : {
		    "type" : "string",
		    "description" : "Condition formulated as Spark SQL. The attributes available are dependent on the context."
		  },
		  "description" : {
		    "type" : "string",
		    "description" : "A textual description of the condition to be shown in error messages."
		  }
		}
	      },
	      "metricsFailCondition" : {
		"type" : "string",
		"description" : "optional spark sql expression evaluated as where-clause against dataframe of metrics. Available columns are dataObjectId, key, value.\r\nIf there are any rows passing the where clause, a MetricCheckFailed exception is thrown."
	      },
	      "saveModeOptions" : {
		"oneOf" : [ {
		  "type" : "object",
		  "description" : "This class can be used to override save mode without further special parameters.",
		  "additionalProperties" : false,
		  "required" : [ "saveMode", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "SaveModeGenericOptions"
		    },
		    "saveMode" : {
		      "type" : "string",
		      "enum" : [ "Merge ", "OverwriteOptimized ", "OverwritePreserveDirectories ", "Ignore ", "ErrorIfExists ", "Append ", "Overwrite " ]
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Options to control detailed behaviour of SaveMode.Merge.\r\nIn Spark expressions use table alias \\'existing\\' to reference columns of the existing table data, and table alias \\'new\\' to reference columns of new data set.",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "SaveModeMergeOptions"
		    },
		    "deleteCondition" : {
		      "type" : "string",
		      "description" : "A condition to control if matched records are deleted. If no condition is given, *no* records are delete."
		    },
		    "updateCondition" : {
		      "type" : "string",
		      "description" : "A condition to control if matched records are updated. If no condition is given all matched records are updated (default).\r\nNote that delete is applied before update. Records selected for deletion are automatically excluded from the updates."
		    },
		    "updateColumns" : {
		      "type" : "array",
		      "description" : "List of column names to update in update clause. If empty all columns (except primary keys) are updated (default)",
		      "items" : {
			"type" : "string"
		      }
		    },
		    "insertCondition" : {
		      "type" : "string",
		      "description" : "A condition to control if unmatched records are inserted. If no condition is given all unmatched records are inserted (default)."
		    },
		    "insertColumnsToIgnore" : {
		      "type" : "array",
		      "description" : "List of column names to ignore in insert clause. If empty all columns are inserted (default).",
		      "items" : {
			"type" : "string"
		      }
		    },
		    "additionalMergePredicate" : {
		      "type" : "string",
		      "description" : "To optimize performance for SDLSaveMode.Merge it might be interesting to limit the records read from the existing table data, e.g. merge operation might use only the last 7 days."
		    }
		  }
		} ]
	      },
	      "metadata" : {
		"oneOf" : [ {
		  "$ref" : "#/definitions/ActionMetadata"
		} ]
	      }
	    }
	  },
	  "HistorizeAction" : {
	    "type" : "object",
	    "description" : "[[Action]] to historize a subfeed.\r\nHistorization creates a technical history of data by creating valid-from/to columns.\r\nIt needs a transactional table as output with defined primary keys.",
	    "additionalProperties" : false,
	    "required" : [ "id", "inputId", "outputId" ],
	    "properties" : {
	      "id" : {
		"type" : "string"
	      },
	      "inputId" : {
		"type" : "string",
		"description" : "inputs DataObject"
	      },
	      "outputId" : {
		"type" : "string",
		"description" : "output DataObject"
	      },
	      "transformer" : {
		"type" : "object",
		"description" : "Configuration of a custom Spark-DataFrame transformation between one input and one output (1:1)\r\nDefine a transform function which receives a DataObjectIds, a DataFrames and a map of options and has to return a\r\nDataFrame, see also[[CustomDfTransformer]].\r\nNote about Python transformation: Environment with Python and PySpark needed.\r\nPySpark session is initialize and available under variables`sc`,`session`,`sqlContext`.\r\nOther variables available are\r\n-`inputDf`: Input DataFrame\r\n-`options`: Transformation options as Map[String,String]\r\n-`dataObjectId`: Id of input dataObject as String\r\nOutput DataFrame must be set with`setOutputDf(df)` .",
		"additionalProperties" : false,
		"properties" : {
		  "className" : {
		    "type" : "string",
		    "description" : "Optional class name implementing trait[[CustomDfTransformer]]"
		  },
		  "scalaFile" : {
		    "type" : "string",
		    "description" : "Optional file where scala code for transformation is loaded from. The scala code in the file needs to be a function of type[[fnTransformType]] ."
		  },
		  "scalaCode" : {
		    "type" : "string",
		    "description" : "Optional scala code for transformation. The scala code needs to be a function of type[[fnTransformType]] ."
		  },
		  "sqlCode" : {
		    "type" : "string",
		    "description" : "Optional SQL code for transformation.\r\nUse tokens %{<key>} to replace with runtimeOptions in SQL code.\r\nExample: \\\"select * from test where run = %{runId}\\\""
		  },
		  "pythonFile" : {
		    "type" : "string",
		    "description" : "Optional pythonFile to use for python transformation. The python code can use variables inputDf, dataObjectId and options. The transformed DataFrame has to be set with setOutputDf."
		  },
		  "pythonCode" : {
		    "type" : "string",
		    "description" : "Optional pythonCode to use for python transformation. The python code can use variables inputDf, dataObjectId and options. The transformed DataFrame has to be set with setOutputDf."
		  },
		  "options" : {
		    "type" : "object",
		    "description" : "Options to pass to the transformation",
		    "additionalProperties" : {
		      "type" : "string"
		    }
		  },
		  "runtimeOptions" : {
		    "type" : "object",
		    "description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\r\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
		    "additionalProperties" : {
		      "type" : "string"
		    }
		  }
		}
	      },
	      "transformers" : {
		"type" : "array",
		"description" : "optional list of transformations to apply before historization. See[[sparktransformer]] for a list of included Transformers.\r\nThe transformations are applied according to the lists ordering.",
		"items" : {
		  "oneOf" : [ {
		    "type" : "object",
		    "description" : "Interface to implement Spark-DataFrame transformers working with one input and one output (1:1).\r\nThis trait extends DfSparkTransformer to pass a map of options as parameter to the transform function. This is mainly\r\nused by custom transformers.",
		    "additionalProperties" : false,
		    "required" : [ "type" ],
		    "properties" : {
		      "type" : {
			"const" : "OptionsDfTransformer"
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Repartition DataFrame\r\nFor detailled description about repartitioning DataFrames see also[[SparkRepartitionDef]]",
		    "additionalProperties" : false,
		    "required" : [ "numberOfTasksPerPartition", "type" ],
		    "properties" : {
		      "type" : {
			"const" : "RepartitionTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      },
		      "numberOfTasksPerPartition" : {
			"type" : "integer",
			"description" : "Number of Spark tasks to create per partition value by repartitioning the DataFrame."
		      },
		      "keyCols" : {
			"type" : "array",
			"description" : "Optional key columns to distribute records over Spark tasks inside a partition value.",
			"items" : {
			  "type" : "string"
			}
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Standardize datatypes of a DataFrame.\r\nCurrent implementation converts all decimal datatypes to a corresponding integral or float datatype",
		    "additionalProperties" : false,
		    "required" : [ "type" ],
		    "properties" : {
		      "type" : {
			"const" : "StandardizeDatatypesTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Configuration of a custom Spark-DataFrame transformation between one input and one output (1:1) as Scala code which is compiled at runtime.\r\nDefine a transform function which receives a DataObjectId, a DataFrame and a map of options and has to return a\r\nDataFrame. The scala code has to implement a function of type[[fnTransformType]] .",
		    "additionalProperties" : false,
		    "required" : [ "type" ],
		    "properties" : {
		      "type" : {
			"const" : "ScalaCodeDfTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      },
		      "code" : {
			"type" : "string",
			"description" : "Scala code for transformation. The scala code needs to be a function of type[[fnTransformType]] ."
		      },
		      "file" : {
			"type" : "string",
			"description" : "File where scala code for transformation is loaded from. The scala code in the file needs to be a function of type[[fnTransformType]] ."
		      },
		      "options" : {
			"type" : "object",
			"description" : "Options to pass to the transformation",
			"additionalProperties" : {
			  "type" : "string"
			}
		      },
		      "runtimeOptions" : {
			"type" : "object",
			"description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\r\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
			"additionalProperties" : {
			  "type" : "string"
			}
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Apply a column blacklist to a DataFrame.",
		    "additionalProperties" : false,
		    "required" : [ "columnBlacklist", "type" ],
		    "properties" : {
		      "type" : {
			"const" : "BlacklistTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      },
		      "columnBlacklist" : {
			"type" : "array",
			"description" : "List of columns to exclude from DataFrame",
			"items" : {
			  "type" : "string"
			}
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Configuration of a custom Spark-DataFrame transformation between one input and one output (1:1) as Java/Scala Class.\r\nDefine a transform function which receives a DataObjectId, a DataFrame and a map of options and has to return a\r\nDataFrame. The Java/Scala class has to implement interface[[CustomDfTransformer]] .",
		    "additionalProperties" : false,
		    "required" : [ "className", "type" ],
		    "properties" : {
		      "type" : {
			"const" : "ScalaClassDfTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      },
		      "className" : {
			"type" : "string",
			"description" : "class name implementing trait[[CustomDfTransformer]]"
		      },
		      "options" : {
			"type" : "object",
			"description" : "Options to pass to the transformation",
			"additionalProperties" : {
			  "type" : "string"
			}
		      },
		      "runtimeOptions" : {
			"type" : "object",
			"description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\r\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
			"additionalProperties" : {
			  "type" : "string"
			}
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Add additional columns to the DataFrame by extracting information from the context.",
		    "additionalProperties" : false,
		    "required" : [ "additionalColumns", "type" ],
		    "properties" : {
		      "type" : {
			"const" : "AdditionalColumnsTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      },
		      "additionalColumns" : {
			"type" : "object",
			"description" : "optional tuples of [column name, spark sql expression] to be added as additional columns to the dataframe.\r\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
			"additionalProperties" : {
			  "type" : "string"
			}
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Configuration of a custom Spark-DataFrame transformation between one input and one output (1:1) as Python/PySpark code.\r\nNote that this transformer needs a Python and PySpark environment installed.\r\nPySpark session is initialize and available under variables`sc`,`session`,`sqlContext`.\r\nOther variables available are\r\n-`inputDf`: Input DataFrame\r\n-`options`: Transformation options as Map[String,String]\r\n-`dataObjectId`: Id of input dataObject as String\r\nOutput DataFrame must be set with`setOutputDf(df)` .",
		    "additionalProperties" : false,
		    "required" : [ "type" ],
		    "properties" : {
		      "type" : {
			"const" : "PythonCodeDfTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      },
		      "code" : {
			"type" : "string",
			"description" : "Optional python code to user for python transformation. The python code can use variables inputDf, dataObjectId and options. The transformed DataFrame has to be set with setOutputDf."
		      },
		      "file" : {
			"type" : "string",
			"description" : "Optional file with python code to use for python transformation. The python code can use variables inputDf, dataObjectId and options. The transformed DataFrame has to be set with setOutputDf."
		      },
		      "options" : {
			"type" : "object",
			"description" : "Options to pass to the transformation",
			"additionalProperties" : {
			  "type" : "string"
			}
		      },
		      "runtimeOptions" : {
			"type" : "object",
			"description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\r\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
			"additionalProperties" : {
			  "type" : "string"
			}
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Configuration of a custom Spark-DataFrame transformation between one input and one output (1:1) as SQL code.\r\nThe input data is available as temporary view in SQL. As name for the temporary view the input DataObjectId is used\r\n(special characters are replaces by underscores). A special token \\'%{inputViewName}\\' will be replaced with the name of\r\nthe temporary view at runtime.",
		    "additionalProperties" : false,
		    "required" : [ "code", "type" ],
		    "properties" : {
		      "type" : {
			"const" : "SQLDfTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      },
		      "code" : {
			"type" : "string",
			"description" : "SQL code for transformation.\r\nUse tokens %{<key>} to replace with runtimeOptions in SQL code.\r\nExample: \\\"select * from test where run = %{runId}\\\"\r\nA special token %{inputViewName} can be used to insert the temporary view name."
		      },
		      "options" : {
			"type" : "object",
			"description" : "Options to pass to the transformation",
			"additionalProperties" : {
			  "type" : "string"
			}
		      },
		      "runtimeOptions" : {
			"type" : "object",
			"description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\r\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
			"additionalProperties" : {
			  "type" : "string"
			}
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Apply validation rules to a DataFrame and collect potential violation error messages in a new column.",
		    "additionalProperties" : false,
		    "required" : [ "rules", "type" ],
		    "properties" : {
		      "type" : {
			"const" : "DataValidationTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      },
		      "rules" : {
			"type" : "array",
			"description" : "list of validation rules to apply to the DataFrame",
			"items" : {
			  "oneOf" : [ {
			    "type" : "object",
			    "description" : "Definition for a row level data validation rule.",
			    "additionalProperties" : false,
			    "required" : [ "condition", "type" ],
			    "properties" : {
			      "type" : {
				"const" : "RowLevelValidationRule"
			      },
			      "condition" : {
				"type" : "string",
				"description" : "a Spark SQL expression defining the condition to be tested."
			      },
			      "errorMsg" : {
				"type" : "string",
				"description" : "Optional error msg to be create if the condition fails. Default is to use a text representation of the condition."
			      }
			    }
			  } ]
			}
		      },
		      "errorsColumn" : {
			"type" : "string",
			"description" : "Optional column name for the list of error messages. Default is \\\"errors\\\"."
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Apply a column whitelist to a DataFrame.",
		    "additionalProperties" : false,
		    "required" : [ "columnWhitelist", "type" ],
		    "properties" : {
		      "type" : {
			"const" : "WhitelistTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      },
		      "columnWhitelist" : {
			"type" : "array",
			"description" : "List of columns to keep from DataFrame",
			"items" : {
			  "type" : "string"
			}
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Apply a filter condition to a DataFrame.",
		    "additionalProperties" : false,
		    "required" : [ "filterClause", "type" ],
		    "properties" : {
		      "type" : {
			"const" : "FilterTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      },
		      "filterClause" : {
			"type" : "string",
			"description" : "Spark SQL expression to filter the DataFrame"
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Configuration of a custom Spark-DataFrame transformation between one input and one output (1:1) as Scala code which is compiled at runtime.\r\nThe code is loaded from a Notebook. It should define a transform function with a configurable name, which receives a DataObjectId, a DataFrame\r\nand a map of options and has to return a DataFrame, see also ([[fnTransformType]] ).\r\nNotebook-cells starting with \\\"//!IGNORE\\\" will be ignored.",
		    "additionalProperties" : false,
		    "required" : [ "url", "functionName", "type" ],
		    "properties" : {
		      "type" : {
			"const" : "ScalaNotebookDfTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      },
		      "url" : {
			"type" : "string",
			"description" : "Url to download notebook in IPYNB-format, which defines transformation."
		      },
		      "functionName" : {
			"type" : "string",
			"description" : "The notebook needs to contain a Scala-function with this name and type[[fnTransformType]] ."
		      },
		      "authMode" : {
			"oneOf" : [ {
			  "type" : "object",
			  "description" : "Connect by custom authorization header",
			  "additionalProperties" : false,
			  "required" : [ "secretVariable", "type" ],
			  "properties" : {
			    "type" : {
			      "const" : "AuthHeaderMode"
			    },
			    "headerName" : {
			      "type" : "string"
			    },
			    "secretVariable" : {
			      "type" : "string"
			    }
			  }
			}, {
			  "type" : "object",
			  "description" : "Validate by SSL Certificates : Only location an credentials. Additional attributes should be\r\nsupplied via options map",
			  "additionalProperties" : false,
			  "required" : [ "keystorePath", "keystorePassVariable", "truststorePath", "truststorePassVariable", "type" ],
			  "properties" : {
			    "type" : {
			      "const" : "SSLCertsAuthMode"
			    },
			    "keystorePath" : {
			      "type" : "string"
			    },
			    "keystoreType" : {
			      "type" : "string"
			    },
			    "keystorePassVariable" : {
			      "type" : "string"
			    },
			    "truststorePath" : {
			      "type" : "string"
			    },
			    "truststoreType" : {
			      "type" : "string"
			    },
			    "truststorePassVariable" : {
			      "type" : "string"
			    }
			  }
			}, {
			  "type" : "object",
			  "description" : "Connect by basic authentication",
			  "additionalProperties" : false,
			  "required" : [ "userVariable", "passwordVariable", "type" ],
			  "properties" : {
			    "type" : {
			      "const" : "BasicAuthMode"
			    },
			    "userVariable" : {
			      "type" : "string"
			    },
			    "passwordVariable" : {
			      "type" : "string"
			    }
			  }
			}, {
			  "type" : "object",
			  "description" : "Connect with custom HTTP authentication",
			  "additionalProperties" : false,
			  "required" : [ "className", "options", "type" ],
			  "properties" : {
			    "type" : {
			      "const" : "CustomHttpAuthMode"
			    },
			    "className" : {
			      "type" : "string",
			      "description" : "class name implementing trait[[CustomHttpAuthModeLogic]]"
			    },
			    "options" : {
			      "type" : "object",
			      "description" : "Options to pass to the custom auth mode logc in prepare function",
			      "additionalProperties" : {
				"type" : "string"
			      }
			    }
			  }
			}, {
			  "type" : "object",
			  "description" : "Validate by SASL_SSL Authentication : user / password and truststore",
			  "additionalProperties" : false,
			  "required" : [ "username", "passwordVariable", "sslMechanism", "truststorePath", "truststorePassVariable", "type" ],
			  "properties" : {
			    "type" : {
			      "const" : "SASLSCRAMAuthMode"
			    },
			    "username" : {
			      "type" : "string"
			    },
			    "passwordVariable" : {
			      "type" : "string"
			    },
			    "sslMechanism" : {
			      "type" : "string"
			    },
			    "truststorePath" : {
			      "type" : "string"
			    },
			    "truststoreType" : {
			      "type" : "string"
			    },
			    "truststorePassVariable" : {
			      "type" : "string"
			    }
			  }
			}, {
			  "type" : "object",
			  "description" : "Connect by token\r\nFor HTTP Connection this is used as Bearer token in Authorization header.",
			  "additionalProperties" : false,
			  "required" : [ "tokenVariable", "type" ],
			  "properties" : {
			    "type" : {
			      "const" : "TokenAuthMode"
			    },
			    "tokenVariable" : {
			      "type" : "string"
			    }
			  }
			}, {
			  "type" : "object",
			  "description" : "Connect by using Keycloak to manage token and token refresh giving clientId/secret as information.\r\nFor HTTP Connection this is used as Bearer token in Authorization header.",
			  "additionalProperties" : false,
			  "required" : [ "ssoServer", "ssoRealm", "ssoGrantType", "clientIdVariable", "clientSecretVariable", "type" ],
			  "properties" : {
			    "type" : {
			      "const" : "KeycloakClientSecretAuthMode"
			    },
			    "ssoServer" : {
			      "type" : "string"
			    },
			    "ssoRealm" : {
			      "type" : "string"
			    },
			    "ssoGrantType" : {
			      "type" : "string"
			    },
			    "clientIdVariable" : {
			      "type" : "string"
			    },
			    "clientSecretVariable" : {
			      "type" : "string"
			    }
			  }
			}, {
			  "type" : "object",
			  "description" : "Validate by user and private/public key\r\nPrivate key is read from .ssh",
			  "additionalProperties" : false,
			  "required" : [ "userVariable", "type" ],
			  "properties" : {
			    "type" : {
			      "const" : "PublicKeyAuthMode"
			    },
			    "userVariable" : {
			      "type" : "string"
			    }
			  }
			} ]
		      },
		      "options" : {
			"type" : "object",
			"description" : "Options to pass to the transformation",
			"additionalProperties" : {
			  "type" : "string"
			}
		      },
		      "runtimeOptions" : {
			"type" : "object",
			"description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\r\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
			"additionalProperties" : {
			  "type" : "string"
			}
		      }
		    }
		  } ]
		}
	      },
	      "columnBlacklist" : {
		"type" : "array",
		"description" : "Remove all columns on blacklist from dataframe",
		"items" : {
		  "type" : "string"
		}
	      },
	      "columnWhitelist" : {
		"type" : "array",
		"description" : "Keep only columns on whitelist in dataframe",
		"items" : {
		  "type" : "string"
		}
	      },
	      "additionalColumns" : {
		"type" : "object",
		"description" : "optional tuples of [column name, spark sql expression] to be added as additional columns to the dataframe.\r\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
		"additionalProperties" : {
		  "type" : "string"
		}
	      },
	      "standardizeDatatypes" : {
		"type" : "string"
	      },
	      "filterClause" : {
		"type" : "string",
		"description" : "Filter of data to be processed by historization. It can be used to exclude historical data not needed to create new history, for performance reasons.\r\nNote that filterClause is only applied if mergeModeEnable=false. Use mergeModeAdditionalJoinPredicate if mergeModeEnable=true to achieve a similar performance tuning."
	      },
	      "historizeBlacklist" : {
		"type" : "array",
		"description" : "optional list of columns to ignore when comparing two records in historization. Can not be used together with[[historizeWhitelist]] .",
		"items" : {
		  "type" : "string"
		}
	      },
	      "historizeWhitelist" : {
		"type" : "array",
		"description" : "optional final list of columns to use when comparing two records in historization. Can not be used together with[[historizeBlacklist]] .",
		"items" : {
		  "type" : "string"
		}
	      },
	      "ignoreOldDeletedColumns" : {
		"type" : "string",
		"description" : "if true, remove no longer existing columns in Schema Evolution"
	      },
	      "ignoreOldDeletedNestedColumns" : {
		"type" : "string",
		"description" : "if true, remove no longer existing columns from nested data types in Schema Evolution.\r\nKeeping deleted columns in complex data types has performance impact as all new data\r\nin the future has to be converted by a complex function."
	      },
	      "mergeModeEnable" : {
		"type" : "string",
		"description" : "Set to true to use saveMode.Merge for much better performance. Output DataObject must implement[[CanMergeDataFrame]] if enabled (default = false)."
	      },
	      "mergeModeAdditionalJoinPredicate" : {
		"type" : "string",
		"description" : "To optimize performance it might be interesting to limit the records read from the existing table data, e.g. it might be sufficient to use only the last 7 days.\r\nSpecify a condition to select existing data to be used in transformation as Spark SQL expression.\r\nUse table alias \\'existing\\' to reference columns of the existing table data."
	      },
	      "breakDataFrameLineage" : {
		"type" : "string"
	      },
	      "persist" : {
		"type" : "string"
	      },
	      "executionMode" : {
		"oneOf" : [ {
		  "type" : "object",
		  "description" : "Execution mode to incrementally process file-based DataObjects.\r\nIt takes all existing files in the input DataObject and removes (deletes) them after processing.\r\nInput partition values are applied when searching for files and also used as output partition values.",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "FileIncrementalMoveMode"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "An execution mode for incremental processing by remembering DataObjects state from last increment.",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "DataObjectStateIncrementalMode"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Spark streaming execution mode uses Spark Structured Streaming to incrementally execute data loads and keep track of processed data.\r\nThis mode needs a DataObject implementing CanCreateStreamingDataFrame and works only with SparkSubFeeds.\r\nThis mode can be executed synchronously in the DAG by using triggerType=Once, or asynchronously as Streaming Query with triggerType = ProcessingTime or Continuous.",
		  "additionalProperties" : false,
		  "required" : [ "checkpointLocation", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "SparkStreamingMode"
		    },
		    "checkpointLocation" : {
		      "type" : "string",
		      "description" : "location for checkpoints of streaming query to keep state"
		    },
		    "triggerType" : {
		      "type" : "string",
		      "description" : "define execution interval of Spark streaming query. Possible values are Once (default), ProcessingTime & Continuous. See[[Trigger]] for details.\r\n                      Note that this is only applied if SDL is executed in streaming mode. If SDL is executed in normal mode, TriggerType=Once is used always.\r\nIf triggerType=Once, the action is repeated with Trigger.Once in SDL streaming mode."
		    },
		    "triggerTime" : {
		      "type" : "string",
		      "description" : "Time as String in triggerType = ProcessingTime or Continuous. See[[Trigger]] for details."
		    },
		    "inputOptions" : {
		      "type" : "object",
		      "description" : "additional option to apply when reading streaming source. This overwrites options set by the DataObjects.",
		      "additionalProperties" : {
			"type" : "string"
		      }
		    },
		    "outputOptions" : {
		      "type" : "object",
		      "description" : "additional option to apply when writing to streaming sink. This overwrites options set by the DataObjects.",
		      "additionalProperties" : {
			"type" : "string"
		      }
		    },
		    "outputMode" : {
		      "type" : "string",
		      "enum" : [ "Append", "Complete", "Update" ]
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Compares max entry in \\\"compare column\\\" between mainOutput and mainInput and incrementally loads the delta.\r\nThis mode works only with SparkSubFeeds. The filter is not propagated to following actions.",
		  "additionalProperties" : false,
		  "required" : [ "compareCol", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "SparkIncrementalMode"
		    },
		    "compareCol" : {
		      "type" : "string",
		      "description" : "a comparable column name existing in mainInput and mainOutput used to identify the delta. Column content should be bigger for newer records."
		    },
		    "alternativeOutputId" : {
		      "type" : "string",
		      "description" : "optional alternative outputId of DataObject later in the DAG. This replaces the mainOutputId.\r\nIt can be used to ensure processing all partitions over multiple actions in case of errors."
		    },
		    "applyCondition" : {
		      "type" : "object",
		      "description" : "Definition of a Spark SQL condition with description.\r\nThis is used for example to define failConditions of[[PartitionDiffMode]] .",
		      "additionalProperties" : false,
		      "required" : [ "expression" ],
		      "properties" : {
			"expression" : {
			  "type" : "string",
			  "description" : "Condition formulated as Spark SQL. The attributes available are dependent on the context."
			},
			"description" : {
			  "type" : "string",
			  "description" : "A textual description of the condition to be shown in error messages."
			}
		      }
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "An execution mode which forces processing all data from it\\'s inputs.",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "ProcessAllMode"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "An execution mode which just validates that partition values are given.\r\nNote: For start nodes of the DAG partition values can be defined by command line, for subsequent nodes partition values are passed on from previous nodes.",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "FailIfNoPartitionValuesMode"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Partition difference execution mode lists partitions on mainInput & mainOutput DataObject and starts loading all missing partitions.\r\nPartition columns to be used for comparision need to be a common \\'init\\' of input and output partition columns.\r\nThis mode needs mainInput/Output DataObjects which CanHandlePartitions to list partitions.\r\nPartition values are passed to following actions for partition columns which they have in common.",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "PartitionDiffMode"
		    },
		    "partitionColNb" : {
		      "type" : "integer",
		      "description" : "optional number of partition columns to use as a common \\'init\\'."
		    },
		    "alternativeOutputId" : {
		      "type" : "string",
		      "description" : "optional alternative outputId of DataObject later in the DAG. This replaces the mainOutputId.\r\nIt can be used to ensure processing all partitions over multiple actions in case of errors."
		    },
		    "nbOfPartitionValuesPerRun" : {
		      "type" : "integer",
		      "description" : "optional restriction of the number of partition values per run."
		    },
		    "applyCondition" : {
		      "type" : "string",
		      "description" : "Condition to decide if execution mode should be applied or not. Define a spark sql expression working with attributes of[[DefaultExecutionModeExpressionData]] returning a boolean.\r\nDefault is to apply the execution mode if given partition values (partition values from command line or passed from previous action) are not empty."
		    },
		    "failCondition" : {
		      "type" : "string"
		    },
		    "failConditions" : {
		      "type" : "array",
		      "description" : "List of conditions to fail application of execution mode if true. Define as spark sql expressions working with attributes of[[PartitionDiffModeExpressionData]] returning a boolean.\r\nDefault is that the application of the PartitionDiffMode does not fail the action. If there is no data to process, the following actions are skipped.\r\nMultiple conditions are evaluated individually and every condition may fail the execution mode (or-logic)",
		      "items" : {
			"type" : "object",
			"description" : "Definition of a Spark SQL condition with description.\r\nThis is used for example to define failConditions of[[PartitionDiffMode]] .",
			"additionalProperties" : false,
			"required" : [ "expression" ],
			"properties" : {
			  "expression" : {
			    "type" : "string",
			    "description" : "Condition formulated as Spark SQL. The attributes available are dependent on the context."
			  },
			  "description" : {
			    "type" : "string",
			    "description" : "A textual description of the condition to be shown in error messages."
			  }
			}
		      }
		    },
		    "selectExpression" : {
		      "type" : "string",
		      "description" : "optional expression to define or refine the list of selected output partitions. Define a spark sql expression working with the attributes of[[PartitionDiffModeExpressionData]] returning a list<map<string,string>>.\r\nDefault is to return the originally selected output partitions found in attribute selectedOutputPartitionValues."
		    },
		    "applyPartitionValuesTransform" : {
		      "type" : "string",
		      "description" : "If true applies the partition values transform of custom transformations on input partition values before comparison with output partition values.\r\nIf enabled input and output partition columns can be different. Default is to disable the transformation of partition values."
		    },
		    "selectAdditionalInputExpression" : {
		      "type" : "string",
		      "description" : "optional expression to refine the list of selected input partitions. Note that primarily output partitions are selected by PartitionDiffMode.\r\nThe selected output partitions are then transformed back to the input partitions needed to create the selected output partitions. This is one-to-one except if applyPartitionValuesTransform=true.\r\nAnd sometimes there is a need for additional input data to create the output partitions, e.g. if you aggregate a window of 7 days for every day.\r\nYou can customize selected input partitions by defining a spark sql expression working with the attributes of[[PartitionDiffModeExpressionData]] returning a list<map<string,string>>.\r\nDefault is to return the originally selected input partitions found in attribute selectedInputPartitionValues."
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Execution mode to create custom partition execution mode logic.\r\nDefine a function which receives main input&output DataObject and returns partition values to process as Seq[Map[String,String]\\\\]",
		  "additionalProperties" : false,
		  "required" : [ "className", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "CustomPartitionMode"
		    },
		    "className" : {
		      "type" : "string",
		      "description" : "class name implementing trait[[CustomPartitionModeLogic]]"
		    },
		    "alternativeOutputId" : {
		      "type" : "string",
		      "description" : "optional alternative outputId of DataObject later in the DAG. This replaces the mainOutputId.\r\nIt can be used to ensure processing all partitions over multiple actions in case of errors."
		    },
		    "options" : {
		      "type" : "object",
		      "description" : "Options specified in the configuration for this execution mode",
		      "additionalProperties" : {
			"type" : "string"
		      }
		    }
		  }
		} ]
	      },
	      "executionCondition" : {
		"type" : "object",
		"description" : "Definition of a Spark SQL condition with description.\r\nThis is used for example to define failConditions of[[PartitionDiffMode]] .",
		"additionalProperties" : false,
		"required" : [ "expression" ],
		"properties" : {
		  "expression" : {
		    "type" : "string",
		    "description" : "Condition formulated as Spark SQL. The attributes available are dependent on the context."
		  },
		  "description" : {
		    "type" : "string",
		    "description" : "A textual description of the condition to be shown in error messages."
		  }
		}
	      },
	      "metricsFailCondition" : {
		"type" : "string",
		"description" : "optional spark sql expression evaluated as where-clause against dataframe of metrics. Available columns are dataObjectId, key, value.\r\nIf there are any rows passing the where clause, a MetricCheckFailed exception is thrown."
	      },
	      "metadata" : {
		"oneOf" : [ {
		  "$ref" : "#/definitions/ActionMetadata"
		} ]
	      }
	    }
	  },
	  "Table" : {
	    "type" : "object",
	    "description" : "Table attributes",
	    "additionalProperties" : false,
	    "required" : [ "name" ],
	    "properties" : {
	      "db" : {
		"type" : "string",
		"description" : "optional override of db defined by connection"
	      },
	      "name" : {
		"type" : "string",
		"description" : "table name"
	      },
	      "query" : {
		"type" : "string",
		"description" : "optional select query"
	      },
	      "primaryKey" : {
		"type" : "array",
		"description" : "optional sequence of primary key columns",
		"items" : {
		  "type" : "string"
		}
	      },
	      "foreignKeys" : {
		"type" : "array",
		"description" : "optional sequence of foreign key definitions.\r\nThis is used as metadata for a data catalog.",
		"items" : {
		  "type" : "object",
		  "description" : "Foreign key definition",
		  "additionalProperties" : false,
		  "required" : [ "table", "columns" ],
		  "properties" : {
		    "db" : {
		      "type" : "string",
		      "description" : "target database, if not defined it is assumed to be the same as the table owning the foreign key"
		    },
		    "table" : {
		      "type" : "string",
		      "description" : "referenced target table name"
		    },
		    "columns" : {
		      "type" : "object",
		      "description" : "mapping of source column(s) to referenced target table column(s)",
		      "additionalProperties" : {
			"type" : "string"
		      }
		    },
		    "name" : {
		      "type" : "string",
		      "description" : "optional name for foreign key, e.g to depict it\\'s role"
		    }
		  }
		}
	      },
	      "options" : {
		"type" : "object",
		"description" : "",
		"additionalProperties" : {
		  "type" : "string"
		}
	      }
	    }
	  },
	  "DeltaLakeTableDataObject" : {
	    "type" : "object",
	    "additionalProperties" : false,
	    "required" : [ "id", "table" ],
	    "properties" : {
	      "id" : {
		"type" : "string"
	      },
	      "path" : {
		"type" : "string"
	      },
	      "partitions" : {
		"type" : "array",
		"items" : {
		  "type" : "string"
		}
	      },
	      "options" : {
		"type" : "object",
		"additionalProperties" : {
		  "type" : "string"
		}
	      },
	      "schemaMin" : {
		"type" : "string"
	      },
	      "table" : {
		"oneOf" : [ {
		  "$ref" : "#/definitions/Table"
		} ]
	      },
	      "saveMode" : {
		"type" : "string",
		"enum" : [ "Merge ", "OverwriteOptimized ", "OverwritePreserveDirectories ", "Ignore ", "ErrorIfExists ", "Append ", "Overwrite " ]
	      },
	      "allowSchemaEvolution" : {
		"type" : "string"
	      },
	      "retentionPeriod" : {
		"type" : "integer"
	      },
	      "acl" : {
		"type" : "object",
		"description" : "Describes a complete ACL Specification (basic owner/group/other permissions AND extended ACLS)\r\nto be applied to a Data Object on writing",
		"additionalProperties" : false,
		"required" : [ "permission", "acls" ],
		"properties" : {
		  "permission" : {
		    "type" : "string",
		    "description" : ": File system permission string in symbolic notation form (e.g. rwxr-xr-x)"
		  },
		  "acls" : {
		    "type" : "array",
		    "description" : ": a sequence of[[AclElement]] s",
		    "items" : {
		      "type" : "object",
		      "description" : "Describes a single extended ACL to be applied to a Data Object\r\nin addition to the basic file system permissions",
		      "additionalProperties" : false,
		      "required" : [ "aclType", "name", "permission" ],
		      "properties" : {
			"aclType" : {
			  "type" : "string",
			  "description" : ": type of ACL to be added \\\"group\\\", \\\"user\\\""
			},
			"name" : {
			  "type" : "string",
			  "description" : ": the name of the user/group for which an ACL definition is being added"
			},
			"permission" : {
			  "type" : "string",
			  "description" : ": the permission (rwx syntax) to be granted"
			}
		      }
		    }
		  }
		}
	      },
	      "connectionId" : {
		"type" : "string"
	      },
	      "expectedPartitionsCondition" : {
		"type" : "string"
	      },
	      "housekeepingMode" : {
		"oneOf" : [ {
		  "type" : "object",
		  "description" : "Archive and compact old partitions:\r\nArchive partition reduces the number of partitions in the past by moving older partitions into special \\\"archive partitions\\\".\r\nCompact partition reduces the number of files in a partition by rewriting them with Spark.\r\nExample: archive and compact a table with partition layout run_id=<integer>\r\n- archive partitions after 1000 partitions into \\\"archive partition\\\" equal to floor(run_id/1000)\r\n- compact \\\"archive partition\\\" when full\r\n{{{\r\nhousekeepingMode = {\r\ntype = PartitionArchiveCompactionMode\r\narchivePartitionExpression = \\\"if( elements[\\'run_id\\'] < runId - 1000, map(\\'run_id\\', elements[\\'run_id\\'] div 1000), elements)\\\"\r\ncompactPartitionExpression = \\\"elements[\\'run_id\\'] % 1000 = 0 and elements[\\'run_id\\'] <= runId - 2000\\\"\r\n}\r\n}}}",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "PartitionArchiveCompactionMode"
		    },
		    "archivePartitionExpression" : {
		      "type" : "string",
		      "description" : "Expression to define the archive partition for a given partition. Define a spark\r\nsql expression working with the attributes of[[PartitionExpressionData]] returning archive\r\npartition values as Map[String,String]. If return value is the same as input elements, partition is not touched,\r\notherwise all files of the partition are moved to the returned partition definition.\r\nBe aware that the value of the partition columns changes for these files/records."
		    },
		    "compactPartitionExpression" : {
		      "type" : "string",
		      "description" : "Expression to define partitions which should be compacted. Define a spark\r\nsql expression working with the attributes of[[PartitionExpressionData]] returning a\r\nboolean = true when this partition should be compacted.\r\nOnce a partition is compacted, it is marked as compacted and will not be compacted again.\r\nIt is therefore ok to return true for all partitions which should be compacted, regardless if they have been compacted already."
		    },
		    "description" : {
		      "type" : "string"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Keep partitions while retention condition is fulfilled, delete other partitions.\r\nExample: cleanup partitions with partition layout dt=<yyyymmdd> after 90 days:\r\n{{{\r\nhousekeepingMode = {\r\ntype = PartitionRetentionMode\r\nretentionCondition = \\\"datediff(now(), to_date(elements[\\'dt\\'], \\'yyyyMMdd\\')) <= 90\\\"\r\n}\r\n}}}",
		  "additionalProperties" : false,
		  "required" : [ "retentionCondition", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "PartitionRetentionMode"
		    },
		    "retentionCondition" : {
		      "type" : "string",
		      "description" : "Condition to decide if a partition should be kept. Define a spark sql expression\r\nworking with the attributes of[[PartitionExpressionData]] returning a boolean with value true if the partition should be kept."
		    },
		    "description" : {
		      "type" : "string"
		    }
		  }
		} ]
	      },
	      "metadata" : {
		"oneOf" : [ {
		  "$ref" : "#/definitions/DataObjectMetadata"
		} ]
	      }
	    }
	  },
	  "SftpFileRefConnection" : {
	    "type" : "object",
	    "description" : "SFTP Connection information",
	    "additionalProperties" : false,
	    "required" : [ "id", "host", "authMode" ],
	    "properties" : {
	      "id" : {
		"type" : "string",
		"description" : "unique id of this connection"
	      },
	      "host" : {
		"type" : "string",
		"description" : "sftp host"
	      },
	      "port" : {
		"type" : "integer",
		"description" : "port of sftp service, default is 22"
	      },
	      "authMode" : {
		"oneOf" : [ {
		  "type" : "object",
		  "description" : "Connect by custom authorization header",
		  "additionalProperties" : false,
		  "required" : [ "secretVariable", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "AuthHeaderMode"
		    },
		    "headerName" : {
		      "type" : "string"
		    },
		    "secretVariable" : {
		      "type" : "string"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Validate by SSL Certificates : Only location an credentials. Additional attributes should be\r\nsupplied via options map",
		  "additionalProperties" : false,
		  "required" : [ "keystorePath", "keystorePassVariable", "truststorePath", "truststorePassVariable", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "SSLCertsAuthMode"
		    },
		    "keystorePath" : {
		      "type" : "string"
		    },
		    "keystoreType" : {
		      "type" : "string"
		    },
		    "keystorePassVariable" : {
		      "type" : "string"
		    },
		    "truststorePath" : {
		      "type" : "string"
		    },
		    "truststoreType" : {
		      "type" : "string"
		    },
		    "truststorePassVariable" : {
		      "type" : "string"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Connect by basic authentication",
		  "additionalProperties" : false,
		  "required" : [ "userVariable", "passwordVariable", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "BasicAuthMode"
		    },
		    "userVariable" : {
		      "type" : "string"
		    },
		    "passwordVariable" : {
		      "type" : "string"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Connect with custom HTTP authentication",
		  "additionalProperties" : false,
		  "required" : [ "className", "options", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "CustomHttpAuthMode"
		    },
		    "className" : {
		      "type" : "string",
		      "description" : "class name implementing trait[[CustomHttpAuthModeLogic]]"
		    },
		    "options" : {
		      "type" : "object",
		      "description" : "Options to pass to the custom auth mode logc in prepare function",
		      "additionalProperties" : {
			"type" : "string"
		      }
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Validate by SASL_SSL Authentication : user / password and truststore",
		  "additionalProperties" : false,
		  "required" : [ "username", "passwordVariable", "sslMechanism", "truststorePath", "truststorePassVariable", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "SASLSCRAMAuthMode"
		    },
		    "username" : {
		      "type" : "string"
		    },
		    "passwordVariable" : {
		      "type" : "string"
		    },
		    "sslMechanism" : {
		      "type" : "string"
		    },
		    "truststorePath" : {
		      "type" : "string"
		    },
		    "truststoreType" : {
		      "type" : "string"
		    },
		    "truststorePassVariable" : {
		      "type" : "string"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Connect by token\r\nFor HTTP Connection this is used as Bearer token in Authorization header.",
		  "additionalProperties" : false,
		  "required" : [ "tokenVariable", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "TokenAuthMode"
		    },
		    "tokenVariable" : {
		      "type" : "string"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Connect by using Keycloak to manage token and token refresh giving clientId/secret as information.\r\nFor HTTP Connection this is used as Bearer token in Authorization header.",
		  "additionalProperties" : false,
		  "required" : [ "ssoServer", "ssoRealm", "ssoGrantType", "clientIdVariable", "clientSecretVariable", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "KeycloakClientSecretAuthMode"
		    },
		    "ssoServer" : {
		      "type" : "string"
		    },
		    "ssoRealm" : {
		      "type" : "string"
		    },
		    "ssoGrantType" : {
		      "type" : "string"
		    },
		    "clientIdVariable" : {
		      "type" : "string"
		    },
		    "clientSecretVariable" : {
		      "type" : "string"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Validate by user and private/public key\r\nPrivate key is read from .ssh",
		  "additionalProperties" : false,
		  "required" : [ "userVariable", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "PublicKeyAuthMode"
		    },
		    "userVariable" : {
		      "type" : "string"
		    }
		  }
		} ]
	      },
	      "ignoreHostKeyVerification" : {
		"type" : "string",
		"description" : "do not validate host key if true, default is false"
	      },
	      "maxParallelConnections" : {
		"type" : "integer",
		"description" : "number of parallel sftp connections created by an instance of this connection"
	      },
	      "connectionPoolMaxIdleTimeSec" : {
		"type" : "integer",
		"description" : "timeout to close unused connections in the pool"
	      },
	      "metadata" : {
		"oneOf" : [ {
		  "$ref" : "#/definitions/ConnectionMetadata"
		} ]
	      }
	    }
	  },
	  "SFtpFileRefDataObject" : {
	    "type" : "object",
	    "description" : "Connects to SFtp files\r\nNeeds java library \\\"com.hieronymus % sshj % 0.21.1\\\"\r\nThe following authentication mechanisms are supported\r\n-> public/private-key: private key must be saved in ~/.ssh, public key must be registered on server.\r\n-> user/pwd authentication: user and password is taken from two variables set as parameters.\r\nThese variables could come from clear text (CLEAR), a file (FILE) or an environment variable (ENV)",
	    "additionalProperties" : false,
	    "required" : [ "id", "path", "connectionId" ],
	    "properties" : {
	      "id" : {
		"type" : "string"
	      },
	      "path" : {
		"type" : "string"
	      },
	      "connectionId" : {
		"type" : "string"
	      },
	      "partitions" : {
		"type" : "array",
		"items" : {
		  "type" : "string"
		}
	      },
	      "partitionLayout" : {
		"type" : "string",
		"description" : "partition layout defines how partition values can be extracted from the path.\r\nUse \\\"%<colname>%\\\" as token to extract the value for a partition column.\r\nWith \\\"%<colname:regex>%\\\" a regex can be given to limit search. This is especially useful\r\nif there is no char to delimit the last token from the rest of the path or also between\r\ntwo tokens."
	      },
	      "saveMode" : {
		"type" : "string",
		"enum" : [ "Merge ", "OverwriteOptimized ", "OverwritePreserveDirectories ", "Ignore ", "ErrorIfExists ", "Append ", "Overwrite " ],
		"description" : "Overwrite or Append new data."
	      },
	      "expectedPartitionsCondition" : {
		"type" : "string",
		"description" : "Optional definition of partitions expected to exist.\r\nDefine a Spark SQL expression that is evaluated against a[[PartitionValues]] instance and returns true or false\r\nDefault is to expect all partitions to exist."
	      },
	      "metadata" : {
		"oneOf" : [ {
		  "$ref" : "#/definitions/DataObjectMetadata"
		} ]
	      }
	    }
	  },
	  "TickTockHiveTableDataObject" : {
	    "type" : "object",
	    "additionalProperties" : false,
	    "required" : [ "id", "table" ],
	    "properties" : {
	      "id" : {
		"type" : "string"
	      },
	      "path" : {
		"type" : "string"
	      },
	      "partitions" : {
		"type" : "array",
		"items" : {
		  "type" : "string"
		}
	      },
	      "analyzeTableAfterWrite" : {
		"type" : "string"
	      },
	      "dateColumnType" : {
		"type" : "string",
		"enum" : [ "Default ", "String ", "Date " ]
	      },
	      "schemaMin" : {
		"type" : "string"
	      },
	      "table" : {
		"oneOf" : [ {
		  "$ref" : "#/definitions/Table"
		} ]
	      },
	      "numInitialHdfsPartitions" : {
		"type" : "integer"
	      },
	      "saveMode" : {
		"type" : "string",
		"enum" : [ "Merge ", "OverwriteOptimized ", "OverwritePreserveDirectories ", "Ignore ", "ErrorIfExists ", "Append ", "Overwrite " ]
	      },
	      "acl" : {
		"type" : "object",
		"description" : "Describes a complete ACL Specification (basic owner/group/other permissions AND extended ACLS)\r\nto be applied to a Data Object on writing",
		"additionalProperties" : false,
		"required" : [ "permission", "acls" ],
		"properties" : {
		  "permission" : {
		    "type" : "string",
		    "description" : ": File system permission string in symbolic notation form (e.g. rwxr-xr-x)"
		  },
		  "acls" : {
		    "type" : "array",
		    "description" : ": a sequence of[[AclElement]] s",
		    "items" : {
		      "type" : "object",
		      "description" : "Describes a single extended ACL to be applied to a Data Object\r\nin addition to the basic file system permissions",
		      "additionalProperties" : false,
		      "required" : [ "aclType", "name", "permission" ],
		      "properties" : {
			"aclType" : {
			  "type" : "string",
			  "description" : ": type of ACL to be added \\\"group\\\", \\\"user\\\""
			},
			"name" : {
			  "type" : "string",
			  "description" : ": the name of the user/group for which an ACL definition is being added"
			},
			"permission" : {
			  "type" : "string",
			  "description" : ": the permission (rwx syntax) to be granted"
			}
		      }
		    }
		  }
		}
	      },
	      "connectionId" : {
		"type" : "string"
	      },
	      "expectedPartitionsCondition" : {
		"type" : "string"
	      },
	      "housekeepingMode" : {
		"oneOf" : [ {
		  "type" : "object",
		  "description" : "Archive and compact old partitions:\r\nArchive partition reduces the number of partitions in the past by moving older partitions into special \\\"archive partitions\\\".\r\nCompact partition reduces the number of files in a partition by rewriting them with Spark.\r\nExample: archive and compact a table with partition layout run_id=<integer>\r\n- archive partitions after 1000 partitions into \\\"archive partition\\\" equal to floor(run_id/1000)\r\n- compact \\\"archive partition\\\" when full\r\n{{{\r\nhousekeepingMode = {\r\ntype = PartitionArchiveCompactionMode\r\narchivePartitionExpression = \\\"if( elements[\\'run_id\\'] < runId - 1000, map(\\'run_id\\', elements[\\'run_id\\'] div 1000), elements)\\\"\r\ncompactPartitionExpression = \\\"elements[\\'run_id\\'] % 1000 = 0 and elements[\\'run_id\\'] <= runId - 2000\\\"\r\n}\r\n}}}",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "PartitionArchiveCompactionMode"
		    },
		    "archivePartitionExpression" : {
		      "type" : "string",
		      "description" : "Expression to define the archive partition for a given partition. Define a spark\r\nsql expression working with the attributes of[[PartitionExpressionData]] returning archive\r\npartition values as Map[String,String]. If return value is the same as input elements, partition is not touched,\r\notherwise all files of the partition are moved to the returned partition definition.\r\nBe aware that the value of the partition columns changes for these files/records."
		    },
		    "compactPartitionExpression" : {
		      "type" : "string",
		      "description" : "Expression to define partitions which should be compacted. Define a spark\r\nsql expression working with the attributes of[[PartitionExpressionData]] returning a\r\nboolean = true when this partition should be compacted.\r\nOnce a partition is compacted, it is marked as compacted and will not be compacted again.\r\nIt is therefore ok to return true for all partitions which should be compacted, regardless if they have been compacted already."
		    },
		    "description" : {
		      "type" : "string"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Keep partitions while retention condition is fulfilled, delete other partitions.\r\nExample: cleanup partitions with partition layout dt=<yyyymmdd> after 90 days:\r\n{{{\r\nhousekeepingMode = {\r\ntype = PartitionRetentionMode\r\nretentionCondition = \\\"datediff(now(), to_date(elements[\\'dt\\'], \\'yyyyMMdd\\')) <= 90\\\"\r\n}\r\n}}}",
		  "additionalProperties" : false,
		  "required" : [ "retentionCondition", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "PartitionRetentionMode"
		    },
		    "retentionCondition" : {
		      "type" : "string",
		      "description" : "Condition to decide if a partition should be kept. Define a spark sql expression\r\nworking with the attributes of[[PartitionExpressionData]] returning a boolean with value true if the partition should be kept."
		    },
		    "description" : {
		      "type" : "string"
		    }
		  }
		} ]
	      },
	      "metadata" : {
		"oneOf" : [ {
		  "$ref" : "#/definitions/DataObjectMetadata"
		} ]
	      }
	    }
	  },
	  "CustomSparkAction" : {
	    "type" : "object",
	    "description" : "[[Action]] to transform data according to a custom transformer.\r\nAllows to transform multiple input and output dataframes.",
	    "additionalProperties" : false,
	    "required" : [ "id", "inputIds", "outputIds" ],
	    "properties" : {
	      "id" : {
		"type" : "string"
	      },
	      "inputIds" : {
		"type" : "array",
		"description" : "input DataObject\\'s",
		"items" : {
		  "type" : "string"
		}
	      },
	      "outputIds" : {
		"type" : "array",
		"description" : "output DataObject\\'s",
		"items" : {
		  "type" : "string"
		}
	      },
	      "transformer" : {
		"type" : "object",
		"description" : "Configuration of a custom Spark-DataFrame transformation between many inputs and many outputs (n:m).\r\nDefine a transform function which receives a map of input DataObjectIds with DataFrames and a map of options and has\r\nto return a map of output DataObjectIds with DataFrames, see also trait[[CustomDfsTransformer]] .",
		"additionalProperties" : false,
		"properties" : {
		  "className" : {
		    "type" : "string",
		    "description" : "Optional class name implementing trait[[CustomDfsTransformer]]"
		  },
		  "scalaFile" : {
		    "type" : "string",
		    "description" : "Optional file where scala code for transformation is loaded from. The scala code in the file needs to be a function of type[[fnTransformType]] ."
		  },
		  "scalaCode" : {
		    "type" : "string",
		    "description" : "Optional scala code for transformation. The scala code needs to be a function of type[[fnTransformType]] ."
		  },
		  "sqlCode" : {
		    "type" : "object",
		    "description" : "Optional map of DataObjectId and corresponding SQL Code.\r\nUse tokens %{<key>} to replace with runtimeOptions in SQL code.\r\nExample: \\\"select * from test where run = %{runId}\\\"",
		    "additionalProperties" : {
		      "type" : "string"
		    }
		  },
		  "options" : {
		    "type" : "object",
		    "description" : "Options to pass to the transformation",
		    "additionalProperties" : {
		      "type" : "string"
		    }
		  },
		  "runtimeOptions" : {
		    "type" : "object",
		    "description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\r\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
		    "additionalProperties" : {
		      "type" : "string"
		    }
		  }
		}
	      },
	      "transformers" : {
		"type" : "array",
		"items" : {
		  "oneOf" : [ {
		    "type" : "object",
		    "description" : "Configuration of a custom Spark-DataFrame transformation between many inputs and many outputs (n:m) as Scala code which is compiled at runtime.\r\nDefine a transform function which receives a map of input DataObjectIds with DataFrames and a map of options and has\r\nto return a map of output DataObjectIds with DataFrames. The scala code has to implement a function of type[[fnTransformType]] .",
		    "additionalProperties" : false,
		    "required" : [ "type" ],
		    "properties" : {
		      "type" : {
			"const" : "ScalaCodeDfsTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      },
		      "code" : {
			"type" : "string",
			"description" : "Scala code for transformation. The scala code needs to be a function of type[[fnTransformType]] ."
		      },
		      "file" : {
			"type" : "string",
			"description" : "File where scala code for transformation is loaded from. The scala code in the file needs to be a function of type[[fnTransformType]] ."
		      },
		      "options" : {
			"type" : "object",
			"description" : "Options to pass to the transformation",
			"additionalProperties" : {
			  "type" : "string"
			}
		      },
		      "runtimeOptions" : {
			"type" : "object",
			"description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\r\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
			"additionalProperties" : {
			  "type" : "string"
			}
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Configuration of a custom Spark-DataFrame transformation between many inputs and many outputs (n:m) as SQL code.\r\nThe input data is available as temporary views in SQL. As name for the temporary views the input DataObjectId is used\r\n(special characters are replaces by underscores).",
		    "additionalProperties" : false,
		    "required" : [ "code", "type" ],
		    "properties" : {
		      "type" : {
			"const" : "SQLDfsTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      },
		      "code" : {
			"type" : "object",
			"description" : "SQL code for transformation.\r\nUse tokens %{<key>} to replace with runtimeOptions in SQL code.\r\nExample: \\\"select * from test where run = %{runId}\\\"\r\nA special token %{inputViewName} can be used to insert the temporary view name.",
			"additionalProperties" : {
			  "type" : "string"
			}
		      },
		      "options" : {
			"type" : "object",
			"description" : "Options to pass to the transformation",
			"additionalProperties" : {
			  "type" : "string"
			}
		      },
		      "runtimeOptions" : {
			"type" : "object",
			"description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\r\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
			"additionalProperties" : {
			  "type" : "string"
			}
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Configuration of a custom Spark-DataFrame transformation between many inputs and many outputs (n:m)\r\nDefine a transform function which receives a map of input DataObjectIds with DataFrames and a map of options and as\r\nto return a map of output DataObjectIds with DataFrames, see also trait[[CustomDfsTransformer]] .",
		    "additionalProperties" : false,
		    "required" : [ "className", "type" ],
		    "properties" : {
		      "type" : {
			"const" : "ScalaClassDfsTransformer"
		      },
		      "name" : {
			"type" : "string",
			"description" : "name of the transformer"
		      },
		      "description" : {
			"type" : "string",
			"description" : "Optional description of the transformer"
		      },
		      "className" : {
			"type" : "string",
			"description" : "class name implementing trait[[CustomDfsTransformer]]"
		      },
		      "options" : {
			"type" : "object",
			"description" : "Options to pass to the transformation",
			"additionalProperties" : {
			  "type" : "string"
			}
		      },
		      "runtimeOptions" : {
			"type" : "object",
			"description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\r\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
			"additionalProperties" : {
			  "type" : "string"
			}
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "A Transformer to use single DataFrame Transformers as multiple DataFrame Transformers.\r\nThis works by selecting the SubFeeds (DataFrames) the single DataFrame Transformer should be applied to.\r\nAll other SubFeeds will be passed through without transformation.",
		    "additionalProperties" : false,
		    "required" : [ "transformer", "subFeedsToApply", "type" ],
		    "properties" : {
		      "type" : {
			"const" : "DfTransformerWrapperDfsTransformer"
		      },
		      "transformer" : {
			"oneOf" : [ {
			  "type" : "object",
			  "description" : "Interface to implement Spark-DataFrame transformers working with one input and one output (1:1).\r\nThis trait extends DfSparkTransformer to pass a map of options as parameter to the transform function. This is mainly\r\nused by custom transformers.",
			  "additionalProperties" : false,
			  "required" : [ "type" ],
			  "properties" : {
			    "type" : {
			      "const" : "OptionsDfTransformer"
			    }
			  }
			}, {
			  "type" : "object",
			  "description" : "Repartition DataFrame\r\nFor detailled description about repartitioning DataFrames see also[[SparkRepartitionDef]]",
			  "additionalProperties" : false,
			  "required" : [ "numberOfTasksPerPartition", "type" ],
			  "properties" : {
			    "type" : {
			      "const" : "RepartitionTransformer"
			    },
			    "name" : {
			      "type" : "string",
			      "description" : "name of the transformer"
			    },
			    "description" : {
			      "type" : "string",
			      "description" : "Optional description of the transformer"
			    },
			    "numberOfTasksPerPartition" : {
			      "type" : "integer",
			      "description" : "Number of Spark tasks to create per partition value by repartitioning the DataFrame."
			    },
			    "keyCols" : {
			      "type" : "array",
			      "description" : "Optional key columns to distribute records over Spark tasks inside a partition value.",
			      "items" : {
				"type" : "string"
			      }
			    }
			  }
			}, {
			  "type" : "object",
			  "description" : "Standardize datatypes of a DataFrame.\r\nCurrent implementation converts all decimal datatypes to a corresponding integral or float datatype",
			  "additionalProperties" : false,
			  "required" : [ "type" ],
			  "properties" : {
			    "type" : {
			      "const" : "StandardizeDatatypesTransformer"
			    },
			    "name" : {
			      "type" : "string",
			      "description" : "name of the transformer"
			    },
			    "description" : {
			      "type" : "string",
			      "description" : "Optional description of the transformer"
			    }
			  }
			}, {
			  "type" : "object",
			  "description" : "Configuration of a custom Spark-DataFrame transformation between one input and one output (1:1) as Scala code which is compiled at runtime.\r\nDefine a transform function which receives a DataObjectId, a DataFrame and a map of options and has to return a\r\nDataFrame. The scala code has to implement a function of type[[fnTransformType]] .",
			  "additionalProperties" : false,
			  "required" : [ "type" ],
			  "properties" : {
			    "type" : {
			      "const" : "ScalaCodeDfTransformer"
			    },
			    "name" : {
			      "type" : "string",
			      "description" : "name of the transformer"
			    },
			    "description" : {
			      "type" : "string",
			      "description" : "Optional description of the transformer"
			    },
			    "code" : {
			      "type" : "string",
			      "description" : "Scala code for transformation. The scala code needs to be a function of type[[fnTransformType]] ."
			    },
			    "file" : {
			      "type" : "string",
			      "description" : "File where scala code for transformation is loaded from. The scala code in the file needs to be a function of type[[fnTransformType]] ."
			    },
			    "options" : {
			      "type" : "object",
			      "description" : "Options to pass to the transformation",
			      "additionalProperties" : {
				"type" : "string"
			      }
			    },
			    "runtimeOptions" : {
			      "type" : "object",
			      "description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\r\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
			      "additionalProperties" : {
				"type" : "string"
			      }
			    }
			  }
			}, {
			  "type" : "object",
			  "description" : "Apply a column blacklist to a DataFrame.",
			  "additionalProperties" : false,
			  "required" : [ "columnBlacklist", "type" ],
			  "properties" : {
			    "type" : {
			      "const" : "BlacklistTransformer"
			    },
			    "name" : {
			      "type" : "string",
			      "description" : "name of the transformer"
			    },
			    "description" : {
			      "type" : "string",
			      "description" : "Optional description of the transformer"
			    },
			    "columnBlacklist" : {
			      "type" : "array",
			      "description" : "List of columns to exclude from DataFrame",
			      "items" : {
				"type" : "string"
			      }
			    }
			  }
			}, {
			  "type" : "object",
			  "description" : "Configuration of a custom Spark-DataFrame transformation between one input and one output (1:1) as Java/Scala Class.\r\nDefine a transform function which receives a DataObjectId, a DataFrame and a map of options and has to return a\r\nDataFrame. The Java/Scala class has to implement interface[[CustomDfTransformer]] .",
			  "additionalProperties" : false,
			  "required" : [ "className", "type" ],
			  "properties" : {
			    "type" : {
			      "const" : "ScalaClassDfTransformer"
			    },
			    "name" : {
			      "type" : "string",
			      "description" : "name of the transformer"
			    },
			    "description" : {
			      "type" : "string",
			      "description" : "Optional description of the transformer"
			    },
			    "className" : {
			      "type" : "string",
			      "description" : "class name implementing trait[[CustomDfTransformer]]"
			    },
			    "options" : {
			      "type" : "object",
			      "description" : "Options to pass to the transformation",
			      "additionalProperties" : {
				"type" : "string"
			      }
			    },
			    "runtimeOptions" : {
			      "type" : "object",
			      "description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\r\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
			      "additionalProperties" : {
				"type" : "string"
			      }
			    }
			  }
			}, {
			  "type" : "object",
			  "description" : "Add additional columns to the DataFrame by extracting information from the context.",
			  "additionalProperties" : false,
			  "required" : [ "additionalColumns", "type" ],
			  "properties" : {
			    "type" : {
			      "const" : "AdditionalColumnsTransformer"
			    },
			    "name" : {
			      "type" : "string",
			      "description" : "name of the transformer"
			    },
			    "description" : {
			      "type" : "string",
			      "description" : "Optional description of the transformer"
			    },
			    "additionalColumns" : {
			      "type" : "object",
			      "description" : "optional tuples of [column name, spark sql expression] to be added as additional columns to the dataframe.\r\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
			      "additionalProperties" : {
				"type" : "string"
			      }
			    }
			  }
			}, {
			  "type" : "object",
			  "description" : "Configuration of a custom Spark-DataFrame transformation between one input and one output (1:1) as Python/PySpark code.\r\nNote that this transformer needs a Python and PySpark environment installed.\r\nPySpark session is initialize and available under variables`sc`,`session`,`sqlContext`.\r\nOther variables available are\r\n-`inputDf`: Input DataFrame\r\n-`options`: Transformation options as Map[String,String]\r\n-`dataObjectId`: Id of input dataObject as String\r\nOutput DataFrame must be set with`setOutputDf(df)` .",
			  "additionalProperties" : false,
			  "required" : [ "type" ],
			  "properties" : {
			    "type" : {
			      "const" : "PythonCodeDfTransformer"
			    },
			    "name" : {
			      "type" : "string",
			      "description" : "name of the transformer"
			    },
			    "description" : {
			      "type" : "string",
			      "description" : "Optional description of the transformer"
			    },
			    "code" : {
			      "type" : "string",
			      "description" : "Optional python code to user for python transformation. The python code can use variables inputDf, dataObjectId and options. The transformed DataFrame has to be set with setOutputDf."
			    },
			    "file" : {
			      "type" : "string",
			      "description" : "Optional file with python code to use for python transformation. The python code can use variables inputDf, dataObjectId and options. The transformed DataFrame has to be set with setOutputDf."
			    },
			    "options" : {
			      "type" : "object",
			      "description" : "Options to pass to the transformation",
			      "additionalProperties" : {
				"type" : "string"
			      }
			    },
			    "runtimeOptions" : {
			      "type" : "object",
			      "description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\r\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
			      "additionalProperties" : {
				"type" : "string"
			      }
			    }
			  }
			}, {
			  "type" : "object",
			  "description" : "Configuration of a custom Spark-DataFrame transformation between one input and one output (1:1) as SQL code.\r\nThe input data is available as temporary view in SQL. As name for the temporary view the input DataObjectId is used\r\n(special characters are replaces by underscores). A special token \\'%{inputViewName}\\' will be replaced with the name of\r\nthe temporary view at runtime.",
			  "additionalProperties" : false,
			  "required" : [ "code", "type" ],
			  "properties" : {
			    "type" : {
			      "const" : "SQLDfTransformer"
			    },
			    "name" : {
			      "type" : "string",
			      "description" : "name of the transformer"
			    },
			    "description" : {
			      "type" : "string",
			      "description" : "Optional description of the transformer"
			    },
			    "code" : {
			      "type" : "string",
			      "description" : "SQL code for transformation.\r\nUse tokens %{<key>} to replace with runtimeOptions in SQL code.\r\nExample: \\\"select * from test where run = %{runId}\\\"\r\nA special token %{inputViewName} can be used to insert the temporary view name."
			    },
			    "options" : {
			      "type" : "object",
			      "description" : "Options to pass to the transformation",
			      "additionalProperties" : {
				"type" : "string"
			      }
			    },
			    "runtimeOptions" : {
			      "type" : "object",
			      "description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\r\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
			      "additionalProperties" : {
				"type" : "string"
			      }
			    }
			  }
			}, {
			  "type" : "object",
			  "description" : "Apply validation rules to a DataFrame and collect potential violation error messages in a new column.",
			  "additionalProperties" : false,
			  "required" : [ "rules", "type" ],
			  "properties" : {
			    "type" : {
			      "const" : "DataValidationTransformer"
			    },
			    "name" : {
			      "type" : "string",
			      "description" : "name of the transformer"
			    },
			    "description" : {
			      "type" : "string",
			      "description" : "Optional description of the transformer"
			    },
			    "rules" : {
			      "type" : "array",
			      "description" : "list of validation rules to apply to the DataFrame",
			      "items" : {
				"oneOf" : [ {
				  "type" : "object",
				  "description" : "Definition for a row level data validation rule.",
				  "additionalProperties" : false,
				  "required" : [ "condition", "type" ],
				  "properties" : {
				    "type" : {
				      "const" : "RowLevelValidationRule"
				    },
				    "condition" : {
				      "type" : "string",
				      "description" : "a Spark SQL expression defining the condition to be tested."
				    },
				    "errorMsg" : {
				      "type" : "string",
				      "description" : "Optional error msg to be create if the condition fails. Default is to use a text representation of the condition."
				    }
				  }
				} ]
			      }
			    },
			    "errorsColumn" : {
			      "type" : "string",
			      "description" : "Optional column name for the list of error messages. Default is \\\"errors\\\"."
			    }
			  }
			}, {
			  "type" : "object",
			  "description" : "Apply a column whitelist to a DataFrame.",
			  "additionalProperties" : false,
			  "required" : [ "columnWhitelist", "type" ],
			  "properties" : {
			    "type" : {
			      "const" : "WhitelistTransformer"
			    },
			    "name" : {
			      "type" : "string",
			      "description" : "name of the transformer"
			    },
			    "description" : {
			      "type" : "string",
			      "description" : "Optional description of the transformer"
			    },
			    "columnWhitelist" : {
			      "type" : "array",
			      "description" : "List of columns to keep from DataFrame",
			      "items" : {
				"type" : "string"
			      }
			    }
			  }
			}, {
			  "type" : "object",
			  "description" : "Apply a filter condition to a DataFrame.",
			  "additionalProperties" : false,
			  "required" : [ "filterClause", "type" ],
			  "properties" : {
			    "type" : {
			      "const" : "FilterTransformer"
			    },
			    "name" : {
			      "type" : "string",
			      "description" : "name of the transformer"
			    },
			    "description" : {
			      "type" : "string",
			      "description" : "Optional description of the transformer"
			    },
			    "filterClause" : {
			      "type" : "string",
			      "description" : "Spark SQL expression to filter the DataFrame"
			    }
			  }
			}, {
			  "type" : "object",
			  "description" : "Configuration of a custom Spark-DataFrame transformation between one input and one output (1:1) as Scala code which is compiled at runtime.\r\nThe code is loaded from a Notebook. It should define a transform function with a configurable name, which receives a DataObjectId, a DataFrame\r\nand a map of options and has to return a DataFrame, see also ([[fnTransformType]] ).\r\nNotebook-cells starting with \\\"//!IGNORE\\\" will be ignored.",
			  "additionalProperties" : false,
			  "required" : [ "url", "functionName", "type" ],
			  "properties" : {
			    "type" : {
			      "const" : "ScalaNotebookDfTransformer"
			    },
			    "name" : {
			      "type" : "string",
			      "description" : "name of the transformer"
			    },
			    "description" : {
			      "type" : "string",
			      "description" : "Optional description of the transformer"
			    },
			    "url" : {
			      "type" : "string",
			      "description" : "Url to download notebook in IPYNB-format, which defines transformation."
			    },
			    "functionName" : {
			      "type" : "string",
			      "description" : "The notebook needs to contain a Scala-function with this name and type[[fnTransformType]] ."
			    },
			    "authMode" : {
			      "oneOf" : [ {
				"type" : "object",
				"description" : "Connect by custom authorization header",
				"additionalProperties" : false,
				"required" : [ "secretVariable", "type" ],
				"properties" : {
				  "type" : {
				    "const" : "AuthHeaderMode"
				  },
				  "headerName" : {
				    "type" : "string"
				  },
				  "secretVariable" : {
				    "type" : "string"
				  }
				}
			      }, {
				"type" : "object",
				"description" : "Validate by SSL Certificates : Only location an credentials. Additional attributes should be\r\nsupplied via options map",
				"additionalProperties" : false,
				"required" : [ "keystorePath", "keystorePassVariable", "truststorePath", "truststorePassVariable", "type" ],
				"properties" : {
				  "type" : {
				    "const" : "SSLCertsAuthMode"
				  },
				  "keystorePath" : {
				    "type" : "string"
				  },
				  "keystoreType" : {
				    "type" : "string"
				  },
				  "keystorePassVariable" : {
				    "type" : "string"
				  },
				  "truststorePath" : {
				    "type" : "string"
				  },
				  "truststoreType" : {
				    "type" : "string"
				  },
				  "truststorePassVariable" : {
				    "type" : "string"
				  }
				}
			      }, {
				"type" : "object",
				"description" : "Connect by basic authentication",
				"additionalProperties" : false,
				"required" : [ "userVariable", "passwordVariable", "type" ],
				"properties" : {
				  "type" : {
				    "const" : "BasicAuthMode"
				  },
				  "userVariable" : {
				    "type" : "string"
				  },
				  "passwordVariable" : {
				    "type" : "string"
				  }
				}
			      }, {
				"type" : "object",
				"description" : "Connect with custom HTTP authentication",
				"additionalProperties" : false,
				"required" : [ "className", "options", "type" ],
				"properties" : {
				  "type" : {
				    "const" : "CustomHttpAuthMode"
				  },
				  "className" : {
				    "type" : "string",
				    "description" : "class name implementing trait[[CustomHttpAuthModeLogic]]"
				  },
				  "options" : {
				    "type" : "object",
				    "description" : "Options to pass to the custom auth mode logc in prepare function",
				    "additionalProperties" : {
				      "type" : "string"
				    }
				  }
				}
			      }, {
				"type" : "object",
				"description" : "Validate by SASL_SSL Authentication : user / password and truststore",
				"additionalProperties" : false,
				"required" : [ "username", "passwordVariable", "sslMechanism", "truststorePath", "truststorePassVariable", "type" ],
				"properties" : {
				  "type" : {
				    "const" : "SASLSCRAMAuthMode"
				  },
				  "username" : {
				    "type" : "string"
				  },
				  "passwordVariable" : {
				    "type" : "string"
				  },
				  "sslMechanism" : {
				    "type" : "string"
				  },
				  "truststorePath" : {
				    "type" : "string"
				  },
				  "truststoreType" : {
				    "type" : "string"
				  },
				  "truststorePassVariable" : {
				    "type" : "string"
				  }
				}
			      }, {
				"type" : "object",
				"description" : "Connect by token\r\nFor HTTP Connection this is used as Bearer token in Authorization header.",
				"additionalProperties" : false,
				"required" : [ "tokenVariable", "type" ],
				"properties" : {
				  "type" : {
				    "const" : "TokenAuthMode"
				  },
				  "tokenVariable" : {
				    "type" : "string"
				  }
				}
			      }, {
				"type" : "object",
				"description" : "Connect by using Keycloak to manage token and token refresh giving clientId/secret as information.\r\nFor HTTP Connection this is used as Bearer token in Authorization header.",
				"additionalProperties" : false,
				"required" : [ "ssoServer", "ssoRealm", "ssoGrantType", "clientIdVariable", "clientSecretVariable", "type" ],
				"properties" : {
				  "type" : {
				    "const" : "KeycloakClientSecretAuthMode"
				  },
				  "ssoServer" : {
				    "type" : "string"
				  },
				  "ssoRealm" : {
				    "type" : "string"
				  },
				  "ssoGrantType" : {
				    "type" : "string"
				  },
				  "clientIdVariable" : {
				    "type" : "string"
				  },
				  "clientSecretVariable" : {
				    "type" : "string"
				  }
				}
			      }, {
				"type" : "object",
				"description" : "Validate by user and private/public key\r\nPrivate key is read from .ssh",
				"additionalProperties" : false,
				"required" : [ "userVariable", "type" ],
				"properties" : {
				  "type" : {
				    "const" : "PublicKeyAuthMode"
				  },
				  "userVariable" : {
				    "type" : "string"
				  }
				}
			      } ]
			    },
			    "options" : {
			      "type" : "object",
			      "description" : "Options to pass to the transformation",
			      "additionalProperties" : {
				"type" : "string"
			      }
			    },
			    "runtimeOptions" : {
			      "type" : "object",
			      "description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\r\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
			      "additionalProperties" : {
				"type" : "string"
			      }
			    }
			  }
			} ]
		      },
		      "subFeedsToApply" : {
			"type" : "array",
			"description" : "Names of SubFeeds the transformation should be applied to.",
			"items" : {
			  "type" : "string"
			}
		      }
		    }
		  }, {
		    "type" : "object",
		    "description" : "Interface to implement Spark-DataFrame transformers working with many inputs and many outputs (n:m)\r\nThis trait extends DfSparkTransformer to pass a map of options as parameter to the transform function. This is mainly\r\nused by custom transformers.",
		    "additionalProperties" : false,
		    "required" : [ "type" ],
		    "properties" : {
		      "type" : {
			"const" : "OptionsDfsTransformer"
		      }
		    }
		  } ]
		}
	      },
	      "breakDataFrameLineage" : {
		"type" : "string"
	      },
	      "persist" : {
		"type" : "string"
	      },
	      "mainInputId" : {
		"type" : "string",
		"description" : "optional selection of main inputId used for execution mode and partition values propagation. Only needed if there are multiple input DataObject\\'s."
	      },
	      "mainOutputId" : {
		"type" : "string",
		"description" : "optional selection of main outputId used for execution mode and partition values propagation. Only needed if there are multiple output DataObject\\'s."
	      },
	      "executionMode" : {
		"oneOf" : [ {
		  "type" : "object",
		  "description" : "Execution mode to incrementally process file-based DataObjects.\r\nIt takes all existing files in the input DataObject and removes (deletes) them after processing.\r\nInput partition values are applied when searching for files and also used as output partition values.",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "FileIncrementalMoveMode"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "An execution mode for incremental processing by remembering DataObjects state from last increment.",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "DataObjectStateIncrementalMode"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Spark streaming execution mode uses Spark Structured Streaming to incrementally execute data loads and keep track of processed data.\r\nThis mode needs a DataObject implementing CanCreateStreamingDataFrame and works only with SparkSubFeeds.\r\nThis mode can be executed synchronously in the DAG by using triggerType=Once, or asynchronously as Streaming Query with triggerType = ProcessingTime or Continuous.",
		  "additionalProperties" : false,
		  "required" : [ "checkpointLocation", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "SparkStreamingMode"
		    },
		    "checkpointLocation" : {
		      "type" : "string",
		      "description" : "location for checkpoints of streaming query to keep state"
		    },
		    "triggerType" : {
		      "type" : "string",
		      "description" : "define execution interval of Spark streaming query. Possible values are Once (default), ProcessingTime & Continuous. See[[Trigger]] for details.\r\n                      Note that this is only applied if SDL is executed in streaming mode. If SDL is executed in normal mode, TriggerType=Once is used always.\r\nIf triggerType=Once, the action is repeated with Trigger.Once in SDL streaming mode."
		    },
		    "triggerTime" : {
		      "type" : "string",
		      "description" : "Time as String in triggerType = ProcessingTime or Continuous. See[[Trigger]] for details."
		    },
		    "inputOptions" : {
		      "type" : "object",
		      "description" : "additional option to apply when reading streaming source. This overwrites options set by the DataObjects.",
		      "additionalProperties" : {
			"type" : "string"
		      }
		    },
		    "outputOptions" : {
		      "type" : "object",
		      "description" : "additional option to apply when writing to streaming sink. This overwrites options set by the DataObjects.",
		      "additionalProperties" : {
			"type" : "string"
		      }
		    },
		    "outputMode" : {
		      "type" : "string",
		      "enum" : [ "Append", "Complete", "Update" ]
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Compares max entry in \\\"compare column\\\" between mainOutput and mainInput and incrementally loads the delta.\r\nThis mode works only with SparkSubFeeds. The filter is not propagated to following actions.",
		  "additionalProperties" : false,
		  "required" : [ "compareCol", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "SparkIncrementalMode"
		    },
		    "compareCol" : {
		      "type" : "string",
		      "description" : "a comparable column name existing in mainInput and mainOutput used to identify the delta. Column content should be bigger for newer records."
		    },
		    "alternativeOutputId" : {
		      "type" : "string",
		      "description" : "optional alternative outputId of DataObject later in the DAG. This replaces the mainOutputId.\r\nIt can be used to ensure processing all partitions over multiple actions in case of errors."
		    },
		    "applyCondition" : {
		      "type" : "object",
		      "description" : "Definition of a Spark SQL condition with description.\r\nThis is used for example to define failConditions of[[PartitionDiffMode]] .",
		      "additionalProperties" : false,
		      "required" : [ "expression" ],
		      "properties" : {
			"expression" : {
			  "type" : "string",
			  "description" : "Condition formulated as Spark SQL. The attributes available are dependent on the context."
			},
			"description" : {
			  "type" : "string",
			  "description" : "A textual description of the condition to be shown in error messages."
			}
		      }
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "An execution mode which forces processing all data from it\\'s inputs.",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "ProcessAllMode"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "An execution mode which just validates that partition values are given.\r\nNote: For start nodes of the DAG partition values can be defined by command line, for subsequent nodes partition values are passed on from previous nodes.",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "FailIfNoPartitionValuesMode"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Partition difference execution mode lists partitions on mainInput & mainOutput DataObject and starts loading all missing partitions.\r\nPartition columns to be used for comparision need to be a common \\'init\\' of input and output partition columns.\r\nThis mode needs mainInput/Output DataObjects which CanHandlePartitions to list partitions.\r\nPartition values are passed to following actions for partition columns which they have in common.",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "PartitionDiffMode"
		    },
		    "partitionColNb" : {
		      "type" : "integer",
		      "description" : "optional number of partition columns to use as a common \\'init\\'."
		    },
		    "alternativeOutputId" : {
		      "type" : "string",
		      "description" : "optional alternative outputId of DataObject later in the DAG. This replaces the mainOutputId.\r\nIt can be used to ensure processing all partitions over multiple actions in case of errors."
		    },
		    "nbOfPartitionValuesPerRun" : {
		      "type" : "integer",
		      "description" : "optional restriction of the number of partition values per run."
		    },
		    "applyCondition" : {
		      "type" : "string",
		      "description" : "Condition to decide if execution mode should be applied or not. Define a spark sql expression working with attributes of[[DefaultExecutionModeExpressionData]] returning a boolean.\r\nDefault is to apply the execution mode if given partition values (partition values from command line or passed from previous action) are not empty."
		    },
		    "failCondition" : {
		      "type" : "string"
		    },
		    "failConditions" : {
		      "type" : "array",
		      "description" : "List of conditions to fail application of execution mode if true. Define as spark sql expressions working with attributes of[[PartitionDiffModeExpressionData]] returning a boolean.\r\nDefault is that the application of the PartitionDiffMode does not fail the action. If there is no data to process, the following actions are skipped.\r\nMultiple conditions are evaluated individually and every condition may fail the execution mode (or-logic)",
		      "items" : {
			"type" : "object",
			"description" : "Definition of a Spark SQL condition with description.\r\nThis is used for example to define failConditions of[[PartitionDiffMode]] .",
			"additionalProperties" : false,
			"required" : [ "expression" ],
			"properties" : {
			  "expression" : {
			    "type" : "string",
			    "description" : "Condition formulated as Spark SQL. The attributes available are dependent on the context."
			  },
			  "description" : {
			    "type" : "string",
			    "description" : "A textual description of the condition to be shown in error messages."
			  }
			}
		      }
		    },
		    "selectExpression" : {
		      "type" : "string",
		      "description" : "optional expression to define or refine the list of selected output partitions. Define a spark sql expression working with the attributes of[[PartitionDiffModeExpressionData]] returning a list<map<string,string>>.\r\nDefault is to return the originally selected output partitions found in attribute selectedOutputPartitionValues."
		    },
		    "applyPartitionValuesTransform" : {
		      "type" : "string",
		      "description" : "If true applies the partition values transform of custom transformations on input partition values before comparison with output partition values.\r\nIf enabled input and output partition columns can be different. Default is to disable the transformation of partition values."
		    },
		    "selectAdditionalInputExpression" : {
		      "type" : "string",
		      "description" : "optional expression to refine the list of selected input partitions. Note that primarily output partitions are selected by PartitionDiffMode.\r\nThe selected output partitions are then transformed back to the input partitions needed to create the selected output partitions. This is one-to-one except if applyPartitionValuesTransform=true.\r\nAnd sometimes there is a need for additional input data to create the output partitions, e.g. if you aggregate a window of 7 days for every day.\r\nYou can customize selected input partitions by defining a spark sql expression working with the attributes of[[PartitionDiffModeExpressionData]] returning a list<map<string,string>>.\r\nDefault is to return the originally selected input partitions found in attribute selectedInputPartitionValues."
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Execution mode to create custom partition execution mode logic.\r\nDefine a function which receives main input&output DataObject and returns partition values to process as Seq[Map[String,String]\\\\]",
		  "additionalProperties" : false,
		  "required" : [ "className", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "CustomPartitionMode"
		    },
		    "className" : {
		      "type" : "string",
		      "description" : "class name implementing trait[[CustomPartitionModeLogic]]"
		    },
		    "alternativeOutputId" : {
		      "type" : "string",
		      "description" : "optional alternative outputId of DataObject later in the DAG. This replaces the mainOutputId.\r\nIt can be used to ensure processing all partitions over multiple actions in case of errors."
		    },
		    "options" : {
		      "type" : "object",
		      "description" : "Options specified in the configuration for this execution mode",
		      "additionalProperties" : {
			"type" : "string"
		      }
		    }
		  }
		} ]
	      },
	      "executionCondition" : {
		"type" : "object",
		"description" : "Definition of a Spark SQL condition with description.\r\nThis is used for example to define failConditions of[[PartitionDiffMode]] .",
		"additionalProperties" : false,
		"required" : [ "expression" ],
		"properties" : {
		  "expression" : {
		    "type" : "string",
		    "description" : "Condition formulated as Spark SQL. The attributes available are dependent on the context."
		  },
		  "description" : {
		    "type" : "string",
		    "description" : "A textual description of the condition to be shown in error messages."
		  }
		}
	      },
	      "metricsFailCondition" : {
		"type" : "string",
		"description" : "optional spark sql expression evaluated as where-clause against dataframe of metrics. Available columns are dataObjectId, key, value.\r\nIf there are any rows passing the where clause, a MetricCheckFailed exception is thrown."
	      },
	      "metadata" : {
		"oneOf" : [ {
		  "$ref" : "#/definitions/ActionMetadata"
		} ]
	      },
	      "recursiveInputIds" : {
		"type" : "array",
		"description" : "output of action that are used as input in the same action",
		"items" : {
		  "type" : "string"
		}
	      },
	      "inputIdsToIgnoreFilter" : {
		"type" : "array",
		"description" : "optional list of input ids to ignore filter (partition values & filter clause)",
		"items" : {
		  "type" : "string"
		}
	      }
	    }
	  },
	  "ParquetFileDataObject" : {
	    "type" : "object",
	    "description" : "A[[io.smartdatalake.workflow.dataobject.DataObject]]backed by an Apache Hive data source.\r\nIt manages read and write access and configurations required for[[io.smartdatalake.workflow.action.Action]]s to\r\nwork on Parquet formatted files.\r\nReading and writing details are delegated to Apache Spark[[org.apache.spark.sql.DataFrameReader]]\r\nand[[org.apache.spark.sql.DataFrameWriter]] respectively.\r\n\nSEE: [[org.apache.spark.sql.DataFrameReader]]\r\nSEE: [[org.apache.spark.sql.DataFrameWriter]]",
	    "additionalProperties" : false,
	    "required" : [ "id", "path" ],
	    "properties" : {
	      "id" : {
		"type" : "string",
		"description" : "unique name of this data object"
	      },
	      "path" : {
		"type" : "string",
		"description" : "Hadoop directory where this data object reads/writes it\\'s files.\r\nIf it doesn\\'t contain scheme and authority, the connections pathPrefix is applied. If pathPrefix is not\r\ndefined or doesn\\'t define scheme and authority, default schema and authority is applied.\r\nOptionally defined partitions are appended with hadoop standard partition layout to this path.\r\nOnly files ending with *.parquet* are considered as data for this DataObject."
	      },
	      "partitions" : {
		"type" : "array",
		"description" : "partition columns for this data object",
		"items" : {
		  "type" : "string"
		}
	      },
	      "parquetOptions" : {
		"type" : "object",
		"description" : "Settings for the underlying[[org.apache.spark.sql.DataFrameReader]]and\r\n[[org.apache.spark.sql.DataFrameWriter]] .",
		"additionalProperties" : {
		  "type" : "string"
		}
	      },
	      "schema" : {
		"type" : "string",
		"description" : "An optional schema for the spark data frame to be validated on read and write. Note: Existing Parquet files\r\ncontain a source schema. Therefore, this schema is ignored when reading from existing Parquet files.\r\nAs this corresponds to the schema on write, it must not include the optional filenameColumn on read."
	      },
	      "schemaMin" : {
		"type" : "string"
	      },
	      "saveMode" : {
		"type" : "string",
		"enum" : [ "Merge ", "OverwriteOptimized ", "OverwritePreserveDirectories ", "Ignore ", "ErrorIfExists ", "Append ", "Overwrite " ],
		"description" : "spark[[SaveMode]] to use when writing files, default is \\\"overwrite\\\""
	      },
	      "sparkRepartition" : {
		"type" : "object",
		"description" : "This controls repartitioning of the DataFrame before writing with Spark to Hadoop.\r\nWhen writing multiple partitions of a partitioned DataObject, the number of spark tasks created is equal to numberOfTasksPerPartition\r\nmultiplied with the number of partitions to write. To spread the records of a partition only over numberOfTasksPerPartition spark tasks,\r\nkeyCols must be given which are used to derive a task number inside the partition (hashvalue(keyCols) modulo numberOfTasksPerPartition).\r\nWhen writing to an unpartitioned DataObject or only one partition of a partitioned DataObject, the number of spark tasks created is equal\r\nto numberOfTasksPerPartition. Optional keyCols can be used to keep corresponding records together in the same task/file.",
		"additionalProperties" : false,
		"required" : [ "numberOfTasksPerPartition" ],
		"properties" : {
		  "numberOfTasksPerPartition" : {
		    "type" : "integer",
		    "description" : "Number of Spark tasks to create per partition before writing to DataObject by repartitioning the DataFrame.\r\nThis controls how many files are created in each Hadoop partition."
		  },
		  "keyCols" : {
		    "type" : "array",
		    "description" : "Optional key columns to distribute records over Spark tasks inside a Hadoop partition.\r\nIf DataObject has Hadoop partitions defined, keyCols must be defined.",
		    "items" : {
		      "type" : "string"
		    }
		  },
		  "sortCols" : {
		    "type" : "array",
		    "description" : "Optional columns to sort records inside files created.",
		    "items" : {
		      "type" : "string"
		    }
		  },
		  "filename" : {
		    "type" : "string",
		    "description" : "Option filename to rename target file(s). If numberOfTasksPerPartition is greater than 1,\r\nmultiple files can exist in a directory and a number is inserted into the filename after the first \\'.\\'.\r\nExample: filename=data.csv -> files created are data.1.csv, data.2.csv, ..."
		  }
		}
	      },
	      "acl" : {
		"type" : "object",
		"description" : "Describes a complete ACL Specification (basic owner/group/other permissions AND extended ACLS)\r\nto be applied to a Data Object on writing",
		"additionalProperties" : false,
		"required" : [ "permission", "acls" ],
		"properties" : {
		  "permission" : {
		    "type" : "string",
		    "description" : ": File system permission string in symbolic notation form (e.g. rwxr-xr-x)"
		  },
		  "acls" : {
		    "type" : "array",
		    "description" : ": a sequence of[[AclElement]] s",
		    "items" : {
		      "type" : "object",
		      "description" : "Describes a single extended ACL to be applied to a Data Object\r\nin addition to the basic file system permissions",
		      "additionalProperties" : false,
		      "required" : [ "aclType", "name", "permission" ],
		      "properties" : {
			"aclType" : {
			  "type" : "string",
			  "description" : ": type of ACL to be added \\\"group\\\", \\\"user\\\""
			},
			"name" : {
			  "type" : "string",
			  "description" : ": the name of the user/group for which an ACL definition is being added"
			},
			"permission" : {
			  "type" : "string",
			  "description" : ": the permission (rwx syntax) to be granted"
			}
		      }
		    }
		  }
		}
	      },
	      "connectionId" : {
		"type" : "string",
		"description" : "optional id of[[io.smartdatalake.workflow.connection.HadoopFileConnection]]"
	      },
	      "filenameColumn" : {
		"type" : "string"
	      },
	      "expectedPartitionsCondition" : {
		"type" : "string",
		"description" : "Optional definition of partitions expected to exist.\r\nDefine a Spark SQL expression that is evaluated against a[[PartitionValues]] instance and returns true or false\r\nDefault is to expect all partitions to exist."
	      },
	      "housekeepingMode" : {
		"oneOf" : [ {
		  "type" : "object",
		  "description" : "Archive and compact old partitions:\r\nArchive partition reduces the number of partitions in the past by moving older partitions into special \\\"archive partitions\\\".\r\nCompact partition reduces the number of files in a partition by rewriting them with Spark.\r\nExample: archive and compact a table with partition layout run_id=<integer>\r\n- archive partitions after 1000 partitions into \\\"archive partition\\\" equal to floor(run_id/1000)\r\n- compact \\\"archive partition\\\" when full\r\n{{{\r\nhousekeepingMode = {\r\ntype = PartitionArchiveCompactionMode\r\narchivePartitionExpression = \\\"if( elements[\\'run_id\\'] < runId - 1000, map(\\'run_id\\', elements[\\'run_id\\'] div 1000), elements)\\\"\r\ncompactPartitionExpression = \\\"elements[\\'run_id\\'] % 1000 = 0 and elements[\\'run_id\\'] <= runId - 2000\\\"\r\n}\r\n}}}",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "PartitionArchiveCompactionMode"
		    },
		    "archivePartitionExpression" : {
		      "type" : "string",
		      "description" : "Expression to define the archive partition for a given partition. Define a spark\r\nsql expression working with the attributes of[[PartitionExpressionData]] returning archive\r\npartition values as Map[String,String]. If return value is the same as input elements, partition is not touched,\r\notherwise all files of the partition are moved to the returned partition definition.\r\nBe aware that the value of the partition columns changes for these files/records."
		    },
		    "compactPartitionExpression" : {
		      "type" : "string",
		      "description" : "Expression to define partitions which should be compacted. Define a spark\r\nsql expression working with the attributes of[[PartitionExpressionData]] returning a\r\nboolean = true when this partition should be compacted.\r\nOnce a partition is compacted, it is marked as compacted and will not be compacted again.\r\nIt is therefore ok to return true for all partitions which should be compacted, regardless if they have been compacted already."
		    },
		    "description" : {
		      "type" : "string"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Keep partitions while retention condition is fulfilled, delete other partitions.\r\nExample: cleanup partitions with partition layout dt=<yyyymmdd> after 90 days:\r\n{{{\r\nhousekeepingMode = {\r\ntype = PartitionRetentionMode\r\nretentionCondition = \\\"datediff(now(), to_date(elements[\\'dt\\'], \\'yyyyMMdd\\')) <= 90\\\"\r\n}\r\n}}}",
		  "additionalProperties" : false,
		  "required" : [ "retentionCondition", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "PartitionRetentionMode"
		    },
		    "retentionCondition" : {
		      "type" : "string",
		      "description" : "Condition to decide if a partition should be kept. Define a spark sql expression\r\nworking with the attributes of[[PartitionExpressionData]] returning a boolean with value true if the partition should be kept."
		    },
		    "description" : {
		      "type" : "string"
		    }
		  }
		} ]
	      },
	      "metadata" : {
		"oneOf" : [ {
		  "$ref" : "#/definitions/DataObjectMetadata"
		} ]
	      }
	    }
	  },
	  "HiveTableConnection" : {
	    "type" : "object",
	    "description" : "Connection information for hive tables",
	    "additionalProperties" : false,
	    "required" : [ "id", "db", "pathPrefix" ],
	    "properties" : {
	      "id" : {
		"type" : "string",
		"description" : "unique id of this connection"
	      },
	      "db" : {
		"type" : "string",
		"description" : "hive db"
	      },
	      "pathPrefix" : {
		"type" : "string",
		"description" : "schema, authority and base path for tables directory on hadoop"
	      },
	      "acl" : {
		"type" : "object",
		"description" : "Describes a complete ACL Specification (basic owner/group/other permissions AND extended ACLS)\r\nto be applied to a Data Object on writing",
		"additionalProperties" : false,
		"required" : [ "permission", "acls" ],
		"properties" : {
		  "permission" : {
		    "type" : "string",
		    "description" : ": File system permission string in symbolic notation form (e.g. rwxr-xr-x)"
		  },
		  "acls" : {
		    "type" : "array",
		    "description" : ": a sequence of[[AclElement]] s",
		    "items" : {
		      "type" : "object",
		      "description" : "Describes a single extended ACL to be applied to a Data Object\r\nin addition to the basic file system permissions",
		      "additionalProperties" : false,
		      "required" : [ "aclType", "name", "permission" ],
		      "properties" : {
			"aclType" : {
			  "type" : "string",
			  "description" : ": type of ACL to be added \\\"group\\\", \\\"user\\\""
			},
			"name" : {
			  "type" : "string",
			  "description" : ": the name of the user/group for which an ACL definition is being added"
			},
			"permission" : {
			  "type" : "string",
			  "description" : ": the permission (rwx syntax) to be granted"
			}
		      }
		    }
		  }
		}
	      },
	      "metadata" : {
		"oneOf" : [ {
		  "$ref" : "#/definitions/ConnectionMetadata"
		} ]
	      }
	    }
	  },
	  "ActionMetadata" : {
	    "type" : "object",
	    "description" : "Additional metadata for an Action",
	    "additionalProperties" : false,
	    "properties" : {
	      "name" : {
		"type" : "string",
		"description" : "Readable name of the Action"
	      },
	      "description" : {
		"type" : "string",
		"description" : "Description of the content of the Action"
	      },
	      "feed" : {
		"type" : "string",
		"description" : "Name of the feed this Action belongs to"
	      },
	      "tags" : {
		"type" : "array",
		"description" : "Optional custom tags for this object",
		"items" : {
		  "type" : "string"
		}
	      }
	    }
	  },
	  "DataObjectsExporterDataObject" : {
	    "type" : "object",
	    "description" : "Exports a util[[DataFrame]]that contains properties and metadata extracted from all[[DataObject]]s\r\nthat are registered in the current[[InstanceRegistry]].\r\nAlternatively, it can export the properties and metadata of all[[DataObject]]s defined in config files. For this, the\r\nconfiguration \\\"config\\\" has to be set to the location of the config.\r\nExample:\r\n{{{\r\n```dataObjects = {\r\n...\r\ndataobject-exporter {\r\ntype = DataObjectsExporterDataObject\r\nconfig = path/to/myconfiguration.conf\r\n}\r\n...\r\n}\r\n}}}\nThe config value can point to a configuration file or a directory containing configuration files.\r\n\nSEE: Refer to[[ConfigLoader.loadConfigFromFilesystem()]] for details about the configuration loading.",
	    "additionalProperties" : false,
	    "required" : [ "id" ],
	    "properties" : {
	      "id" : {
		"type" : "string"
	      },
	      "config" : {
		"type" : "string"
	      },
	      "metadata" : {
		"oneOf" : [ {
		  "$ref" : "#/definitions/DataObjectMetadata"
		} ]
	      }
	    }
	  },
	  "DataObjectMetadata" : {
	    "type" : "object",
	    "description" : "Additional metadata for a DataObject",
	    "additionalProperties" : false,
	    "properties" : {
	      "name" : {
		"type" : "string",
		"description" : "Readable name of the DataObject"
	      },
	      "description" : {
		"type" : "string",
		"description" : "Description of the content of the DataObject"
	      },
	      "layer" : {
		"type" : "string",
		"description" : "Name of the layer this DataObject belongs to"
	      },
	      "subjectArea" : {
		"type" : "string",
		"description" : "Name of the subject area this DataObject belongs to"
	      },
	      "tags" : {
		"type" : "array",
		"description" : "Optional custom tags for this object",
		"items" : {
		  "type" : "string"
		}
	      }
	    }
	  },
	  "JdbcTableDataObject" : {
	    "type" : "object",
	    "description" : "[[DataObject]] of type JDBC.\r\nProvides details for an action to access tables in a database through JDBC.",
	    "additionalProperties" : false,
	    "required" : [ "id", "table", "connectionId" ],
	    "properties" : {
	      "id" : {
		"type" : "string",
		"description" : "unique name of this data object"
	      },
	      "createSql" : {
		"type" : "string",
		"description" : "DDL-statement to be executed in prepare phase, using output jdbc connection.\r\nNote that it is also possible to let Spark create the table in Init-phase. See jdbcOptions to customize column data types for auto-created DDL-statement."
	      },
	      "preReadSql" : {
		"type" : "string",
		"description" : "SQL-statement to be executed in exec phase before reading input table, using input jdbc connection.\r\nUse tokens with syntax %{<spark sql expression>} to substitute with values from[[DefaultExpressionData]] ."
	      },
	      "postReadSql" : {
		"type" : "string",
		"description" : "SQL-statement to be executed in exec phase after reading input table and before action is finished, using input jdbc connection\r\nUse tokens with syntax %{<spark sql expression>} to substitute with values from[[DefaultExpressionData]] ."
	      },
	      "preWriteSql" : {
		"type" : "string",
		"description" : "SQL-statement to be executed in exec phase before writing output table, using output jdbc connection\r\nUse tokens with syntax %{<spark sql expression>} to substitute with values from[[DefaultExpressionData]] ."
	      },
	      "postWriteSql" : {
		"type" : "string",
		"description" : "SQL-statement to be executed in exec phase after writing output table, using output jdbc connection\r\nUse tokens with syntax %{<spark sql expression>} to substitute with values from[[DefaultExpressionData]] ."
	      },
	      "schemaMin" : {
		"type" : "string",
		"description" : "An optional, minimal schema that this DataObject must have to pass schema validation on reading and writing."
	      },
	      "table" : {
		"oneOf" : [ {
		  "$ref" : "#/definitions/Table"
		} ]
	      },
	      "jdbcFetchSize" : {
		"type" : "integer",
		"description" : "Number of rows to be fetched together by the Jdbc driver"
	      },
	      "saveMode" : {
		"type" : "string",
		"enum" : [ "Merge ", "OverwriteOptimized ", "OverwritePreserveDirectories ", "Ignore ", "ErrorIfExists ", "Append ", "Overwrite " ],
		"description" : "[[SDLSaveMode]] to use when writing table, default is \\\"Overwrite\\\". Only \\\"Append\\\" and \\\"Overwrite\\\" supported."
	      },
	      "allowSchemaEvolution" : {
		"type" : "string",
		"description" : "If set to true schema evolution will automatically occur when writing to this DataObject with different schema, otherwise SDL will stop with error."
	      },
	      "connectionId" : {
		"type" : "string",
		"description" : "Id of JdbcConnection configuration"
	      },
	      "jdbcOptions" : {
		"type" : "object",
		"description" : "Any jdbc options according to[[https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html]] .\r\nNote that some options above set and override some of this options explicitly.\r\nUse \\\"createTableOptions\\\" and \\\"createTableColumnTypes\\\" to control automatic creating of database tables.",
		"additionalProperties" : {
		  "type" : "string"
		}
	      },
	      "virtualPartitions" : {
		"type" : "array",
		"description" : "Virtual partition columns. Note that this doesn\\'t need to be the same as the database partition\r\ncolumns for this table. But it is important that there is an index on these columns to efficiently\r\nlist existing \\\"partitions\\\".",
		"items" : {
		  "type" : "string"
		}
	      },
	      "expectedPartitionsCondition" : {
		"type" : "string",
		"description" : "Optional definition of partitions expected to exist.\r\nDefine a Spark SQL expression that is evaluated against a[[PartitionValues]] instance and returns true or false\r\nDefault is to expect all partitions to exist."
	      },
	      "metadata" : {
		"oneOf" : [ {
		  "$ref" : "#/definitions/DataObjectMetadata"
		} ]
	      }
	    }
	  },
	  "RelaxedCsvFileDataObject" : {
	    "type" : "object",
	    "description" : "A[[DataObject]] which allows for more flexible CSV parsing.\r\nThe standard CsvFileDataObject doesnt support reading multiple CSV-Files with different column order, missing columns\r\nor additional columns.\r\nRelaxCsvFileDataObject works more like reading JSON-Files. You need to define a schema, then it tries to read every file\r\nwith that schema independently of the column order, adding missing columns and removing superfluous ones.\r\nCSV files are read by Spark as whole text files and then parsed manually with Sparks CSV parser class. You can therefore use the\r\nnormal CSV options of spark, but some properties are fixed, e.g. header=true, inferSchema=false, enforceSchema (ignored).\r\n\nNOTE: This data object sets the following default values for`csvOptions`: delimiter = \\\",\\\", quote = null\r\nAll other`csvOption` default to the values defined by Apache Spark.\r\nSEE: [[org.apache.spark.sql.DataFrameReader]]\r\nSEE: [[org.apache.spark.sql.DataFrameWriter]] \r\nIf mode is permissive you can retrieve the corrupt input record by adding <options.columnNameOfCorruptRecord> as field to the schema.\r\nRelaxCsvFileDataObject also supports getting an error msg by adding \\\"<options.columnNameOfCorruptRecord>_msg\\\" as field to the schema.",
	    "additionalProperties" : false,
	    "required" : [ "id", "path" ],
	    "properties" : {
	      "id" : {
		"type" : "string"
	      },
	      "path" : {
		"type" : "string"
	      },
	      "csvOptions" : {
		"type" : "object",
		"description" : "Settings for the underlying[[org.apache.spark.sql.DataFrameReader]]and[[org.apache.spark.sql.DataFrameWriter]] .",
		"additionalProperties" : {
		  "type" : "string"
		}
	      },
	      "partitions" : {
		"type" : "array",
		"items" : {
		  "type" : "string"
		}
	      },
	      "schema" : {
		"type" : "string",
		"description" : "The data object schema."
	      },
	      "schemaMin" : {
		"type" : "string"
	      },
	      "dateColumnType" : {
		"type" : "string",
		"enum" : [ "Default ", "String ", "Date " ],
		"description" : "Specifies the string format used for writing date typed data."
	      },
	      "treatMissingColumnsAsCorrupt" : {
		"type" : "string",
		"description" : "If set to true records from files with missing columns in its header are treated as corrupt (default=false).\r\nCorrupt records are handled according to options.mode (default=permissive)."
	      },
	      "treatSuperfluousColumnsAsCorrupt" : {
		"type" : "string",
		"description" : "If set to true records from files with superfluous columns in its header are treated as corrupt (default=false).\r\nCorrupt records are handled according to options.mode (default=permissive)."
	      },
	      "saveMode" : {
		"type" : "string",
		"enum" : [ "Merge ", "OverwriteOptimized ", "OverwritePreserveDirectories ", "Ignore ", "ErrorIfExists ", "Append ", "Overwrite " ]
	      },
	      "sparkRepartition" : {
		"type" : "object",
		"description" : "This controls repartitioning of the DataFrame before writing with Spark to Hadoop.\r\nWhen writing multiple partitions of a partitioned DataObject, the number of spark tasks created is equal to numberOfTasksPerPartition\r\nmultiplied with the number of partitions to write. To spread the records of a partition only over numberOfTasksPerPartition spark tasks,\r\nkeyCols must be given which are used to derive a task number inside the partition (hashvalue(keyCols) modulo numberOfTasksPerPartition).\r\nWhen writing to an unpartitioned DataObject or only one partition of a partitioned DataObject, the number of spark tasks created is equal\r\nto numberOfTasksPerPartition. Optional keyCols can be used to keep corresponding records together in the same task/file.",
		"additionalProperties" : false,
		"required" : [ "numberOfTasksPerPartition" ],
		"properties" : {
		  "numberOfTasksPerPartition" : {
		    "type" : "integer",
		    "description" : "Number of Spark tasks to create per partition before writing to DataObject by repartitioning the DataFrame.\r\nThis controls how many files are created in each Hadoop partition."
		  },
		  "keyCols" : {
		    "type" : "array",
		    "description" : "Optional key columns to distribute records over Spark tasks inside a Hadoop partition.\r\nIf DataObject has Hadoop partitions defined, keyCols must be defined.",
		    "items" : {
		      "type" : "string"
		    }
		  },
		  "sortCols" : {
		    "type" : "array",
		    "description" : "Optional columns to sort records inside files created.",
		    "items" : {
		      "type" : "string"
		    }
		  },
		  "filename" : {
		    "type" : "string",
		    "description" : "Option filename to rename target file(s). If numberOfTasksPerPartition is greater than 1,\r\nmultiple files can exist in a directory and a number is inserted into the filename after the first \\'.\\'.\r\nExample: filename=data.csv -> files created are data.1.csv, data.2.csv, ..."
		  }
		}
	      },
	      "acl" : {
		"type" : "object",
		"description" : "Describes a complete ACL Specification (basic owner/group/other permissions AND extended ACLS)\r\nto be applied to a Data Object on writing",
		"additionalProperties" : false,
		"required" : [ "permission", "acls" ],
		"properties" : {
		  "permission" : {
		    "type" : "string",
		    "description" : ": File system permission string in symbolic notation form (e.g. rwxr-xr-x)"
		  },
		  "acls" : {
		    "type" : "array",
		    "description" : ": a sequence of[[AclElement]] s",
		    "items" : {
		      "type" : "object",
		      "description" : "Describes a single extended ACL to be applied to a Data Object\r\nin addition to the basic file system permissions",
		      "additionalProperties" : false,
		      "required" : [ "aclType", "name", "permission" ],
		      "properties" : {
			"aclType" : {
			  "type" : "string",
			  "description" : ": type of ACL to be added \\\"group\\\", \\\"user\\\""
			},
			"name" : {
			  "type" : "string",
			  "description" : ": the name of the user/group for which an ACL definition is being added"
			},
			"permission" : {
			  "type" : "string",
			  "description" : ": the permission (rwx syntax) to be granted"
			}
		      }
		    }
		  }
		}
	      },
	      "connectionId" : {
		"type" : "string"
	      },
	      "filenameColumn" : {
		"type" : "string"
	      },
	      "expectedPartitionsCondition" : {
		"type" : "string",
		"description" : "Optional definition of partitions expected to exist.\r\nDefine a Spark SQL expression that is evaluated against a[[PartitionValues]] instance and returns true or false\r\nDefault is to expect all partitions to exist."
	      },
	      "metadata" : {
		"oneOf" : [ {
		  "$ref" : "#/definitions/DataObjectMetadata"
		} ]
	      }
	    }
	  },
	  "HadoopFileConnection" : {
	    "type" : "object",
	    "description" : "Connection information for files on hadoop",
	    "additionalProperties" : false,
	    "required" : [ "id", "pathPrefix" ],
	    "properties" : {
	      "id" : {
		"type" : "string",
		"description" : "unique id of this connection"
	      },
	      "pathPrefix" : {
		"type" : "string",
		"description" : "schema, authority and base path for accessing files on hadoop"
	      },
	      "acl" : {
		"type" : "object",
		"description" : "Describes a complete ACL Specification (basic owner/group/other permissions AND extended ACLS)\r\nto be applied to a Data Object on writing",
		"additionalProperties" : false,
		"required" : [ "permission", "acls" ],
		"properties" : {
		  "permission" : {
		    "type" : "string",
		    "description" : ": File system permission string in symbolic notation form (e.g. rwxr-xr-x)"
		  },
		  "acls" : {
		    "type" : "array",
		    "description" : ": a sequence of[[AclElement]] s",
		    "items" : {
		      "type" : "object",
		      "description" : "Describes a single extended ACL to be applied to a Data Object\r\nin addition to the basic file system permissions",
		      "additionalProperties" : false,
		      "required" : [ "aclType", "name", "permission" ],
		      "properties" : {
			"aclType" : {
			  "type" : "string",
			  "description" : ": type of ACL to be added \\\"group\\\", \\\"user\\\""
			},
			"name" : {
			  "type" : "string",
			  "description" : ": the name of the user/group for which an ACL definition is being added"
			},
			"permission" : {
			  "type" : "string",
			  "description" : ": the permission (rwx syntax) to be granted"
			}
		      }
		    }
		  }
		}
	      },
	      "metadata" : {
		"oneOf" : [ {
		  "$ref" : "#/definitions/ConnectionMetadata"
		} ]
	      }
	    }
	  },
	  "DeltaLakeTableConnection" : {
	    "type" : "object",
	    "additionalProperties" : false,
	    "required" : [ "id", "db", "pathPrefix" ],
	    "properties" : {
	      "id" : {
		"type" : "string"
	      },
	      "db" : {
		"type" : "string"
	      },
	      "pathPrefix" : {
		"type" : "string"
	      },
	      "acl" : {
		"type" : "object",
		"description" : "Describes a complete ACL Specification (basic owner/group/other permissions AND extended ACLS)\r\nto be applied to a Data Object on writing",
		"additionalProperties" : false,
		"required" : [ "permission", "acls" ],
		"properties" : {
		  "permission" : {
		    "type" : "string",
		    "description" : ": File system permission string in symbolic notation form (e.g. rwxr-xr-x)"
		  },
		  "acls" : {
		    "type" : "array",
		    "description" : ": a sequence of[[AclElement]] s",
		    "items" : {
		      "type" : "object",
		      "description" : "Describes a single extended ACL to be applied to a Data Object\r\nin addition to the basic file system permissions",
		      "additionalProperties" : false,
		      "required" : [ "aclType", "name", "permission" ],
		      "properties" : {
			"aclType" : {
			  "type" : "string",
			  "description" : ": type of ACL to be added \\\"group\\\", \\\"user\\\""
			},
			"name" : {
			  "type" : "string",
			  "description" : ": the name of the user/group for which an ACL definition is being added"
			},
			"permission" : {
			  "type" : "string",
			  "description" : ": the permission (rwx syntax) to be granted"
			}
		      }
		    }
		  }
		}
	      },
	      "checkDeltaLakeSparkOptions" : {
		"type" : "string"
	      },
	      "metadata" : {
		"oneOf" : [ {
		  "$ref" : "#/definitions/ConnectionMetadata"
		} ]
	      }
	    }
	  },
	  "AvroFileDataObject" : {
	    "type" : "object",
	    "description" : "A[[io.smartdatalake.workflow.dataobject.DataObject]]backed by an Avro data source.\r\nIt manages read and write access and configurations required for[[io.smartdatalake.workflow.action.Action]]s to\r\nwork on Avro formatted files.\r\nReading and writing details are delegated to Apache Spark[[org.apache.spark.sql.DataFrameReader]]\r\nand[[org.apache.spark.sql.DataFrameWriter]]respectively. The reader and writer implementations are provided by\r\nthe[[https://github.com/databricks/spark-avro databricks spark-avro]] project.\r\n\nSEE: [[org.apache.spark.sql.DataFrameReader]]\r\nSEE: [[org.apache.spark.sql.DataFrameWriter]]",
	    "additionalProperties" : false,
	    "required" : [ "id", "path" ],
	    "properties" : {
	      "id" : {
		"type" : "string"
	      },
	      "path" : {
		"type" : "string"
	      },
	      "partitions" : {
		"type" : "array",
		"items" : {
		  "type" : "string"
		}
	      },
	      "avroOptions" : {
		"type" : "object",
		"description" : "Settings for the underlying[[org.apache.spark.sql.DataFrameReader]]and\r\n[[org.apache.spark.sql.DataFrameWriter]] .",
		"additionalProperties" : {
		  "type" : "string"
		}
	      },
	      "schema" : {
		"type" : "string",
		"description" : "An optional schema for the spark data frame to be validated on read and write. Note: Existing Avro files\r\ncontain a source schema. Therefore, this schema is ignored when reading from existing Avro files.\r\nAs this corresponds to the schema on write, it must not include the optional filenameColumn on read."
	      },
	      "schemaMin" : {
		"type" : "string"
	      },
	      "saveMode" : {
		"type" : "string",
		"enum" : [ "Merge ", "OverwriteOptimized ", "OverwritePreserveDirectories ", "Ignore ", "ErrorIfExists ", "Append ", "Overwrite " ]
	      },
	      "sparkRepartition" : {
		"type" : "object",
		"description" : "This controls repartitioning of the DataFrame before writing with Spark to Hadoop.\r\nWhen writing multiple partitions of a partitioned DataObject, the number of spark tasks created is equal to numberOfTasksPerPartition\r\nmultiplied with the number of partitions to write. To spread the records of a partition only over numberOfTasksPerPartition spark tasks,\r\nkeyCols must be given which are used to derive a task number inside the partition (hashvalue(keyCols) modulo numberOfTasksPerPartition).\r\nWhen writing to an unpartitioned DataObject or only one partition of a partitioned DataObject, the number of spark tasks created is equal\r\nto numberOfTasksPerPartition. Optional keyCols can be used to keep corresponding records together in the same task/file.",
		"additionalProperties" : false,
		"required" : [ "numberOfTasksPerPartition" ],
		"properties" : {
		  "numberOfTasksPerPartition" : {
		    "type" : "integer",
		    "description" : "Number of Spark tasks to create per partition before writing to DataObject by repartitioning the DataFrame.\r\nThis controls how many files are created in each Hadoop partition."
		  },
		  "keyCols" : {
		    "type" : "array",
		    "description" : "Optional key columns to distribute records over Spark tasks inside a Hadoop partition.\r\nIf DataObject has Hadoop partitions defined, keyCols must be defined.",
		    "items" : {
		      "type" : "string"
		    }
		  },
		  "sortCols" : {
		    "type" : "array",
		    "description" : "Optional columns to sort records inside files created.",
		    "items" : {
		      "type" : "string"
		    }
		  },
		  "filename" : {
		    "type" : "string",
		    "description" : "Option filename to rename target file(s). If numberOfTasksPerPartition is greater than 1,\r\nmultiple files can exist in a directory and a number is inserted into the filename after the first \\'.\\'.\r\nExample: filename=data.csv -> files created are data.1.csv, data.2.csv, ..."
		  }
		}
	      },
	      "acl" : {
		"type" : "object",
		"description" : "Describes a complete ACL Specification (basic owner/group/other permissions AND extended ACLS)\r\nto be applied to a Data Object on writing",
		"additionalProperties" : false,
		"required" : [ "permission", "acls" ],
		"properties" : {
		  "permission" : {
		    "type" : "string",
		    "description" : ": File system permission string in symbolic notation form (e.g. rwxr-xr-x)"
		  },
		  "acls" : {
		    "type" : "array",
		    "description" : ": a sequence of[[AclElement]] s",
		    "items" : {
		      "type" : "object",
		      "description" : "Describes a single extended ACL to be applied to a Data Object\r\nin addition to the basic file system permissions",
		      "additionalProperties" : false,
		      "required" : [ "aclType", "name", "permission" ],
		      "properties" : {
			"aclType" : {
			  "type" : "string",
			  "description" : ": type of ACL to be added \\\"group\\\", \\\"user\\\""
			},
			"name" : {
			  "type" : "string",
			  "description" : ": the name of the user/group for which an ACL definition is being added"
			},
			"permission" : {
			  "type" : "string",
			  "description" : ": the permission (rwx syntax) to be granted"
			}
		      }
		    }
		  }
		}
	      },
	      "connectionId" : {
		"type" : "string"
	      },
	      "filenameColumn" : {
		"type" : "string"
	      },
	      "expectedPartitionsCondition" : {
		"type" : "string",
		"description" : "Optional definition of partitions expected to exist.\r\nDefine a Spark SQL expression that is evaluated against a[[PartitionValues]] instance and returns true or false\r\nDefault is to expect all partitions to exist."
	      },
	      "housekeepingMode" : {
		"oneOf" : [ {
		  "type" : "object",
		  "description" : "Archive and compact old partitions:\r\nArchive partition reduces the number of partitions in the past by moving older partitions into special \\\"archive partitions\\\".\r\nCompact partition reduces the number of files in a partition by rewriting them with Spark.\r\nExample: archive and compact a table with partition layout run_id=<integer>\r\n- archive partitions after 1000 partitions into \\\"archive partition\\\" equal to floor(run_id/1000)\r\n- compact \\\"archive partition\\\" when full\r\n{{{\r\nhousekeepingMode = {\r\ntype = PartitionArchiveCompactionMode\r\narchivePartitionExpression = \\\"if( elements[\\'run_id\\'] < runId - 1000, map(\\'run_id\\', elements[\\'run_id\\'] div 1000), elements)\\\"\r\ncompactPartitionExpression = \\\"elements[\\'run_id\\'] % 1000 = 0 and elements[\\'run_id\\'] <= runId - 2000\\\"\r\n}\r\n}}}",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "PartitionArchiveCompactionMode"
		    },
		    "archivePartitionExpression" : {
		      "type" : "string",
		      "description" : "Expression to define the archive partition for a given partition. Define a spark\r\nsql expression working with the attributes of[[PartitionExpressionData]] returning archive\r\npartition values as Map[String,String]. If return value is the same as input elements, partition is not touched,\r\notherwise all files of the partition are moved to the returned partition definition.\r\nBe aware that the value of the partition columns changes for these files/records."
		    },
		    "compactPartitionExpression" : {
		      "type" : "string",
		      "description" : "Expression to define partitions which should be compacted. Define a spark\r\nsql expression working with the attributes of[[PartitionExpressionData]] returning a\r\nboolean = true when this partition should be compacted.\r\nOnce a partition is compacted, it is marked as compacted and will not be compacted again.\r\nIt is therefore ok to return true for all partitions which should be compacted, regardless if they have been compacted already."
		    },
		    "description" : {
		      "type" : "string"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Keep partitions while retention condition is fulfilled, delete other partitions.\r\nExample: cleanup partitions with partition layout dt=<yyyymmdd> after 90 days:\r\n{{{\r\nhousekeepingMode = {\r\ntype = PartitionRetentionMode\r\nretentionCondition = \\\"datediff(now(), to_date(elements[\\'dt\\'], \\'yyyyMMdd\\')) <= 90\\\"\r\n}\r\n}}}",
		  "additionalProperties" : false,
		  "required" : [ "retentionCondition", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "PartitionRetentionMode"
		    },
		    "retentionCondition" : {
		      "type" : "string",
		      "description" : "Condition to decide if a partition should be kept. Define a spark sql expression\r\nworking with the attributes of[[PartitionExpressionData]] returning a boolean with value true if the partition should be kept."
		    },
		    "description" : {
		      "type" : "string"
		    }
		  }
		} ]
	      },
	      "metadata" : {
		"oneOf" : [ {
		  "$ref" : "#/definitions/DataObjectMetadata"
		} ]
	      }
	    }
	  },
	  "CsvFileDataObject" : {
	    "type" : "object",
	    "description" : "A[[DataObject]]backed by a comma-separated value (CSV) data source.\r\nIt manages read and write access and configurations required for[[io.smartdatalake.workflow.action.Action]]s to\r\nwork on CSV formatted files.\r\nCSV reading and writing details are delegated to Apache Spark[[org.apache.spark.sql.DataFrameReader]]\r\nand[[org.apache.spark.sql.DataFrameWriter]]respectively.\r\nRead Schema specifications:\r\nIf a data object schema is not defined via the`schema`attribute (default) and`inferSchema`option is\r\ndisabled (default) in`csvOptions`, then all column types are set to String and the first row of the CSV file is read\r\nto determine the column names and the number of fields.\r\nIf the`header`option is disabled (default) in`csvOptions`, then the header is defined as \\\"_c#\\\" for each column\r\nwhere \\\"#\\\" is the column index.\r\nOtherwise the first row of the CSV file is not included in the DataFrame content and its entries\r\nare used as the column names for the schema.\r\nIf a data object schema is not defined via the`schema`attribute and`inferSchema`is enabled in`csvOptions`, then\r\nthe`samplingRatio`(default: 1.0) option in`csvOptions` is used to extract a sample from the CSV file in order to\r\ndetermine the input schema automatically.\r\n\nNOTE: This data object sets the following default values for`csvOptions`: delimiter = \\\"|\\\", quote = null, header = false, and inferSchema = false.\r\nAll other`csvOption` default to the values defined by Apache Spark.\r\n\nSEE: [[org.apache.spark.sql.DataFrameReader]]\r\nSEE: [[org.apache.spark.sql.DataFrameWriter]]",
	    "additionalProperties" : false,
	    "required" : [ "id", "path" ],
	    "properties" : {
	      "id" : {
		"type" : "string"
	      },
	      "path" : {
		"type" : "string"
	      },
	      "csvOptions" : {
		"type" : "object",
		"description" : "Settings for the underlying[[org.apache.spark.sql.DataFrameReader]]and[[org.apache.spark.sql.DataFrameWriter]] .",
		"additionalProperties" : {
		  "type" : "string"
		}
	      },
	      "partitions" : {
		"type" : "array",
		"items" : {
		  "type" : "string"
		}
	      },
	      "schema" : {
		"type" : "string",
		"description" : "An optional data object schema. If defined, any automatic schema inference is avoided. As this corresponds to the schema on write, it must not include the optional filenameColumn on read."
	      },
	      "schemaMin" : {
		"type" : "string"
	      },
	      "dateColumnType" : {
		"type" : "string",
		"enum" : [ "Default ", "String ", "Date " ],
		"description" : "Specifies the string format used for writing date typed data."
	      },
	      "saveMode" : {
		"type" : "string",
		"enum" : [ "Merge ", "OverwriteOptimized ", "OverwritePreserveDirectories ", "Ignore ", "ErrorIfExists ", "Append ", "Overwrite " ]
	      },
	      "sparkRepartition" : {
		"type" : "object",
		"description" : "This controls repartitioning of the DataFrame before writing with Spark to Hadoop.\r\nWhen writing multiple partitions of a partitioned DataObject, the number of spark tasks created is equal to numberOfTasksPerPartition\r\nmultiplied with the number of partitions to write. To spread the records of a partition only over numberOfTasksPerPartition spark tasks,\r\nkeyCols must be given which are used to derive a task number inside the partition (hashvalue(keyCols) modulo numberOfTasksPerPartition).\r\nWhen writing to an unpartitioned DataObject or only one partition of a partitioned DataObject, the number of spark tasks created is equal\r\nto numberOfTasksPerPartition. Optional keyCols can be used to keep corresponding records together in the same task/file.",
		"additionalProperties" : false,
		"required" : [ "numberOfTasksPerPartition" ],
		"properties" : {
		  "numberOfTasksPerPartition" : {
		    "type" : "integer",
		    "description" : "Number of Spark tasks to create per partition before writing to DataObject by repartitioning the DataFrame.\r\nThis controls how many files are created in each Hadoop partition."
		  },
		  "keyCols" : {
		    "type" : "array",
		    "description" : "Optional key columns to distribute records over Spark tasks inside a Hadoop partition.\r\nIf DataObject has Hadoop partitions defined, keyCols must be defined.",
		    "items" : {
		      "type" : "string"
		    }
		  },
		  "sortCols" : {
		    "type" : "array",
		    "description" : "Optional columns to sort records inside files created.",
		    "items" : {
		      "type" : "string"
		    }
		  },
		  "filename" : {
		    "type" : "string",
		    "description" : "Option filename to rename target file(s). If numberOfTasksPerPartition is greater than 1,\r\nmultiple files can exist in a directory and a number is inserted into the filename after the first \\'.\\'.\r\nExample: filename=data.csv -> files created are data.1.csv, data.2.csv, ..."
		  }
		}
	      },
	      "acl" : {
		"type" : "object",
		"description" : "Describes a complete ACL Specification (basic owner/group/other permissions AND extended ACLS)\r\nto be applied to a Data Object on writing",
		"additionalProperties" : false,
		"required" : [ "permission", "acls" ],
		"properties" : {
		  "permission" : {
		    "type" : "string",
		    "description" : ": File system permission string in symbolic notation form (e.g. rwxr-xr-x)"
		  },
		  "acls" : {
		    "type" : "array",
		    "description" : ": a sequence of[[AclElement]] s",
		    "items" : {
		      "type" : "object",
		      "description" : "Describes a single extended ACL to be applied to a Data Object\r\nin addition to the basic file system permissions",
		      "additionalProperties" : false,
		      "required" : [ "aclType", "name", "permission" ],
		      "properties" : {
			"aclType" : {
			  "type" : "string",
			  "description" : ": type of ACL to be added \\\"group\\\", \\\"user\\\""
			},
			"name" : {
			  "type" : "string",
			  "description" : ": the name of the user/group for which an ACL definition is being added"
			},
			"permission" : {
			  "type" : "string",
			  "description" : ": the permission (rwx syntax) to be granted"
			}
		      }
		    }
		  }
		}
	      },
	      "connectionId" : {
		"type" : "string"
	      },
	      "filenameColumn" : {
		"type" : "string"
	      },
	      "expectedPartitionsCondition" : {
		"type" : "string",
		"description" : "Optional definition of partitions expected to exist.\r\nDefine a Spark SQL expression that is evaluated against a[[PartitionValues]] instance and returns true or false\r\nDefault is to expect all partitions to exist."
	      },
	      "housekeepingMode" : {
		"oneOf" : [ {
		  "type" : "object",
		  "description" : "Archive and compact old partitions:\r\nArchive partition reduces the number of partitions in the past by moving older partitions into special \\\"archive partitions\\\".\r\nCompact partition reduces the number of files in a partition by rewriting them with Spark.\r\nExample: archive and compact a table with partition layout run_id=<integer>\r\n- archive partitions after 1000 partitions into \\\"archive partition\\\" equal to floor(run_id/1000)\r\n- compact \\\"archive partition\\\" when full\r\n{{{\r\nhousekeepingMode = {\r\ntype = PartitionArchiveCompactionMode\r\narchivePartitionExpression = \\\"if( elements[\\'run_id\\'] < runId - 1000, map(\\'run_id\\', elements[\\'run_id\\'] div 1000), elements)\\\"\r\ncompactPartitionExpression = \\\"elements[\\'run_id\\'] % 1000 = 0 and elements[\\'run_id\\'] <= runId - 2000\\\"\r\n}\r\n}}}",
		  "additionalProperties" : false,
		  "required" : [ "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "PartitionArchiveCompactionMode"
		    },
		    "archivePartitionExpression" : {
		      "type" : "string",
		      "description" : "Expression to define the archive partition for a given partition. Define a spark\r\nsql expression working with the attributes of[[PartitionExpressionData]] returning archive\r\npartition values as Map[String,String]. If return value is the same as input elements, partition is not touched,\r\notherwise all files of the partition are moved to the returned partition definition.\r\nBe aware that the value of the partition columns changes for these files/records."
		    },
		    "compactPartitionExpression" : {
		      "type" : "string",
		      "description" : "Expression to define partitions which should be compacted. Define a spark\r\nsql expression working with the attributes of[[PartitionExpressionData]] returning a\r\nboolean = true when this partition should be compacted.\r\nOnce a partition is compacted, it is marked as compacted and will not be compacted again.\r\nIt is therefore ok to return true for all partitions which should be compacted, regardless if they have been compacted already."
		    },
		    "description" : {
		      "type" : "string"
		    }
		  }
		}, {
		  "type" : "object",
		  "description" : "Keep partitions while retention condition is fulfilled, delete other partitions.\r\nExample: cleanup partitions with partition layout dt=<yyyymmdd> after 90 days:\r\n{{{\r\nhousekeepingMode = {\r\ntype = PartitionRetentionMode\r\nretentionCondition = \\\"datediff(now(), to_date(elements[\\'dt\\'], \\'yyyyMMdd\\')) <= 90\\\"\r\n}\r\n}}}",
		  "additionalProperties" : false,
		  "required" : [ "retentionCondition", "type" ],
		  "properties" : {
		    "type" : {
		      "const" : "PartitionRetentionMode"
		    },
		    "retentionCondition" : {
		      "type" : "string",
		      "description" : "Condition to decide if a partition should be kept. Define a spark sql expression\r\nworking with the attributes of[[PartitionExpressionData]] returning a boolean with value true if the partition should be kept."
		    },
		    "description" : {
		      "type" : "string"
		    }
		  }
		} ]
	      },
	      "metadata" : {
		"oneOf" : [ {
		  "$ref" : "#/definitions/DataObjectMetadata"
		} ]
	      }
	    }
	  },
	  "CustomDfDataObject" : {
	    "type" : "object",
	    "description" : "Generic[[DataObject]] containing a config object.\r\nE.g. used to implement a CustomAction that reads a Webservice.",
	    "additionalProperties" : false,
	    "required" : [ "id", "creator" ],
	    "properties" : {
	      "id" : {
		"type" : "string"
	      },
	      "creator" : {
		"type" : "object",
		"description" : "Configuration of a custom Spark-DataFrame creator as part of[[CustomDfDataObject]]\r\nDefine a exec function which receives a map of options and returns a DataFrame to be used as input.\r\nOptionally define a schema function to return a StructType used as schema in init-phase.\r\nSee also trait[[CustomDfCreator]] .\r\nNote that for now implementing CustomDfCreator.schema method is only possible with className configuration attribute.",
		"additionalProperties" : false,
		"properties" : {
		  "className" : {
		    "type" : "string",
		    "description" : "Optional class name implementing trait[[CustomDfCreator]]"
		  },
		  "scalaFile" : {
		    "type" : "string",
		    "description" : "Optional file where scala code for creator is loaded from. The scala code in the file needs to be a function of type[[fnExecType]] ."
		  },
		  "scalaCode" : {
		    "type" : "string",
		    "description" : "Optional scala code for creator. The scala code needs to be a function of type[[fnExecType]] ."
		  },
		  "options" : {
		    "type" : "object",
		    "description" : "Options to pass to the creator",
		    "additionalProperties" : {
		      "type" : "string"
		    }
		  }
		}
	      },
	      "schemaMin" : {
		"type" : "string"
	      },
	      "metadata" : {
		"oneOf" : [ {
		  "$ref" : "#/definitions/DataObjectMetadata"
		} ]
	      }
	    }
	  }
	},
	"$schema" : "http://json-schema.org/draft-07/schema#"
      }